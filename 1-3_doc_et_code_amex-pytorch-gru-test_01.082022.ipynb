{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03fb659",
   "metadata": {},
   "source": [
    "**Parcours Ingénieur Machine Learning**<br>\n",
    "**Plus d'informations** : https://openclassrooms.com/fr/paths/148-ingenieur-machine-learning <br>\n",
    "\n",
    "**Auteur** : Viktoriya Zeruk<br>\n",
    "**Date dernière version** : 18/08/2022<br>\n",
    "**Accès projet git** : https://github.com/viczer/P8-Openclassroom_competition_Kaggle <br>\n",
    "**Lien kernel Kaggle** : https://www.kaggle.com/code/victoriazeruk/amex-pytorch-gru-test?scriptVersionId=102969947\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9bd4e8",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; background: rgb(75,0,130);\n",
    "background: linear-gradient(90deg, rgba(75,0,130,1) 47%, rgba(216,191,216,1) 89%, rgba(230,230,250,1) 100%);\">\n",
    "<h2 style=\"margin: auto; font-weight: bold; padding: 30px 30px 0px 30px;\" align=\"center\">Project 8 : Participez a une competition Kaggle !\n",
    " </br>  | Notebook 4 : GRU model test | <br></h2> </div>    \n",
    "\n",
    "<div style=\"display: flex; background: rgb(75,0,130);\n",
    "background: linear-gradient(90deg, rgba(75,0,130,1) 47%, rgba(216,191,216,1) 89%, rgba(230,230,250,1) 100%);\">\n",
    "<h4 style=\"margin: auto; font-weight: bold; padding: 30px 30px 0px 30px;\" align=\"center\"> </h4> \n",
    "</div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c3ee4",
   "metadata": {
    "papermill": {
     "duration": 0.011915,
     "end_time": "2022-08-10T11:01:23.551538",
     "exception": false,
     "start_time": "2022-08-10T11:01:23.539623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Time Series GRU PyTorch Training Notebook\n",
    "This notebook is based on implementation of the [TensorFlow GRU Starter - [0.790]](https://www.kaggle.com/code/cdeotte/tensorflow-gru-starter-0-790/)\n",
    "\n",
    "In this notebook we present starter code for a time series GRU model and starter code for processing Kaggle's 50GB CSV files into multiple saved NumPy files. Using a time series GRU allows us to use all the provided customer data and not just the customer's last data point. We published plots of time series data [here][1]. In this notebook we\n",
    "* Processes the train data from dataframes into 3D NumPy array of dimensions `num_of_customers x 13 x 188`\n",
    "* Save processed arrays as multiple NumPy files on disk\n",
    "* Next we build and train a GRU from the multiple files on disk\n",
    "* We compute validation score\n",
    "* Finally we process and save test data, infer test, and create a submission\n",
    "\n",
    "\n",
    "To view time series EDA which can help give you intuition about feature engineering and improving model architecture, see the notebook [here][1].\n",
    "\n",
    "[1]: https://www.kaggle.com/cdeotte/time-series-eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd16b44",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-08-10T11:01:23.576819Z",
     "iopub.status.busy": "2022-08-10T11:01:23.576067Z",
     "iopub.status.idle": "2022-08-10T11:01:26.903752Z",
     "shell.execute_reply": "2022-08-10T11:01:26.901213Z"
    },
    "papermill": {
     "duration": 3.344795,
     "end_time": "2022-08-10T11:01:26.907915",
     "exception": false,
     "start_time": "2022-08-10T11:01:23.563120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import os, copy, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "print('Using PyTorch version',torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e693331",
   "metadata": {
    "papermill": {
     "duration": 0.005861,
     "end_time": "2022-08-10T11:01:26.920370",
     "exception": false,
     "start_time": "2022-08-10T11:01:26.914509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Process Train Data\n",
    "We process both train and test data in chunks. We split train data into 10 parts and process each part separately and save to disk. We split test into 20 parts. This allows us to avoid memory errors during processing. We can also perform processing on GPU which is faster than CPU. Discussions about data preprocessing are [here][1] and [here][2]\n",
    "\n",
    "[1]: https://www.kaggle.com/competitions/amex-default-prediction/discussion/327828\n",
    "[2]: https://www.kaggle.com/competitions/amex-default-prediction/discussion/328054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6543f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T11:01:26.935304Z",
     "iopub.status.busy": "2022-08-10T11:01:26.934182Z",
     "iopub.status.idle": "2022-08-10T11:01:26.940123Z",
     "shell.execute_reply": "2022-08-10T11:01:26.938943Z"
    },
    "papermill": {
     "duration": 0.015957,
     "end_time": "2022-08-10T11:01:26.942445",
     "exception": false,
     "start_time": "2022-08-10T11:01:26.926488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LOADING JUST FIRST COLUMN OF TRAIN OR TEST IS SLOW\n",
    "# INSTEAD YOU CAN LOAD FIRST COLUMN FROM MY DATASET\n",
    "# OTHERWISE SET VARIABLE TO NONE TO LOAD FROM KAGGLE'S ORIGINAL DATAFRAME\n",
    "PATH_TO_CUSTOMER_HASHES = '../input/amex-data-files/'\n",
    "\n",
    "# AFTER PROCESSING DATA ONCE, UPLOAD TO KAGGLE DATASET\n",
    "# THEN SET VARIABLE BELOW TO FALSE\n",
    "# AND ATTACH DATASET TO NOTEBOOK AND PUT PATH TO DATASET BELOW\n",
    "PROCESS_DATA = False\n",
    "#PATH_TO_DATA = './data/'\n",
    "PATH_TO_DATA = '../input/amex-data-for-transformers-and-rnns/data/'\n",
    "\n",
    "# AFTER TRAINING MODEL, UPLOAD TO KAGGLE DATASET\n",
    "# THEN SET VARIABLE BELOW TO FALSE\n",
    "# AND ATTACH DATASET TO NOTEBOOK AND PUT PATH TO DATASET BELOW\n",
    "TRAIN_MODEL = True\n",
    "PATH_TO_MODEL = './model/'\n",
    "#PATH_TO_MODEL = '../input/amex-data-for-transformers-and-rnns/model/'\n",
    "\n",
    "INFER_TEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a68041",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-10T11:01:26.955492Z",
     "iopub.status.busy": "2022-08-10T11:01:26.955175Z",
     "iopub.status.idle": "2022-08-10T11:01:30.305813Z",
     "shell.execute_reply": "2022-08-10T11:01:30.304385Z"
    },
    "papermill": {
     "duration": 3.360909,
     "end_time": "2022-08-10T11:01:30.309122",
     "exception": false,
     "start_time": "2022-08-10T11:01:26.948213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cupy, cudf # GPU LIBRARIES\n",
    "import numpy as np, pandas as pd # CPU LIBRARIES\n",
    "import matplotlib.pyplot as plt, gc\n",
    "\n",
    "if PROCESS_DATA:\n",
    "    # LOAD TARGETS\n",
    "    targets = cudf.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    targets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    print(f'There are {targets.shape[0]} train targets')\n",
    "    \n",
    "    # GET TRAIN COLUMN NAMES\n",
    "    train = cudf.read_csv('../input/amex-default-prediction/train_data.csv', nrows=1)\n",
    "    T_COLS = train.columns\n",
    "    print(f'There are {len(T_COLS)} train dataframe columns')\n",
    "    \n",
    "    # GET TRAIN CUSTOMER NAMES (use pandas to avoid memory error)\n",
    "    if PATH_TO_CUSTOMER_HASHES:\n",
    "        train = cudf.read_parquet(f'{PATH_TO_CUSTOMER_HASHES}train_customer_hashes.pqt')\n",
    "    else:\n",
    "        train = pd.read_csv('/raid/Kaggle/amex/train_data.csv', usecols=['customer_ID'])\n",
    "        train['customer_ID'] = train['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\n",
    "    customers = train.drop_duplicates().sort_index().values.flatten()\n",
    "    print(f'There are {len(customers)} unique customers in train.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0e94f6",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-10T11:01:30.324336Z",
     "iopub.status.busy": "2022-08-10T11:01:30.323983Z",
     "iopub.status.idle": "2022-08-10T11:01:30.332762Z",
     "shell.execute_reply": "2022-08-10T11:01:30.331646Z"
    },
    "papermill": {
     "duration": 0.018882,
     "end_time": "2022-08-10T11:01:30.335075",
     "exception": false,
     "start_time": "2022-08-10T11:01:30.316193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CALCULATE SIZE OF EACH SEPARATE FILE\n",
    "def get_rows(customers, train, NUM_FILES = 10, verbose = ''):\n",
    "    chunk = len(customers)//NUM_FILES\n",
    "    if verbose != '':\n",
    "        print(f'We will split {verbose} data into {NUM_FILES} separate files.')\n",
    "        print(f'There will be {chunk} customers in each file (except the last file).')\n",
    "        print('Below are number of rows in each file:')\n",
    "    rows = []\n",
    "\n",
    "    for k in range(NUM_FILES):\n",
    "        if k==NUM_FILES-1: cc = customers[k*chunk:]\n",
    "        else: cc = customers[k*chunk:(k+1)*chunk]\n",
    "        s = train.loc[train.customer_ID.isin(cc)].shape[0]\n",
    "        rows.append(s)\n",
    "    if verbose != '': print( rows )\n",
    "    return rows\n",
    "\n",
    "if PROCESS_DATA:\n",
    "    NUM_FILES = 10\n",
    "    rows = get_rows(customers, train, NUM_FILES = NUM_FILES, verbose = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a28179e",
   "metadata": {
    "papermill": {
     "duration": 0.006615,
     "end_time": "2022-08-10T11:01:30.348068",
     "exception": false,
     "start_time": "2022-08-10T11:01:30.341453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess and Feature Engineering\n",
    "The function below processes the data. Discussions describing the process are [here][1] and [here][2]. Currently the code below uses [RAPIDS][3] and GPU to\n",
    "* Reduces memory usage of customer_ID column by converting to int64\n",
    "* Reduces memory usage of date time column (then deletes the column).\n",
    "* We fill NANs\n",
    "* Label encodes the categorical columns\n",
    "* We reduce memory usage dtypes of columns\n",
    "* Converts every customer into a 3D array with sequence length 13 and feature length 188\n",
    "\n",
    "To improve this model, try adding new feautures. The columns have been rearanged to have the 11 categorical features first. This makes building the TensorFlow model later easier. We can also try adding Standard Scaler. Currently the data is being used without scaling from the original Kaggle train data. \n",
    "\n",
    "[1]: https://www.kaggle.com/competitions/amex-default-prediction/discussion/327828\n",
    "[2]: https://www.kaggle.com/competitions/amex-default-prediction/discussion/328054\n",
    "[3]: https://rapids.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd3c3b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T11:01:30.362602Z",
     "iopub.status.busy": "2022-08-10T11:01:30.361746Z",
     "iopub.status.idle": "2022-08-10T11:01:30.381737Z",
     "shell.execute_reply": "2022-08-10T11:01:30.380640Z"
    },
    "papermill": {
     "duration": 0.029512,
     "end_time": "2022-08-10T11:01:30.384051",
     "exception": false,
     "start_time": "2022-08-10T11:01:30.354539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineer(train, PAD_CUSTOMER_TO_13_ROWS = True, targets = None):\n",
    "        \n",
    "    # REDUCE STRING COLUMNS \n",
    "    # from 64 bytes to 8 bytes, and 10 bytes to 3 bytes respectively\n",
    "    train['customer_ID'] = train['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    train.S_2 = cudf.to_datetime( train.S_2 )\n",
    "    train['year'] = (train.S_2.dt.year-2000).astype('int8')\n",
    "    train['month'] = (train.S_2.dt.month).astype('int8')\n",
    "    train['day'] = (train.S_2.dt.day).astype('int8')\n",
    "    del train['S_2']\n",
    "        \n",
    "    # LABEL ENCODE CAT COLUMNS (and reduce to 1 byte)\n",
    "    # with 0: padding, 1: nan, 2,3,4,etc: values\n",
    "    d_63_map = {'CL':2, 'CO':3, 'CR':4, 'XL':5, 'XM':6, 'XZ':7}\n",
    "    train['D_63'] = train.D_63.map(d_63_map).fillna(1).astype('int8')\n",
    "\n",
    "    d_64_map = {'-1':2,'O':3, 'R':4, 'U':5}\n",
    "    train['D_64'] = train.D_64.map(d_64_map).fillna(1).astype('int8')\n",
    "    \n",
    "    CATS = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_66', 'D_68']\n",
    "    OFFSETS = [2,1,2,2,3,2,3,2,2] #2 minus minimal value in full train csv\n",
    "    # then 0 will be padding, 1 will be NAN, 2,3,4,etc will be values\n",
    "    for c,s in zip(CATS,OFFSETS):\n",
    "        train[c] = train[c] + s\n",
    "        train[c] = train[c].fillna(1).astype('int8')\n",
    "    CATS += ['D_63','D_64']\n",
    "    \n",
    "    # ADD NEW FEATURES HERE\n",
    "    # EXAMPLE: train['feature_189'] = etc etc etc\n",
    "    # EXAMPLE: train['feature_190'] = etc etc etc\n",
    "    # IF CATEGORICAL, THEN ADD TO CATS WITH: CATS += ['feaure_190'] etc etc etc\n",
    "    \n",
    "    # REDUCE MEMORY DTYPE\n",
    "    SKIP = ['customer_ID','year','month','day']\n",
    "    for c in train.columns:\n",
    "        if c in SKIP: continue\n",
    "        if str( train[c].dtype )=='int64':\n",
    "            train[c] = train[c].astype('int32')\n",
    "        if str( train[c].dtype )=='float64':\n",
    "            train[c] = train[c].astype('float32')\n",
    "            \n",
    "    # PAD ROWS SO EACH CUSTOMER HAS 13 ROWS\n",
    "    if PAD_CUSTOMER_TO_13_ROWS:\n",
    "        tmp = train[['customer_ID']].groupby('customer_ID').customer_ID.agg('count')\n",
    "        more = cupy.array([],dtype='int64') \n",
    "        for j in range(1,13):\n",
    "            i = tmp.loc[tmp==j].index.values\n",
    "            more = cupy.concatenate([more,cupy.repeat(i,13-j)])\n",
    "        df = train.iloc[:len(more)].copy().fillna(0)\n",
    "        df = df * 0 - 1 #pad numerical columns with -1\n",
    "        df[CATS] = (df[CATS] * 0).astype('int8') #pad categorical columns with 0\n",
    "        df['customer_ID'] = more\n",
    "        train = cudf.concat([train,df],axis=0,ignore_index=True)\n",
    "        \n",
    "    # ADD TARGETS (and reduce to 1 byte)\n",
    "    if targets is not None:\n",
    "        train = train.merge(targets,on='customer_ID',how='left')\n",
    "        train.target = train.target.astype('int8')\n",
    "        \n",
    "    # FILL NAN\n",
    "    train = train.fillna(-0.5) #this applies to numerical columns\n",
    "    \n",
    "    # SORT BY CUSTOMER THEN DATE\n",
    "    train = train.sort_values(['customer_ID','year','month','day']).reset_index(drop=True)\n",
    "    train = train.drop(['year','month','day'],axis=1)\n",
    "    \n",
    "    # REARRANGE COLUMNS WITH 11 CATS FIRST\n",
    "    COLS = list(train.columns[1:])\n",
    "    COLS = ['customer_ID'] + CATS + [c for c in COLS if c not in CATS]\n",
    "    train = train[COLS]\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386e8eca",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-10T11:01:30.397909Z",
     "iopub.status.busy": "2022-08-10T11:01:30.397587Z",
     "iopub.status.idle": "2022-08-10T11:01:30.405348Z",
     "shell.execute_reply": "2022-08-10T11:01:30.404286Z"
    },
    "papermill": {
     "duration": 0.016776,
     "end_time": "2022-08-10T11:01:30.407401",
     "exception": false,
     "start_time": "2022-08-10T11:01:30.390625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if PROCESS_DATA:\n",
    "    # CREATE PROCESSED TRAIN FILES AND SAVE TO DISK        \n",
    "    for k in range(NUM_FILES):\n",
    "\n",
    "        # READ CHUNK OF TRAIN CSV FILE\n",
    "        skip = int(np.sum( rows[:k] ) + 1) #the plus one is for skipping header\n",
    "        train = cudf.read_csv('../input/amex-default-prediction/train_data.csv', nrows=rows[k], \n",
    "                              skiprows=skip, header=None, names=T_COLS)\n",
    "\n",
    "        # FEATURE ENGINEER DATAFRAME\n",
    "        train = feature_engineer(train, targets = targets)\n",
    "\n",
    "        # SAVE FILES\n",
    "        print(f'Train_File_{k+1} has {train.customer_ID.nunique()} customers and shape',train.shape)\n",
    "        tar = train[['customer_ID','target']].drop_duplicates().sort_index()\n",
    "        if not os.path.exists(PATH_TO_DATA): os.makedirs(PATH_TO_DATA)\n",
    "        tar.to_parquet(f'{PATH_TO_DATA}targets_{k+1}.pqt',index=False)\n",
    "        data = train.iloc[:,1:-1].values.reshape((-1,13,188))\n",
    "        cupy.save(f'{PATH_TO_DATA}data_{k+1}',data.astype('float32'))\n",
    "\n",
    "    # CLEAN MEMORY\n",
    "    del train, tar, data\n",
    "    del targets\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce6019",
   "metadata": {
    "papermill": {
     "duration": 0.006032,
     "end_time": "2022-08-10T11:01:30.419329",
     "exception": false,
     "start_time": "2022-08-10T11:01:30.413297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build Model\n",
    "We will just input the sequence data into a basic GRU. We will follow that we two dense layers and finally a sigmoid output to predict default. Try improving the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d271ddd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T11:01:30.432888Z",
     "iopub.status.busy": "2022-08-10T11:01:30.432531Z",
     "iopub.status.idle": "2022-08-10T11:01:30.440736Z",
     "shell.execute_reply": "2022-08-10T11:01:30.439617Z"
    },
    "papermill": {
     "duration": 0.017429,
     "end_time": "2022-08-10T11:01:30.442853",
     "exception": false,
     "start_time": "2022-08-10T11:01:30.425424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SIMPLE GRU MODEL\n",
    "class gru_model(nn.Module):\n",
    "    def __init__(self, in_feats, hid_dim=256, activation=nn.ReLU()):\n",
    "        super(gru_model, self).__init__()\n",
    "        self.num_layers = 1\n",
    "        self.hid_dim = hid_dim\n",
    "        self.activation = activation\n",
    "        self.hidden_state = None\n",
    "        self.encode = nn.GRU(input_size=in_feats,\n",
    "                             hidden_size=hid_dim,\n",
    "                             num_layers=self.num_layers,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_dim, 64),\n",
    "                                    self.activation,\n",
    "                                    nn.Linear(64, 32),\n",
    "                                    self.activation)\n",
    "        self.predict = nn.Linear(32, 2)\n",
    "        \n",
    "    def init_hidden(self, batch_size, device=\"cpu\"):\n",
    "        return torch.autograd.Variable(torch.zeros(self.num_layers, batch_size, self.hid_dim)).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, h = self.encode(x, self.hidden_state)\n",
    "        h = self.hidden(torch.squeeze(h))\n",
    "        return self.predict(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b24d9",
   "metadata": {
    "papermill": {
     "duration": 0.005806,
     "end_time": "2022-08-10T11:01:30.456320",
     "exception": false,
     "start_time": "2022-08-10T11:01:30.450514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Competition Metric Code\n",
    "The code below is from Konstantin Yakovlev's discussion post [here][1]\n",
    "\n",
    "[1]: https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1520c949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T11:01:30.469484Z",
     "iopub.status.busy": "2022-08-10T11:01:30.469197Z",
     "iopub.status.idle": "2022-08-10T11:01:30.478703Z",
     "shell.execute_reply": "2022-08-10T11:01:30.477541Z"
    },
    "papermill": {
     "duration": 0.018729,
     "end_time": "2022-08-10T11:01:30.481137",
     "exception": false,
     "start_time": "2022-08-10T11:01:30.462408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COMPETITION METRIC FROM Konstantin Yakovlev\n",
    "# https://www.kaggle.com/kyakovlev\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\n",
    "def amex_metric(y_true, y_pred):\n",
    "\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "    print(\"G: {:.6f}, D: {:.6f}, ALL: {:6f}\".format(gini[1]/gini[0], top_four, 0.5*(gini[1]/gini[0] + top_four)))\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f195261",
   "metadata": {
    "papermill": {
     "duration": 0.006204,
     "end_time": "2022-08-10T11:01:30.493496",
     "exception": false,
     "start_time": "2022-08-10T11:01:30.487292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Early Stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b970391",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T11:01:30.506962Z",
     "iopub.status.busy": "2022-08-10T11:01:30.506588Z",
     "iopub.status.idle": "2022-08-10T11:01:30.516128Z",
     "shell.execute_reply": "2022-08-10T11:01:30.514728Z"
    },
    "papermill": {
     "duration": 0.019246,
     "end_time": "2022-08-10T11:01:30.518750",
     "exception": false,
     "start_time": "2022-08-10T11:01:30.499504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class early_stopper(object):\n",
    "    def __init__(self, patience=12, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_value = None\n",
    "        self.best_cv = None\n",
    "        self.is_earlystop = False\n",
    "        self.count = 0\n",
    "        self.best_model = None\n",
    "        #self.val_preds = []\n",
    "        #self.val_logits = []\n",
    "\n",
    "    def earlystop(self, loss, value, model=None):#, preds, logits):\n",
    "        \"\"\"\n",
    "        value: evaluation value on valiation dataset\n",
    "        \"\"\"\n",
    "        cv = value\n",
    "        if self.best_value is None:\n",
    "            self.best_value = value\n",
    "            self.best_cv = cv\n",
    "            self.best_model = copy.deepcopy(model).to('cpu')\n",
    "            #self.val_preds = preds\n",
    "            #self.val_logits = logits\n",
    "        elif value < self.best_value + self.delta:\n",
    "            self.count += 1\n",
    "            if self.verbose:\n",
    "                print('EarlyStoper count: {:02d}'.format(self.count))\n",
    "            if self.count >= self.patience:\n",
    "                self.is_earlystop = True\n",
    "        else:\n",
    "            self.best_value = value\n",
    "            self.best_cv = cv\n",
    "            self.best_model = copy.deepcopy(model).to('cpu')\n",
    "            #self.val_preds = preds\n",
    "            #self.val_logits = logits\n",
    "            self.count = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da91430",
   "metadata": {
    "papermill": {
     "duration": 0.005965,
     "end_time": "2022-08-10T11:01:30.530779",
     "exception": false,
     "start_time": "2022-08-10T11:01:30.524814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Model\n",
    "We train 5 folds for 8 epochs each. We save the 5 fold models for test inference later. If you only want to infer without training, then set variable `TRAIN_MODEL = False` in the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ef774f",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-10T11:01:30.544332Z",
     "iopub.status.busy": "2022-08-10T11:01:30.544002Z",
     "iopub.status.idle": "2022-08-10T11:08:53.613711Z",
     "shell.execute_reply": "2022-08-10T11:08:53.612406Z"
    },
    "papermill": {
     "duration": 443.081849,
     "end_time": "2022-08-10T11:08:53.618646",
     "exception": false,
     "start_time": "2022-08-10T11:01:30.536797",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1 with valid files [1, 2]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "In epoch:000|batch:0000, train_loss:0.631196, train_ap:0.4643, train_acc:0.7715, train_auc:0.7099\n",
      "In epoch:000|batch:0050, train_loss:0.365448, train_ap:0.8752, train_acc:0.8789, train_auc:0.9475\n",
      "In epoch:000|batch:0100, train_loss:0.310467, train_ap:0.8593, train_acc:0.9121, train_auc:0.9533\n",
      "In epoch:000|batch:0150, train_loss:0.288109, train_ap:0.9057, train_acc:0.9082, train_auc:0.9641\n",
      "In epoch:000|batch:0200, train_loss:0.275844, train_ap:0.8839, train_acc:0.8867, train_auc:0.9502\n",
      "In epoch:000|batch:0250, train_loss:0.267676, train_ap:0.9218, train_acc:0.9199, train_auc:0.9705\n",
      "In epoch:000|batch:0300, train_loss:0.263178, train_ap:0.8938, train_acc:0.8828, train_auc:0.9571\n",
      "In epoch:000|batch:0350, train_loss:0.260079, train_ap:0.8813, train_acc:0.9062, train_auc:0.9571\n",
      "In epoch:000|batch:0400, train_loss:0.256338, train_ap:0.9009, train_acc:0.8984, train_auc:0.9663\n",
      "In epoch:000|batch:0450, train_loss:0.253229, train_ap:0.9096, train_acc:0.9023, train_auc:0.9641\n",
      "In epoch:000|batch:0500, train_loss:0.251525, train_ap:0.8850, train_acc:0.8945, train_auc:0.9521\n",
      "In epoch:000|batch:0550, train_loss:0.249588, train_ap:0.8751, train_acc:0.8848, train_auc:0.9481\n",
      "In epoch:000|batch:0600, train_loss:0.248247, train_ap:0.8912, train_acc:0.9082, train_auc:0.9608\n",
      "In epoch:000|batch:0650, train_loss:0.246991, train_ap:0.8582, train_acc:0.8906, train_auc:0.9542\n",
      "In epoch:000|batch:0700, train_loss:0.245576, train_ap:0.9218, train_acc:0.9258, train_auc:0.9736\n",
      "In epoch:000|batch:0000, val_loss:0.000421, val_ap:0.9139, val_acc:0.9062, val_auc:0.9676\n",
      "In epoch:000|batch:0050, val_loss:0.000469, val_ap:0.8661, val_acc:0.9023, val_auc:0.9587\n",
      "In epoch:000|batch:0100, val_loss:0.000475, val_ap:0.8906, val_acc:0.8906, val_auc:0.9566\n",
      "In epoch:000|batch:0150, val_loss:0.000470, val_ap:0.8686, val_acc:0.8848, val_auc:0.9463\n",
      "G: 0.912580, D: 0.632966, ALL: 0.772773\n",
      "In epoch:001|batch:0000, train_loss:0.202478, train_ap:0.9031, train_acc:0.9199, train_auc:0.9699\n",
      "In epoch:001|batch:0050, train_loss:0.229785, train_ap:0.9045, train_acc:0.8965, train_auc:0.9634\n",
      "In epoch:001|batch:0100, train_loss:0.227923, train_ap:0.9025, train_acc:0.8945, train_auc:0.9628\n",
      "In epoch:001|batch:0150, train_loss:0.230553, train_ap:0.8733, train_acc:0.8828, train_auc:0.9541\n",
      "In epoch:001|batch:0200, train_loss:0.230681, train_ap:0.8693, train_acc:0.8691, train_auc:0.9402\n",
      "In epoch:001|batch:0250, train_loss:0.230260, train_ap:0.8569, train_acc:0.9043, train_auc:0.9484\n",
      "In epoch:001|batch:0300, train_loss:0.230356, train_ap:0.8785, train_acc:0.8672, train_auc:0.9419\n",
      "In epoch:001|batch:0350, train_loss:0.230876, train_ap:0.8744, train_acc:0.8652, train_auc:0.9375\n",
      "In epoch:001|batch:0400, train_loss:0.231027, train_ap:0.9101, train_acc:0.8945, train_auc:0.9608\n",
      "In epoch:001|batch:0450, train_loss:0.230211, train_ap:0.8801, train_acc:0.8848, train_auc:0.9482\n",
      "In epoch:001|batch:0500, train_loss:0.229835, train_ap:0.8649, train_acc:0.8887, train_auc:0.9487\n",
      "In epoch:001|batch:0550, train_loss:0.229688, train_ap:0.8777, train_acc:0.8652, train_auc:0.9450\n",
      "In epoch:001|batch:0600, train_loss:0.229652, train_ap:0.8804, train_acc:0.8887, train_auc:0.9552\n",
      "In epoch:001|batch:0650, train_loss:0.229572, train_ap:0.9000, train_acc:0.9062, train_auc:0.9558\n",
      "In epoch:001|batch:0700, train_loss:0.229454, train_ap:0.9024, train_acc:0.9004, train_auc:0.9612\n",
      "In epoch:001|batch:0000, val_loss:0.000542, val_ap:0.8557, val_acc:0.8965, val_auc:0.9446\n",
      "In epoch:001|batch:0050, val_loss:0.000469, val_ap:0.9161, val_acc:0.9141, val_auc:0.9635\n",
      "In epoch:001|batch:0100, val_loss:0.000467, val_ap:0.8822, val_acc:0.9180, val_auc:0.9598\n",
      "In epoch:001|batch:0150, val_loss:0.000464, val_ap:0.8728, val_acc:0.8770, val_auc:0.9476\n",
      "G: 0.914376, D: 0.639285, ALL: 0.776831\n",
      "In epoch:002|batch:0000, train_loss:0.206518, train_ap:0.9249, train_acc:0.9121, train_auc:0.9698\n",
      "In epoch:002|batch:0050, train_loss:0.227569, train_ap:0.8966, train_acc:0.8867, train_auc:0.9571\n",
      "In epoch:002|batch:0100, train_loss:0.224875, train_ap:0.9076, train_acc:0.9062, train_auc:0.9615\n",
      "In epoch:002|batch:0150, train_loss:0.225266, train_ap:0.8809, train_acc:0.8848, train_auc:0.9506\n",
      "In epoch:002|batch:0200, train_loss:0.225709, train_ap:0.8618, train_acc:0.8965, train_auc:0.9520\n",
      "In epoch:002|batch:0250, train_loss:0.227105, train_ap:0.8838, train_acc:0.8848, train_auc:0.9536\n",
      "In epoch:002|batch:0300, train_loss:0.226973, train_ap:0.8853, train_acc:0.8887, train_auc:0.9561\n",
      "In epoch:002|batch:0350, train_loss:0.227911, train_ap:0.8877, train_acc:0.9121, train_auc:0.9649\n",
      "In epoch:002|batch:0400, train_loss:0.227486, train_ap:0.8678, train_acc:0.8867, train_auc:0.9444\n",
      "In epoch:002|batch:0450, train_loss:0.227457, train_ap:0.8797, train_acc:0.9199, train_auc:0.9660\n",
      "In epoch:002|batch:0500, train_loss:0.227519, train_ap:0.9029, train_acc:0.8711, train_auc:0.9586\n",
      "In epoch:002|batch:0550, train_loss:0.226838, train_ap:0.9416, train_acc:0.9102, train_auc:0.9686\n",
      "In epoch:002|batch:0600, train_loss:0.226905, train_ap:0.8795, train_acc:0.8906, train_auc:0.9572\n",
      "In epoch:002|batch:0650, train_loss:0.226900, train_ap:0.8801, train_acc:0.8965, train_auc:0.9560\n",
      "In epoch:002|batch:0700, train_loss:0.226885, train_ap:0.9181, train_acc:0.9121, train_auc:0.9660\n",
      "In epoch:002|batch:0000, val_loss:0.000541, val_ap:0.8978, val_acc:0.8867, val_auc:0.9470\n",
      "In epoch:002|batch:0050, val_loss:0.000472, val_ap:0.9120, val_acc:0.8984, val_auc:0.9539\n",
      "In epoch:002|batch:0100, val_loss:0.000474, val_ap:0.8889, val_acc:0.8926, val_auc:0.9608\n",
      "In epoch:002|batch:0150, val_loss:0.000475, val_ap:0.9141, val_acc:0.9199, val_auc:0.9753\n",
      "G: 0.914785, D: 0.640122, ALL: 0.777454\n",
      "In epoch:003|batch:0000, train_loss:0.219236, train_ap:0.9125, train_acc:0.9160, train_auc:0.9654\n",
      "In epoch:003|batch:0050, train_loss:0.229731, train_ap:0.9103, train_acc:0.8984, train_auc:0.9591\n",
      "In epoch:003|batch:0100, train_loss:0.226926, train_ap:0.9076, train_acc:0.9082, train_auc:0.9655\n",
      "In epoch:003|batch:0150, train_loss:0.226351, train_ap:0.8591, train_acc:0.8828, train_auc:0.9527\n",
      "In epoch:003|batch:0200, train_loss:0.225004, train_ap:0.9060, train_acc:0.9199, train_auc:0.9641\n",
      "In epoch:003|batch:0250, train_loss:0.224145, train_ap:0.8904, train_acc:0.8906, train_auc:0.9565\n",
      "In epoch:003|batch:0300, train_loss:0.223370, train_ap:0.9250, train_acc:0.9141, train_auc:0.9655\n",
      "In epoch:003|batch:0350, train_loss:0.223788, train_ap:0.8646, train_acc:0.9082, train_auc:0.9600\n",
      "In epoch:003|batch:0400, train_loss:0.224410, train_ap:0.8615, train_acc:0.8770, train_auc:0.9513\n",
      "In epoch:003|batch:0450, train_loss:0.224291, train_ap:0.8791, train_acc:0.8906, train_auc:0.9515\n",
      "In epoch:003|batch:0500, train_loss:0.224638, train_ap:0.8915, train_acc:0.8965, train_auc:0.9586\n",
      "In epoch:003|batch:0550, train_loss:0.224411, train_ap:0.9042, train_acc:0.8984, train_auc:0.9650\n",
      "In epoch:003|batch:0600, train_loss:0.224705, train_ap:0.8420, train_acc:0.8809, train_auc:0.9482\n",
      "In epoch:003|batch:0650, train_loss:0.224826, train_ap:0.8978, train_acc:0.8926, train_auc:0.9590\n",
      "In epoch:003|batch:0700, train_loss:0.225178, train_ap:0.8731, train_acc:0.8770, train_auc:0.9478\n",
      "In epoch:003|batch:0000, val_loss:0.000451, val_ap:0.8846, val_acc:0.8945, val_auc:0.9551\n",
      "In epoch:003|batch:0050, val_loss:0.000451, val_ap:0.9132, val_acc:0.8945, val_auc:0.9566\n",
      "In epoch:003|batch:0100, val_loss:0.000451, val_ap:0.9034, val_acc:0.9141, val_auc:0.9670\n",
      "In epoch:003|batch:0150, val_loss:0.000451, val_ap:0.8770, val_acc:0.8906, val_auc:0.9569\n",
      "G: 0.916843, D: 0.646274, ALL: 0.781558\n",
      "In epoch:004|batch:0000, train_loss:0.185283, train_ap:0.9165, train_acc:0.9277, train_auc:0.9714\n",
      "In epoch:004|batch:0050, train_loss:0.221371, train_ap:0.8113, train_acc:0.8848, train_auc:0.9337\n",
      "In epoch:004|batch:0100, train_loss:0.221293, train_ap:0.9032, train_acc:0.9199, train_auc:0.9626\n",
      "In epoch:004|batch:0150, train_loss:0.222341, train_ap:0.8963, train_acc:0.9180, train_auc:0.9651\n",
      "In epoch:004|batch:0200, train_loss:0.222806, train_ap:0.9105, train_acc:0.9043, train_auc:0.9634\n",
      "In epoch:004|batch:0250, train_loss:0.222774, train_ap:0.9232, train_acc:0.9043, train_auc:0.9713\n",
      "In epoch:004|batch:0300, train_loss:0.222268, train_ap:0.8973, train_acc:0.8945, train_auc:0.9640\n",
      "In epoch:004|batch:0350, train_loss:0.222599, train_ap:0.9167, train_acc:0.8945, train_auc:0.9654\n",
      "In epoch:004|batch:0400, train_loss:0.222310, train_ap:0.8899, train_acc:0.9082, train_auc:0.9591\n",
      "In epoch:004|batch:0450, train_loss:0.221685, train_ap:0.9292, train_acc:0.9277, train_auc:0.9698\n",
      "In epoch:004|batch:0500, train_loss:0.221944, train_ap:0.9238, train_acc:0.9180, train_auc:0.9758\n",
      "In epoch:004|batch:0550, train_loss:0.222094, train_ap:0.8828, train_acc:0.8848, train_auc:0.9519\n",
      "In epoch:004|batch:0600, train_loss:0.222572, train_ap:0.8731, train_acc:0.9023, train_auc:0.9528\n",
      "In epoch:004|batch:0650, train_loss:0.222268, train_ap:0.8722, train_acc:0.9062, train_auc:0.9609\n",
      "In epoch:004|batch:0700, train_loss:0.222609, train_ap:0.9007, train_acc:0.8965, train_auc:0.9556\n",
      "In epoch:004|batch:0000, val_loss:0.000418, val_ap:0.9310, val_acc:0.9238, val_auc:0.9686\n",
      "In epoch:004|batch:0050, val_loss:0.000442, val_ap:0.8869, val_acc:0.9062, val_auc:0.9621\n",
      "In epoch:004|batch:0100, val_loss:0.000449, val_ap:0.9053, val_acc:0.8887, val_auc:0.9585\n",
      "In epoch:004|batch:0150, val_loss:0.000447, val_ap:0.8189, val_acc:0.8730, val_auc:0.9405\n",
      "G: 0.917521, D: 0.647194, ALL: 0.782358\n",
      "In epoch:005|batch:0000, train_loss:0.251150, train_ap:0.8694, train_acc:0.8906, train_auc:0.9500\n",
      "In epoch:005|batch:0050, train_loss:0.225817, train_ap:0.8954, train_acc:0.9043, train_auc:0.9629\n",
      "In epoch:005|batch:0100, train_loss:0.221184, train_ap:0.8857, train_acc:0.9023, train_auc:0.9505\n",
      "In epoch:005|batch:0150, train_loss:0.218858, train_ap:0.9330, train_acc:0.9258, train_auc:0.9751\n",
      "In epoch:005|batch:0200, train_loss:0.219565, train_ap:0.8858, train_acc:0.9102, train_auc:0.9590\n",
      "In epoch:005|batch:0250, train_loss:0.218527, train_ap:0.9120, train_acc:0.9121, train_auc:0.9695\n",
      "In epoch:005|batch:0300, train_loss:0.217903, train_ap:0.8791, train_acc:0.9004, train_auc:0.9562\n",
      "In epoch:005|batch:0350, train_loss:0.217654, train_ap:0.8794, train_acc:0.9043, train_auc:0.9578\n",
      "In epoch:005|batch:0400, train_loss:0.217546, train_ap:0.8864, train_acc:0.9062, train_auc:0.9556\n",
      "In epoch:005|batch:0450, train_loss:0.217442, train_ap:0.9024, train_acc:0.9062, train_auc:0.9625\n",
      "In epoch:005|batch:0500, train_loss:0.217803, train_ap:0.8717, train_acc:0.9004, train_auc:0.9580\n",
      "In epoch:005|batch:0550, train_loss:0.217829, train_ap:0.9337, train_acc:0.9297, train_auc:0.9726\n",
      "In epoch:005|batch:0600, train_loss:0.217615, train_ap:0.8964, train_acc:0.9043, train_auc:0.9605\n",
      "In epoch:005|batch:0650, train_loss:0.217499, train_ap:0.8831, train_acc:0.9004, train_auc:0.9498\n",
      "In epoch:005|batch:0700, train_loss:0.217727, train_ap:0.8455, train_acc:0.8789, train_auc:0.9455\n",
      "In epoch:005|batch:0000, val_loss:0.000455, val_ap:0.9058, val_acc:0.8926, val_auc:0.9557\n",
      "In epoch:005|batch:0050, val_loss:0.000442, val_ap:0.8978, val_acc:0.8887, val_auc:0.9577\n",
      "In epoch:005|batch:0100, val_loss:0.000435, val_ap:0.9223, val_acc:0.9395, val_auc:0.9764\n",
      "In epoch:005|batch:0150, val_loss:0.000438, val_ap:0.8856, val_acc:0.8848, val_auc:0.9554\n",
      "G: 0.918457, D: 0.648994, ALL: 0.783725\n",
      "In epoch:006|batch:0000, train_loss:0.193616, train_ap:0.9159, train_acc:0.9043, train_auc:0.9703\n",
      "In epoch:006|batch:0050, train_loss:0.216077, train_ap:0.8609, train_acc:0.9004, train_auc:0.9502\n",
      "In epoch:006|batch:0100, train_loss:0.217927, train_ap:0.8805, train_acc:0.9082, train_auc:0.9672\n",
      "In epoch:006|batch:0150, train_loss:0.218367, train_ap:0.8805, train_acc:0.8965, train_auc:0.9576\n",
      "In epoch:006|batch:0200, train_loss:0.216645, train_ap:0.9331, train_acc:0.9082, train_auc:0.9678\n",
      "In epoch:006|batch:0250, train_loss:0.217199, train_ap:0.9036, train_acc:0.8945, train_auc:0.9596\n",
      "In epoch:006|batch:0300, train_loss:0.216769, train_ap:0.9247, train_acc:0.9141, train_auc:0.9752\n",
      "In epoch:006|batch:0350, train_loss:0.217116, train_ap:0.8889, train_acc:0.8926, train_auc:0.9537\n",
      "In epoch:006|batch:0400, train_loss:0.217236, train_ap:0.8908, train_acc:0.8848, train_auc:0.9604\n",
      "In epoch:006|batch:0450, train_loss:0.216851, train_ap:0.9331, train_acc:0.9219, train_auc:0.9738\n",
      "In epoch:006|batch:0500, train_loss:0.216582, train_ap:0.8911, train_acc:0.8887, train_auc:0.9594\n",
      "In epoch:006|batch:0550, train_loss:0.216461, train_ap:0.9049, train_acc:0.8867, train_auc:0.9529\n",
      "In epoch:006|batch:0600, train_loss:0.216238, train_ap:0.8819, train_acc:0.8867, train_auc:0.9503\n",
      "In epoch:006|batch:0650, train_loss:0.216492, train_ap:0.8847, train_acc:0.9023, train_auc:0.9613\n",
      "In epoch:006|batch:0700, train_loss:0.216499, train_ap:0.9102, train_acc:0.8926, train_auc:0.9634\n",
      "In epoch:006|batch:0000, val_loss:0.000441, val_ap:0.9029, val_acc:0.9004, val_auc:0.9586\n",
      "In epoch:006|batch:0050, val_loss:0.000443, val_ap:0.8827, val_acc:0.8984, val_auc:0.9544\n",
      "In epoch:006|batch:0100, val_loss:0.000437, val_ap:0.9182, val_acc:0.9082, val_auc:0.9630\n",
      "In epoch:006|batch:0150, val_loss:0.000436, val_ap:0.8759, val_acc:0.9121, val_auc:0.9516\n",
      "G: 0.918509, D: 0.649705, ALL: 0.784107\n",
      "In epoch:007|batch:0000, train_loss:0.177786, train_ap:0.9399, train_acc:0.9336, train_auc:0.9762\n",
      "In epoch:007|batch:0050, train_loss:0.212071, train_ap:0.8870, train_acc:0.9121, train_auc:0.9617\n",
      "In epoch:007|batch:0100, train_loss:0.214192, train_ap:0.9092, train_acc:0.9023, train_auc:0.9638\n",
      "In epoch:007|batch:0150, train_loss:0.214679, train_ap:0.8988, train_acc:0.9082, train_auc:0.9602\n",
      "In epoch:007|batch:0200, train_loss:0.215452, train_ap:0.8901, train_acc:0.8945, train_auc:0.9559\n",
      "In epoch:007|batch:0250, train_loss:0.215514, train_ap:0.8942, train_acc:0.8906, train_auc:0.9601\n",
      "In epoch:007|batch:0300, train_loss:0.215212, train_ap:0.8801, train_acc:0.9004, train_auc:0.9563\n",
      "In epoch:007|batch:0350, train_loss:0.215928, train_ap:0.9151, train_acc:0.9043, train_auc:0.9600\n",
      "In epoch:007|batch:0400, train_loss:0.216003, train_ap:0.8835, train_acc:0.8906, train_auc:0.9578\n",
      "In epoch:007|batch:0450, train_loss:0.215743, train_ap:0.8978, train_acc:0.9043, train_auc:0.9626\n",
      "In epoch:007|batch:0500, train_loss:0.215823, train_ap:0.8887, train_acc:0.9082, train_auc:0.9503\n",
      "In epoch:007|batch:0550, train_loss:0.215778, train_ap:0.8342, train_acc:0.8672, train_auc:0.9318\n",
      "In epoch:007|batch:0600, train_loss:0.215578, train_ap:0.8189, train_acc:0.8867, train_auc:0.9505\n",
      "In epoch:007|batch:0650, train_loss:0.215439, train_ap:0.8987, train_acc:0.9082, train_auc:0.9638\n",
      "In epoch:007|batch:0700, train_loss:0.215681, train_ap:0.9255, train_acc:0.9043, train_auc:0.9662\n",
      "In epoch:007|batch:0000, val_loss:0.000398, val_ap:0.9184, val_acc:0.9082, val_auc:0.9664\n",
      "In epoch:007|batch:0050, val_loss:0.000439, val_ap:0.8304, val_acc:0.8809, val_auc:0.9486\n",
      "In epoch:007|batch:0100, val_loss:0.000438, val_ap:0.8922, val_acc:0.8945, val_auc:0.9570\n",
      "In epoch:007|batch:0150, val_loss:0.000437, val_ap:0.8855, val_acc:0.9043, val_auc:0.9556\n",
      "G: 0.918676, D: 0.650249, ALL: 0.784462\n",
      "In epoch:008|batch:0000, train_loss:0.211966, train_ap:0.9151, train_acc:0.9160, train_auc:0.9655\n",
      "In epoch:008|batch:0050, train_loss:0.218185, train_ap:0.8944, train_acc:0.8906, train_auc:0.9537\n",
      "In epoch:008|batch:0100, train_loss:0.215765, train_ap:0.8880, train_acc:0.8945, train_auc:0.9592\n",
      "In epoch:008|batch:0150, train_loss:0.216144, train_ap:0.8734, train_acc:0.8984, train_auc:0.9507\n",
      "In epoch:008|batch:0200, train_loss:0.215012, train_ap:0.9099, train_acc:0.8984, train_auc:0.9614\n",
      "In epoch:008|batch:0250, train_loss:0.215046, train_ap:0.9054, train_acc:0.9238, train_auc:0.9677\n",
      "In epoch:008|batch:0300, train_loss:0.215101, train_ap:0.8764, train_acc:0.9004, train_auc:0.9509\n",
      "In epoch:008|batch:0350, train_loss:0.215375, train_ap:0.9155, train_acc:0.9141, train_auc:0.9659\n",
      "In epoch:008|batch:0400, train_loss:0.214776, train_ap:0.9077, train_acc:0.8945, train_auc:0.9650\n",
      "In epoch:008|batch:0450, train_loss:0.214943, train_ap:0.8980, train_acc:0.9199, train_auc:0.9644\n",
      "In epoch:008|batch:0500, train_loss:0.215234, train_ap:0.8995, train_acc:0.9043, train_auc:0.9565\n",
      "In epoch:008|batch:0550, train_loss:0.215378, train_ap:0.8963, train_acc:0.8789, train_auc:0.9540\n",
      "In epoch:008|batch:0600, train_loss:0.215644, train_ap:0.8640, train_acc:0.9004, train_auc:0.9526\n",
      "In epoch:008|batch:0650, train_loss:0.215586, train_ap:0.8746, train_acc:0.9043, train_auc:0.9548\n",
      "In epoch:008|batch:0700, train_loss:0.215544, train_ap:0.8969, train_acc:0.9102, train_auc:0.9586\n",
      "In epoch:008|batch:0000, val_loss:0.000469, val_ap:0.8739, val_acc:0.8848, val_auc:0.9512\n",
      "In epoch:008|batch:0050, val_loss:0.000446, val_ap:0.8910, val_acc:0.8906, val_auc:0.9540\n",
      "In epoch:008|batch:0100, val_loss:0.000440, val_ap:0.9111, val_acc:0.9023, val_auc:0.9662\n",
      "In epoch:008|batch:0150, val_loss:0.000437, val_ap:0.8997, val_acc:0.9043, val_auc:0.9628\n",
      "G: 0.918697, D: 0.649245, ALL: 0.783971\n",
      "EarlyStoper count: 01\n",
      "In epoch:009|batch:0000, train_loss:0.182742, train_ap:0.9196, train_acc:0.9219, train_auc:0.9732\n",
      "In epoch:009|batch:0050, train_loss:0.215459, train_ap:0.9326, train_acc:0.9180, train_auc:0.9745\n",
      "In epoch:009|batch:0100, train_loss:0.215650, train_ap:0.8768, train_acc:0.8945, train_auc:0.9625\n",
      "In epoch:009|batch:0150, train_loss:0.216064, train_ap:0.8990, train_acc:0.9082, train_auc:0.9638\n",
      "In epoch:009|batch:0200, train_loss:0.215402, train_ap:0.9033, train_acc:0.9043, train_auc:0.9625\n",
      "In epoch:009|batch:0250, train_loss:0.215599, train_ap:0.9067, train_acc:0.9102, train_auc:0.9616\n",
      "In epoch:009|batch:0300, train_loss:0.216226, train_ap:0.8883, train_acc:0.8828, train_auc:0.9550\n",
      "In epoch:009|batch:0350, train_loss:0.216159, train_ap:0.8973, train_acc:0.8887, train_auc:0.9631\n",
      "In epoch:009|batch:0400, train_loss:0.215946, train_ap:0.9216, train_acc:0.9141, train_auc:0.9696\n",
      "In epoch:009|batch:0450, train_loss:0.215893, train_ap:0.8955, train_acc:0.9043, train_auc:0.9634\n",
      "In epoch:009|batch:0500, train_loss:0.216047, train_ap:0.9032, train_acc:0.8965, train_auc:0.9589\n",
      "In epoch:009|batch:0550, train_loss:0.215748, train_ap:0.8809, train_acc:0.9004, train_auc:0.9577\n",
      "In epoch:009|batch:0600, train_loss:0.215364, train_ap:0.9081, train_acc:0.9121, train_auc:0.9684\n",
      "In epoch:009|batch:0650, train_loss:0.215374, train_ap:0.8944, train_acc:0.9062, train_auc:0.9646\n",
      "In epoch:009|batch:0700, train_loss:0.215528, train_ap:0.8397, train_acc:0.8809, train_auc:0.9462\n",
      "In epoch:009|batch:0000, val_loss:0.000451, val_ap:0.8944, val_acc:0.8906, val_auc:0.9563\n",
      "In epoch:009|batch:0050, val_loss:0.000440, val_ap:0.8810, val_acc:0.8809, val_auc:0.9526\n",
      "In epoch:009|batch:0100, val_loss:0.000441, val_ap:0.9007, val_acc:0.8984, val_auc:0.9604\n",
      "In epoch:009|batch:0150, val_loss:0.000438, val_ap:0.9031, val_acc:0.9102, val_auc:0.9688\n",
      "G: 0.918699, D: 0.648952, ALL: 0.783825\n",
      "EarlyStoper count: 02\n",
      "In epoch:010|batch:0000, train_loss:0.236450, train_ap:0.8899, train_acc:0.8926, train_auc:0.9540\n",
      "In epoch:010|batch:0050, train_loss:0.217014, train_ap:0.8924, train_acc:0.8984, train_auc:0.9595\n",
      "In epoch:010|batch:0100, train_loss:0.214460, train_ap:0.8994, train_acc:0.9121, train_auc:0.9669\n",
      "In epoch:010|batch:0150, train_loss:0.215290, train_ap:0.8952, train_acc:0.9023, train_auc:0.9593\n",
      "In epoch:010|batch:0200, train_loss:0.215345, train_ap:0.8919, train_acc:0.8965, train_auc:0.9538\n",
      "In epoch:010|batch:0250, train_loss:0.214385, train_ap:0.8801, train_acc:0.8906, train_auc:0.9553\n",
      "In epoch:010|batch:0300, train_loss:0.215235, train_ap:0.8986, train_acc:0.9121, train_auc:0.9685\n",
      "In epoch:010|batch:0350, train_loss:0.215436, train_ap:0.9071, train_acc:0.9043, train_auc:0.9643\n",
      "In epoch:010|batch:0400, train_loss:0.216136, train_ap:0.9036, train_acc:0.9121, train_auc:0.9623\n",
      "In epoch:010|batch:0450, train_loss:0.215933, train_ap:0.9063, train_acc:0.8965, train_auc:0.9612\n",
      "In epoch:010|batch:0500, train_loss:0.215359, train_ap:0.9313, train_acc:0.9102, train_auc:0.9726\n",
      "In epoch:010|batch:0550, train_loss:0.215548, train_ap:0.8983, train_acc:0.9121, train_auc:0.9601\n",
      "In epoch:010|batch:0600, train_loss:0.215757, train_ap:0.9087, train_acc:0.9082, train_auc:0.9641\n",
      "In epoch:010|batch:0650, train_loss:0.215952, train_ap:0.8926, train_acc:0.8887, train_auc:0.9571\n",
      "In epoch:010|batch:0700, train_loss:0.215430, train_ap:0.9097, train_acc:0.9102, train_auc:0.9657\n",
      "In epoch:010|batch:0000, val_loss:0.000461, val_ap:0.8805, val_acc:0.9004, val_auc:0.9559\n",
      "In epoch:010|batch:0050, val_loss:0.000432, val_ap:0.9064, val_acc:0.9043, val_auc:0.9607\n",
      "In epoch:010|batch:0100, val_loss:0.000436, val_ap:0.9032, val_acc:0.8887, val_auc:0.9546\n",
      "In epoch:010|batch:0150, val_loss:0.000437, val_ap:0.8624, val_acc:0.8809, val_auc:0.9424\n",
      "G: 0.918702, D: 0.648742, ALL: 0.783722\n",
      "EarlyStoper count: 03\n",
      "In epoch:011|batch:0000, train_loss:0.204469, train_ap:0.9194, train_acc:0.9160, train_auc:0.9690\n",
      "In epoch:011|batch:0050, train_loss:0.214702, train_ap:0.8855, train_acc:0.9082, train_auc:0.9603\n",
      "In epoch:011|batch:0100, train_loss:0.213146, train_ap:0.9007, train_acc:0.9102, train_auc:0.9669\n",
      "In epoch:011|batch:0150, train_loss:0.215827, train_ap:0.8949, train_acc:0.8848, train_auc:0.9570\n",
      "In epoch:011|batch:0200, train_loss:0.216409, train_ap:0.8888, train_acc:0.8984, train_auc:0.9567\n",
      "In epoch:011|batch:0250, train_loss:0.216614, train_ap:0.8921, train_acc:0.8828, train_auc:0.9591\n",
      "In epoch:011|batch:0300, train_loss:0.216900, train_ap:0.9229, train_acc:0.9102, train_auc:0.9682\n",
      "In epoch:011|batch:0350, train_loss:0.215720, train_ap:0.8805, train_acc:0.9102, train_auc:0.9640\n",
      "In epoch:011|batch:0400, train_loss:0.215970, train_ap:0.9173, train_acc:0.9082, train_auc:0.9644\n",
      "In epoch:011|batch:0450, train_loss:0.215428, train_ap:0.9275, train_acc:0.9199, train_auc:0.9702\n",
      "In epoch:011|batch:0500, train_loss:0.215359, train_ap:0.9221, train_acc:0.9277, train_auc:0.9711\n",
      "In epoch:011|batch:0550, train_loss:0.215242, train_ap:0.9109, train_acc:0.8984, train_auc:0.9601\n",
      "In epoch:011|batch:0600, train_loss:0.215404, train_ap:0.9255, train_acc:0.9238, train_auc:0.9737\n",
      "In epoch:011|batch:0650, train_loss:0.215301, train_ap:0.8849, train_acc:0.9082, train_auc:0.9627\n",
      "In epoch:011|batch:0700, train_loss:0.215443, train_ap:0.8950, train_acc:0.8926, train_auc:0.9595\n",
      "In epoch:011|batch:0000, val_loss:0.000456, val_ap:0.8979, val_acc:0.8848, val_auc:0.9564\n",
      "In epoch:011|batch:0050, val_loss:0.000432, val_ap:0.8981, val_acc:0.9121, val_auc:0.9682\n",
      "In epoch:011|batch:0100, val_loss:0.000441, val_ap:0.8868, val_acc:0.8945, val_auc:0.9558\n",
      "In epoch:011|batch:0150, val_loss:0.000439, val_ap:0.8756, val_acc:0.8848, val_auc:0.9464\n",
      "G: 0.918705, D: 0.648952, ALL: 0.783828\n",
      "EarlyStoper count: 04\n",
      "Early Stopping!\n",
      "Best val_metric is: 0.7844623\n",
      "Inferring validation data...\n",
      "In epoch:011|batch:0000, val_loss:0.000502, val_ap:0.8718, val_acc:0.8789, val_auc:0.9454\n",
      "In epoch:011|batch:0050, val_loss:0.000439, val_ap:0.8753, val_acc:0.9023, val_auc:0.9556\n",
      "In epoch:011|batch:0100, val_loss:0.000441, val_ap:0.9116, val_acc:0.9043, val_auc:0.9656\n",
      "In epoch:011|batch:0150, val_loss:0.000437, val_ap:0.9388, val_acc:0.9336, val_auc:0.9767\n",
      "\n",
      "G: 0.918676, D: 0.650249, ALL: 0.784462\n",
      "Fold 1 CV= 0.7844622992049103\n",
      "\n",
      "#########################\n",
      "### Fold 2 with valid files [3, 4]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "In epoch:000|batch:0000, train_loss:0.769385, train_ap:0.5400, train_acc:0.2559, train_auc:0.7318\n",
      "In epoch:000|batch:0050, train_loss:0.375034, train_ap:0.8952, train_acc:0.9121, train_auc:0.9573\n",
      "In epoch:000|batch:0100, train_loss:0.314410, train_ap:0.8646, train_acc:0.8867, train_auc:0.9436\n",
      "In epoch:000|batch:0150, train_loss:0.291010, train_ap:0.8528, train_acc:0.8711, train_auc:0.9488\n",
      "In epoch:000|batch:0200, train_loss:0.279564, train_ap:0.8824, train_acc:0.8848, train_auc:0.9543\n",
      "In epoch:000|batch:0250, train_loss:0.270480, train_ap:0.9000, train_acc:0.9004, train_auc:0.9622\n",
      "In epoch:000|batch:0300, train_loss:0.264161, train_ap:0.8985, train_acc:0.8926, train_auc:0.9612\n",
      "In epoch:000|batch:0350, train_loss:0.259979, train_ap:0.8894, train_acc:0.9121, train_auc:0.9537\n",
      "In epoch:000|batch:0400, train_loss:0.256826, train_ap:0.8589, train_acc:0.8965, train_auc:0.9504\n",
      "In epoch:000|batch:0450, train_loss:0.254344, train_ap:0.8602, train_acc:0.8906, train_auc:0.9459\n",
      "In epoch:000|batch:0500, train_loss:0.252388, train_ap:0.8907, train_acc:0.9023, train_auc:0.9592\n",
      "In epoch:000|batch:0550, train_loss:0.251045, train_ap:0.8971, train_acc:0.8965, train_auc:0.9539\n",
      "In epoch:000|batch:0600, train_loss:0.249512, train_ap:0.9184, train_acc:0.9141, train_auc:0.9691\n",
      "In epoch:000|batch:0650, train_loss:0.248329, train_ap:0.8405, train_acc:0.8887, train_auc:0.9470\n",
      "In epoch:000|batch:0700, train_loss:0.247230, train_ap:0.9084, train_acc:0.8906, train_auc:0.9633\n",
      "In epoch:000|batch:0000, val_loss:0.000433, val_ap:0.8818, val_acc:0.8965, val_auc:0.9590\n",
      "In epoch:000|batch:0050, val_loss:0.000454, val_ap:0.8773, val_acc:0.9102, val_auc:0.9644\n",
      "In epoch:000|batch:0100, val_loss:0.000457, val_ap:0.8812, val_acc:0.8848, val_auc:0.9503\n",
      "In epoch:000|batch:0150, val_loss:0.000456, val_ap:0.8367, val_acc:0.8984, val_auc:0.9434\n",
      "G: 0.911700, D: 0.632396, ALL: 0.772048\n",
      "In epoch:001|batch:0000, train_loss:0.234826, train_ap:0.8962, train_acc:0.8945, train_auc:0.9536\n",
      "In epoch:001|batch:0050, train_loss:0.224964, train_ap:0.8671, train_acc:0.9023, train_auc:0.9546\n",
      "In epoch:001|batch:0100, train_loss:0.227537, train_ap:0.9267, train_acc:0.8984, train_auc:0.9713\n",
      "In epoch:001|batch:0150, train_loss:0.229392, train_ap:0.8648, train_acc:0.8828, train_auc:0.9456\n",
      "In epoch:001|batch:0200, train_loss:0.230449, train_ap:0.9038, train_acc:0.9062, train_auc:0.9624\n",
      "In epoch:001|batch:0250, train_loss:0.230422, train_ap:0.8866, train_acc:0.8965, train_auc:0.9535\n",
      "In epoch:001|batch:0300, train_loss:0.228726, train_ap:0.9112, train_acc:0.9023, train_auc:0.9642\n",
      "In epoch:001|batch:0350, train_loss:0.228513, train_ap:0.8521, train_acc:0.8945, train_auc:0.9449\n",
      "In epoch:001|batch:0400, train_loss:0.228473, train_ap:0.8832, train_acc:0.8809, train_auc:0.9496\n",
      "In epoch:001|batch:0450, train_loss:0.228400, train_ap:0.8811, train_acc:0.8887, train_auc:0.9580\n",
      "In epoch:001|batch:0500, train_loss:0.228473, train_ap:0.8961, train_acc:0.9121, train_auc:0.9610\n",
      "In epoch:001|batch:0550, train_loss:0.229103, train_ap:0.8224, train_acc:0.8828, train_auc:0.9439\n",
      "In epoch:001|batch:0600, train_loss:0.229740, train_ap:0.8716, train_acc:0.8887, train_auc:0.9517\n",
      "In epoch:001|batch:0650, train_loss:0.229925, train_ap:0.8591, train_acc:0.8965, train_auc:0.9481\n",
      "In epoch:001|batch:0700, train_loss:0.229717, train_ap:0.8899, train_acc:0.9043, train_auc:0.9616\n",
      "In epoch:001|batch:0000, val_loss:0.000385, val_ap:0.9180, val_acc:0.9180, val_auc:0.9691\n",
      "In epoch:001|batch:0050, val_loss:0.000442, val_ap:0.9080, val_acc:0.9121, val_auc:0.9589\n",
      "In epoch:001|batch:0100, val_loss:0.000448, val_ap:0.8179, val_acc:0.8672, val_auc:0.9356\n",
      "In epoch:001|batch:0150, val_loss:0.000447, val_ap:0.8951, val_acc:0.8711, val_auc:0.9517\n",
      "G: 0.914134, D: 0.633027, ALL: 0.773581\n",
      "In epoch:002|batch:0000, train_loss:0.214263, train_ap:0.9010, train_acc:0.9062, train_auc:0.9633\n",
      "In epoch:002|batch:0050, train_loss:0.230095, train_ap:0.9261, train_acc:0.8887, train_auc:0.9656\n",
      "In epoch:002|batch:0100, train_loss:0.227301, train_ap:0.8495, train_acc:0.8848, train_auc:0.9511\n",
      "In epoch:002|batch:0150, train_loss:0.227750, train_ap:0.8770, train_acc:0.8867, train_auc:0.9530\n",
      "In epoch:002|batch:0200, train_loss:0.227073, train_ap:0.8980, train_acc:0.8770, train_auc:0.9516\n",
      "In epoch:002|batch:0250, train_loss:0.226646, train_ap:0.9168, train_acc:0.9141, train_auc:0.9687\n",
      "In epoch:002|batch:0300, train_loss:0.226406, train_ap:0.8783, train_acc:0.8945, train_auc:0.9567\n",
      "In epoch:002|batch:0350, train_loss:0.226811, train_ap:0.9097, train_acc:0.9082, train_auc:0.9731\n",
      "In epoch:002|batch:0400, train_loss:0.226573, train_ap:0.9035, train_acc:0.8711, train_auc:0.9569\n",
      "In epoch:002|batch:0450, train_loss:0.226776, train_ap:0.8830, train_acc:0.8828, train_auc:0.9522\n",
      "In epoch:002|batch:0500, train_loss:0.226755, train_ap:0.9076, train_acc:0.9062, train_auc:0.9660\n",
      "In epoch:002|batch:0550, train_loss:0.226685, train_ap:0.8687, train_acc:0.8613, train_auc:0.9363\n",
      "In epoch:002|batch:0600, train_loss:0.226198, train_ap:0.8486, train_acc:0.8906, train_auc:0.9483\n",
      "In epoch:002|batch:0650, train_loss:0.226131, train_ap:0.8774, train_acc:0.8867, train_auc:0.9602\n",
      "In epoch:002|batch:0700, train_loss:0.226109, train_ap:0.9236, train_acc:0.9297, train_auc:0.9692\n",
      "In epoch:002|batch:0000, val_loss:0.000469, val_ap:0.8938, val_acc:0.8809, val_auc:0.9548\n",
      "In epoch:002|batch:0050, val_loss:0.000444, val_ap:0.9053, val_acc:0.8926, val_auc:0.9580\n",
      "In epoch:002|batch:0100, val_loss:0.000451, val_ap:0.8324, val_acc:0.8926, val_auc:0.9510\n",
      "In epoch:002|batch:0150, val_loss:0.000449, val_ap:0.8628, val_acc:0.9043, val_auc:0.9483\n",
      "G: 0.915175, D: 0.637278, ALL: 0.776226\n",
      "In epoch:003|batch:0000, train_loss:0.215000, train_ap:0.9192, train_acc:0.9277, train_auc:0.9638\n",
      "In epoch:003|batch:0050, train_loss:0.225868, train_ap:0.8611, train_acc:0.8848, train_auc:0.9526\n",
      "In epoch:003|batch:0100, train_loss:0.227245, train_ap:0.8943, train_acc:0.9062, train_auc:0.9645\n",
      "In epoch:003|batch:0150, train_loss:0.226516, train_ap:0.9023, train_acc:0.9277, train_auc:0.9687\n",
      "In epoch:003|batch:0200, train_loss:0.226220, train_ap:0.8821, train_acc:0.8848, train_auc:0.9560\n",
      "In epoch:003|batch:0250, train_loss:0.225025, train_ap:0.9334, train_acc:0.8965, train_auc:0.9725\n",
      "In epoch:003|batch:0300, train_loss:0.224584, train_ap:0.8627, train_acc:0.8867, train_auc:0.9433\n",
      "In epoch:003|batch:0350, train_loss:0.224556, train_ap:0.8957, train_acc:0.9141, train_auc:0.9653\n",
      "In epoch:003|batch:0400, train_loss:0.225089, train_ap:0.9073, train_acc:0.8906, train_auc:0.9583\n",
      "In epoch:003|batch:0450, train_loss:0.225055, train_ap:0.9411, train_acc:0.9238, train_auc:0.9726\n",
      "In epoch:003|batch:0500, train_loss:0.224736, train_ap:0.8916, train_acc:0.8809, train_auc:0.9576\n",
      "In epoch:003|batch:0550, train_loss:0.224651, train_ap:0.8804, train_acc:0.8926, train_auc:0.9532\n",
      "In epoch:003|batch:0600, train_loss:0.224479, train_ap:0.8887, train_acc:0.8926, train_auc:0.9570\n",
      "In epoch:003|batch:0650, train_loss:0.224305, train_ap:0.8740, train_acc:0.9004, train_auc:0.9582\n",
      "In epoch:003|batch:0700, train_loss:0.224173, train_ap:0.8548, train_acc:0.8730, train_auc:0.9422\n",
      "In epoch:003|batch:0000, val_loss:0.000513, val_ap:0.8720, val_acc:0.8848, val_auc:0.9473\n",
      "In epoch:003|batch:0050, val_loss:0.000444, val_ap:0.8877, val_acc:0.8828, val_auc:0.9549\n",
      "In epoch:003|batch:0100, val_loss:0.000446, val_ap:0.8879, val_acc:0.9004, val_auc:0.9561\n",
      "In epoch:003|batch:0150, val_loss:0.000446, val_ap:0.8876, val_acc:0.8965, val_auc:0.9535\n",
      "G: 0.916242, D: 0.643464, ALL: 0.779853\n",
      "In epoch:004|batch:0000, train_loss:0.234806, train_ap:0.8962, train_acc:0.9004, train_auc:0.9573\n",
      "In epoch:004|batch:0050, train_loss:0.225674, train_ap:0.8656, train_acc:0.8984, train_auc:0.9535\n",
      "In epoch:004|batch:0100, train_loss:0.225247, train_ap:0.9126, train_acc:0.9141, train_auc:0.9649\n",
      "In epoch:004|batch:0150, train_loss:0.223853, train_ap:0.9100, train_acc:0.9023, train_auc:0.9694\n",
      "In epoch:004|batch:0200, train_loss:0.223636, train_ap:0.9071, train_acc:0.9180, train_auc:0.9682\n",
      "In epoch:004|batch:0250, train_loss:0.222887, train_ap:0.9050, train_acc:0.8965, train_auc:0.9613\n",
      "In epoch:004|batch:0300, train_loss:0.223155, train_ap:0.8140, train_acc:0.8750, train_auc:0.9307\n",
      "In epoch:004|batch:0350, train_loss:0.223288, train_ap:0.8870, train_acc:0.8926, train_auc:0.9583\n",
      "In epoch:004|batch:0400, train_loss:0.223336, train_ap:0.9154, train_acc:0.8965, train_auc:0.9649\n",
      "In epoch:004|batch:0450, train_loss:0.223087, train_ap:0.8946, train_acc:0.9277, train_auc:0.9725\n",
      "In epoch:004|batch:0500, train_loss:0.223161, train_ap:0.8979, train_acc:0.9141, train_auc:0.9672\n",
      "In epoch:004|batch:0550, train_loss:0.223310, train_ap:0.9030, train_acc:0.8945, train_auc:0.9664\n",
      "In epoch:004|batch:0600, train_loss:0.223181, train_ap:0.8924, train_acc:0.8906, train_auc:0.9517\n",
      "In epoch:004|batch:0650, train_loss:0.222855, train_ap:0.8991, train_acc:0.9141, train_auc:0.9641\n",
      "In epoch:004|batch:0700, train_loss:0.222890, train_ap:0.9040, train_acc:0.9121, train_auc:0.9609\n",
      "In epoch:004|batch:0000, val_loss:0.000461, val_ap:0.8862, val_acc:0.8887, val_auc:0.9578\n",
      "In epoch:004|batch:0050, val_loss:0.000463, val_ap:0.8834, val_acc:0.9102, val_auc:0.9580\n",
      "In epoch:004|batch:0100, val_loss:0.000461, val_ap:0.8803, val_acc:0.8984, val_auc:0.9560\n",
      "In epoch:004|batch:0150, val_loss:0.000457, val_ap:0.9036, val_acc:0.9141, val_auc:0.9646\n",
      "G: 0.916824, D: 0.644138, ALL: 0.780481\n",
      "In epoch:005|batch:0000, train_loss:0.255388, train_ap:0.8899, train_acc:0.8867, train_auc:0.9506\n",
      "In epoch:005|batch:0050, train_loss:0.219587, train_ap:0.8754, train_acc:0.8809, train_auc:0.9533\n",
      "In epoch:005|batch:0100, train_loss:0.219409, train_ap:0.8854, train_acc:0.9082, train_auc:0.9593\n",
      "In epoch:005|batch:0150, train_loss:0.218167, train_ap:0.8712, train_acc:0.9121, train_auc:0.9618\n",
      "In epoch:005|batch:0200, train_loss:0.219149, train_ap:0.8984, train_acc:0.9062, train_auc:0.9574\n",
      "In epoch:005|batch:0250, train_loss:0.218437, train_ap:0.9098, train_acc:0.9160, train_auc:0.9657\n",
      "In epoch:005|batch:0300, train_loss:0.218174, train_ap:0.8906, train_acc:0.8906, train_auc:0.9575\n",
      "In epoch:005|batch:0350, train_loss:0.218461, train_ap:0.9181, train_acc:0.9121, train_auc:0.9663\n",
      "In epoch:005|batch:0400, train_loss:0.218513, train_ap:0.9077, train_acc:0.8906, train_auc:0.9619\n",
      "In epoch:005|batch:0450, train_loss:0.218119, train_ap:0.9110, train_acc:0.9102, train_auc:0.9642\n",
      "In epoch:005|batch:0500, train_loss:0.218274, train_ap:0.9068, train_acc:0.9062, train_auc:0.9719\n",
      "In epoch:005|batch:0550, train_loss:0.218043, train_ap:0.8889, train_acc:0.9062, train_auc:0.9617\n",
      "In epoch:005|batch:0600, train_loss:0.217821, train_ap:0.8832, train_acc:0.9004, train_auc:0.9601\n",
      "In epoch:005|batch:0650, train_loss:0.217744, train_ap:0.9147, train_acc:0.9004, train_auc:0.9663\n",
      "In epoch:005|batch:0700, train_loss:0.217846, train_ap:0.9247, train_acc:0.9141, train_auc:0.9697\n",
      "In epoch:005|batch:0000, val_loss:0.000407, val_ap:0.9021, val_acc:0.9277, val_auc:0.9600\n",
      "In epoch:005|batch:0050, val_loss:0.000451, val_ap:0.8856, val_acc:0.8926, val_auc:0.9535\n",
      "In epoch:005|batch:0100, val_loss:0.000443, val_ap:0.9025, val_acc:0.9199, val_auc:0.9698\n",
      "In epoch:005|batch:0150, val_loss:0.000440, val_ap:0.9096, val_acc:0.9160, val_auc:0.9709\n",
      "G: 0.917955, D: 0.645821, ALL: 0.781888\n",
      "In epoch:006|batch:0000, train_loss:0.241256, train_ap:0.8970, train_acc:0.8926, train_auc:0.9530\n",
      "In epoch:006|batch:0050, train_loss:0.217698, train_ap:0.9000, train_acc:0.8945, train_auc:0.9611\n",
      "In epoch:006|batch:0100, train_loss:0.212770, train_ap:0.8869, train_acc:0.9062, train_auc:0.9591\n",
      "In epoch:006|batch:0150, train_loss:0.214068, train_ap:0.9082, train_acc:0.8945, train_auc:0.9647\n",
      "In epoch:006|batch:0200, train_loss:0.215293, train_ap:0.9125, train_acc:0.9141, train_auc:0.9701\n",
      "In epoch:006|batch:0250, train_loss:0.216382, train_ap:0.8874, train_acc:0.9141, train_auc:0.9623\n",
      "In epoch:006|batch:0300, train_loss:0.216149, train_ap:0.8982, train_acc:0.9043, train_auc:0.9568\n",
      "In epoch:006|batch:0350, train_loss:0.216202, train_ap:0.9216, train_acc:0.9258, train_auc:0.9723\n",
      "In epoch:006|batch:0400, train_loss:0.217183, train_ap:0.8708, train_acc:0.8965, train_auc:0.9538\n",
      "In epoch:006|batch:0450, train_loss:0.217501, train_ap:0.8917, train_acc:0.9121, train_auc:0.9642\n",
      "In epoch:006|batch:0500, train_loss:0.217086, train_ap:0.9081, train_acc:0.9141, train_auc:0.9672\n",
      "In epoch:006|batch:0550, train_loss:0.217309, train_ap:0.9344, train_acc:0.9316, train_auc:0.9777\n",
      "In epoch:006|batch:0600, train_loss:0.217053, train_ap:0.8628, train_acc:0.8906, train_auc:0.9482\n",
      "In epoch:006|batch:0650, train_loss:0.216700, train_ap:0.9079, train_acc:0.9082, train_auc:0.9652\n",
      "In epoch:006|batch:0700, train_loss:0.216577, train_ap:0.9127, train_acc:0.8926, train_auc:0.9652\n",
      "In epoch:006|batch:0000, val_loss:0.000419, val_ap:0.8802, val_acc:0.9023, val_auc:0.9613\n",
      "In epoch:006|batch:0050, val_loss:0.000443, val_ap:0.8877, val_acc:0.9062, val_auc:0.9577\n",
      "In epoch:006|batch:0100, val_loss:0.000439, val_ap:0.8855, val_acc:0.8867, val_auc:0.9541\n",
      "In epoch:006|batch:0150, val_loss:0.000436, val_ap:0.8997, val_acc:0.9062, val_auc:0.9660\n",
      "G: 0.918337, D: 0.645653, ALL: 0.781995\n",
      "In epoch:007|batch:0000, train_loss:0.228062, train_ap:0.8969, train_acc:0.9043, train_auc:0.9597\n",
      "In epoch:007|batch:0050, train_loss:0.217830, train_ap:0.9179, train_acc:0.9004, train_auc:0.9632\n",
      "In epoch:007|batch:0100, train_loss:0.215805, train_ap:0.9063, train_acc:0.9062, train_auc:0.9657\n",
      "In epoch:007|batch:0150, train_loss:0.216223, train_ap:0.8601, train_acc:0.8984, train_auc:0.9498\n",
      "In epoch:007|batch:0200, train_loss:0.215845, train_ap:0.9160, train_acc:0.9023, train_auc:0.9603\n",
      "In epoch:007|batch:0250, train_loss:0.215895, train_ap:0.8841, train_acc:0.8828, train_auc:0.9488\n",
      "In epoch:007|batch:0300, train_loss:0.216060, train_ap:0.8992, train_acc:0.8984, train_auc:0.9592\n",
      "In epoch:007|batch:0350, train_loss:0.216154, train_ap:0.8940, train_acc:0.9004, train_auc:0.9540\n",
      "In epoch:007|batch:0400, train_loss:0.216354, train_ap:0.9195, train_acc:0.9121, train_auc:0.9680\n",
      "In epoch:007|batch:0450, train_loss:0.216050, train_ap:0.8952, train_acc:0.9102, train_auc:0.9593\n",
      "In epoch:007|batch:0500, train_loss:0.215607, train_ap:0.9082, train_acc:0.9238, train_auc:0.9638\n",
      "In epoch:007|batch:0550, train_loss:0.215738, train_ap:0.9212, train_acc:0.9102, train_auc:0.9679\n",
      "In epoch:007|batch:0600, train_loss:0.215707, train_ap:0.9006, train_acc:0.9082, train_auc:0.9657\n",
      "In epoch:007|batch:0650, train_loss:0.215528, train_ap:0.9139, train_acc:0.9023, train_auc:0.9642\n",
      "In epoch:007|batch:0700, train_loss:0.215425, train_ap:0.9349, train_acc:0.9121, train_auc:0.9704\n",
      "In epoch:007|batch:0000, val_loss:0.000413, val_ap:0.8762, val_acc:0.9082, val_auc:0.9596\n",
      "In epoch:007|batch:0050, val_loss:0.000440, val_ap:0.8577, val_acc:0.8906, val_auc:0.9528\n",
      "In epoch:007|batch:0100, val_loss:0.000444, val_ap:0.9003, val_acc:0.8984, val_auc:0.9565\n",
      "In epoch:007|batch:0150, val_loss:0.000440, val_ap:0.8811, val_acc:0.9023, val_auc:0.9598\n",
      "G: 0.918376, D: 0.643086, ALL: 0.780731\n",
      "EarlyStoper count: 01\n",
      "In epoch:008|batch:0000, train_loss:0.224082, train_ap:0.8715, train_acc:0.8867, train_auc:0.9560\n",
      "In epoch:008|batch:0050, train_loss:0.219416, train_ap:0.8912, train_acc:0.9023, train_auc:0.9602\n",
      "In epoch:008|batch:0100, train_loss:0.215695, train_ap:0.9119, train_acc:0.8965, train_auc:0.9642\n",
      "In epoch:008|batch:0150, train_loss:0.213341, train_ap:0.9109, train_acc:0.9043, train_auc:0.9591\n",
      "In epoch:008|batch:0200, train_loss:0.213769, train_ap:0.9278, train_acc:0.9160, train_auc:0.9636\n",
      "In epoch:008|batch:0250, train_loss:0.214574, train_ap:0.8879, train_acc:0.9102, train_auc:0.9670\n",
      "In epoch:008|batch:0300, train_loss:0.215404, train_ap:0.9032, train_acc:0.9062, train_auc:0.9565\n",
      "In epoch:008|batch:0350, train_loss:0.215398, train_ap:0.8844, train_acc:0.9219, train_auc:0.9578\n",
      "In epoch:008|batch:0400, train_loss:0.215671, train_ap:0.8801, train_acc:0.8926, train_auc:0.9540\n",
      "In epoch:008|batch:0450, train_loss:0.215554, train_ap:0.9175, train_acc:0.9141, train_auc:0.9632\n",
      "In epoch:008|batch:0500, train_loss:0.215587, train_ap:0.9170, train_acc:0.9023, train_auc:0.9639\n",
      "In epoch:008|batch:0550, train_loss:0.215593, train_ap:0.8152, train_acc:0.8691, train_auc:0.9284\n",
      "In epoch:008|batch:0600, train_loss:0.215353, train_ap:0.9098, train_acc:0.9238, train_auc:0.9650\n",
      "In epoch:008|batch:0650, train_loss:0.215414, train_ap:0.8919, train_acc:0.9180, train_auc:0.9534\n",
      "In epoch:008|batch:0700, train_loss:0.215279, train_ap:0.9110, train_acc:0.9160, train_auc:0.9677\n",
      "In epoch:008|batch:0000, val_loss:0.000453, val_ap:0.8960, val_acc:0.8945, val_auc:0.9602\n",
      "In epoch:008|batch:0050, val_loss:0.000435, val_ap:0.9275, val_acc:0.9141, val_auc:0.9723\n",
      "In epoch:008|batch:0100, val_loss:0.000441, val_ap:0.8719, val_acc:0.8926, val_auc:0.9551\n",
      "In epoch:008|batch:0150, val_loss:0.000437, val_ap:0.9209, val_acc:0.9082, val_auc:0.9660\n",
      "G: 0.918366, D: 0.643212, ALL: 0.780789\n",
      "EarlyStoper count: 02\n",
      "In epoch:009|batch:0000, train_loss:0.241590, train_ap:0.8861, train_acc:0.9062, train_auc:0.9537\n",
      "In epoch:009|batch:0050, train_loss:0.217365, train_ap:0.9131, train_acc:0.9004, train_auc:0.9634\n",
      "In epoch:009|batch:0100, train_loss:0.216972, train_ap:0.8970, train_acc:0.9004, train_auc:0.9551\n",
      "In epoch:009|batch:0150, train_loss:0.215126, train_ap:0.9063, train_acc:0.8906, train_auc:0.9588\n",
      "In epoch:009|batch:0200, train_loss:0.215851, train_ap:0.8980, train_acc:0.8906, train_auc:0.9574\n",
      "In epoch:009|batch:0250, train_loss:0.215183, train_ap:0.9249, train_acc:0.9180, train_auc:0.9691\n",
      "In epoch:009|batch:0300, train_loss:0.215176, train_ap:0.9135, train_acc:0.9219, train_auc:0.9685\n",
      "In epoch:009|batch:0350, train_loss:0.214789, train_ap:0.8510, train_acc:0.8828, train_auc:0.9441\n",
      "In epoch:009|batch:0400, train_loss:0.214492, train_ap:0.9034, train_acc:0.9062, train_auc:0.9654\n",
      "In epoch:009|batch:0450, train_loss:0.214961, train_ap:0.8718, train_acc:0.8945, train_auc:0.9551\n",
      "In epoch:009|batch:0500, train_loss:0.214673, train_ap:0.8742, train_acc:0.8906, train_auc:0.9567\n",
      "In epoch:009|batch:0550, train_loss:0.214547, train_ap:0.8613, train_acc:0.9004, train_auc:0.9642\n",
      "In epoch:009|batch:0600, train_loss:0.214604, train_ap:0.8955, train_acc:0.9160, train_auc:0.9561\n",
      "In epoch:009|batch:0650, train_loss:0.214681, train_ap:0.9074, train_acc:0.9102, train_auc:0.9534\n",
      "In epoch:009|batch:0700, train_loss:0.215171, train_ap:0.9057, train_acc:0.8945, train_auc:0.9573\n",
      "In epoch:009|batch:0000, val_loss:0.000407, val_ap:0.9106, val_acc:0.9160, val_auc:0.9630\n",
      "In epoch:009|batch:0050, val_loss:0.000440, val_ap:0.8923, val_acc:0.8906, val_auc:0.9583\n",
      "In epoch:009|batch:0100, val_loss:0.000442, val_ap:0.8910, val_acc:0.8848, val_auc:0.9514\n",
      "In epoch:009|batch:0150, val_loss:0.000440, val_ap:0.9148, val_acc:0.9023, val_auc:0.9663\n",
      "G: 0.918368, D: 0.643464, ALL: 0.780916\n",
      "EarlyStoper count: 03\n",
      "In epoch:010|batch:0000, train_loss:0.207624, train_ap:0.9069, train_acc:0.9043, train_auc:0.9660\n",
      "In epoch:010|batch:0050, train_loss:0.211700, train_ap:0.9387, train_acc:0.9297, train_auc:0.9795\n",
      "In epoch:010|batch:0100, train_loss:0.214559, train_ap:0.8748, train_acc:0.9004, train_auc:0.9537\n",
      "In epoch:010|batch:0150, train_loss:0.215840, train_ap:0.9251, train_acc:0.9004, train_auc:0.9633\n",
      "In epoch:010|batch:0200, train_loss:0.215555, train_ap:0.8969, train_acc:0.8848, train_auc:0.9511\n",
      "In epoch:010|batch:0250, train_loss:0.215051, train_ap:0.9098, train_acc:0.9004, train_auc:0.9614\n",
      "In epoch:010|batch:0300, train_loss:0.215394, train_ap:0.9107, train_acc:0.9082, train_auc:0.9711\n",
      "In epoch:010|batch:0350, train_loss:0.215389, train_ap:0.9078, train_acc:0.9004, train_auc:0.9607\n",
      "In epoch:010|batch:0400, train_loss:0.215127, train_ap:0.9108, train_acc:0.8984, train_auc:0.9627\n",
      "In epoch:010|batch:0450, train_loss:0.215174, train_ap:0.8814, train_acc:0.9219, train_auc:0.9642\n",
      "In epoch:010|batch:0500, train_loss:0.215127, train_ap:0.9186, train_acc:0.9121, train_auc:0.9705\n",
      "In epoch:010|batch:0550, train_loss:0.214900, train_ap:0.9244, train_acc:0.9180, train_auc:0.9720\n",
      "In epoch:010|batch:0600, train_loss:0.215324, train_ap:0.8994, train_acc:0.8867, train_auc:0.9579\n",
      "In epoch:010|batch:0650, train_loss:0.215414, train_ap:0.9178, train_acc:0.9102, train_auc:0.9603\n",
      "In epoch:010|batch:0700, train_loss:0.215047, train_ap:0.9060, train_acc:0.9023, train_auc:0.9595\n",
      "In epoch:010|batch:0000, val_loss:0.000396, val_ap:0.9057, val_acc:0.9141, val_auc:0.9650\n",
      "In epoch:010|batch:0050, val_loss:0.000448, val_ap:0.8738, val_acc:0.9062, val_auc:0.9565\n",
      "In epoch:010|batch:0100, val_loss:0.000442, val_ap:0.8763, val_acc:0.8965, val_auc:0.9530\n",
      "In epoch:010|batch:0150, val_loss:0.000440, val_ap:0.9195, val_acc:0.8965, val_auc:0.9639\n",
      "G: 0.918370, D: 0.643338, ALL: 0.780854\n",
      "EarlyStoper count: 04\n",
      "Early Stopping!\n",
      "Best val_metric is: 0.7819949\n",
      "Inferring validation data...\n",
      "In epoch:010|batch:0000, val_loss:0.000395, val_ap:0.8960, val_acc:0.9062, val_auc:0.9640\n",
      "In epoch:010|batch:0050, val_loss:0.000433, val_ap:0.8416, val_acc:0.8848, val_auc:0.9509\n",
      "In epoch:010|batch:0100, val_loss:0.000436, val_ap:0.8914, val_acc:0.8984, val_auc:0.9569\n",
      "In epoch:010|batch:0150, val_loss:0.000437, val_ap:0.8896, val_acc:0.8984, val_auc:0.9605\n",
      "\n",
      "G: 0.918337, D: 0.645653, ALL: 0.781995\n",
      "Fold 2 CV= 0.7819948636542691\n",
      "\n",
      "#########################\n",
      "### Fold 3 with valid files [5, 6]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "In epoch:000|batch:0000, train_loss:0.718319, train_ap:0.2182, train_acc:0.2676, train_auc:0.2964\n",
      "In epoch:000|batch:0050, train_loss:0.389928, train_ap:0.9114, train_acc:0.8906, train_auc:0.9604\n",
      "In epoch:000|batch:0100, train_loss:0.321247, train_ap:0.8632, train_acc:0.9043, train_auc:0.9565\n",
      "In epoch:000|batch:0150, train_loss:0.294109, train_ap:0.8822, train_acc:0.9219, train_auc:0.9480\n",
      "In epoch:000|batch:0200, train_loss:0.281535, train_ap:0.9084, train_acc:0.9102, train_auc:0.9624\n",
      "In epoch:000|batch:0250, train_loss:0.272088, train_ap:0.8779, train_acc:0.8926, train_auc:0.9496\n",
      "In epoch:000|batch:0300, train_loss:0.266827, train_ap:0.8873, train_acc:0.8906, train_auc:0.9587\n",
      "In epoch:000|batch:0350, train_loss:0.262689, train_ap:0.8721, train_acc:0.9062, train_auc:0.9543\n",
      "In epoch:000|batch:0400, train_loss:0.260034, train_ap:0.9296, train_acc:0.8984, train_auc:0.9672\n",
      "In epoch:000|batch:0450, train_loss:0.257063, train_ap:0.9050, train_acc:0.9121, train_auc:0.9641\n",
      "In epoch:000|batch:0500, train_loss:0.254877, train_ap:0.8811, train_acc:0.8906, train_auc:0.9536\n",
      "In epoch:000|batch:0550, train_loss:0.252957, train_ap:0.8683, train_acc:0.8965, train_auc:0.9516\n",
      "In epoch:000|batch:0600, train_loss:0.251135, train_ap:0.8868, train_acc:0.9004, train_auc:0.9547\n",
      "In epoch:000|batch:0650, train_loss:0.249448, train_ap:0.8770, train_acc:0.8926, train_auc:0.9494\n",
      "In epoch:000|batch:0700, train_loss:0.248231, train_ap:0.8844, train_acc:0.8984, train_auc:0.9515\n",
      "In epoch:000|batch:0000, val_loss:0.000412, val_ap:0.9252, val_acc:0.9199, val_auc:0.9684\n",
      "In epoch:000|batch:0050, val_loss:0.000450, val_ap:0.8527, val_acc:0.8867, val_auc:0.9509\n",
      "In epoch:000|batch:0100, val_loss:0.000450, val_ap:0.9104, val_acc:0.9121, val_auc:0.9619\n",
      "In epoch:000|batch:0150, val_loss:0.000451, val_ap:0.8776, val_acc:0.8828, val_auc:0.9488\n",
      "G: 0.915193, D: 0.634774, ALL: 0.774984\n",
      "In epoch:001|batch:0000, train_loss:0.212549, train_ap:0.8920, train_acc:0.8867, train_auc:0.9607\n",
      "In epoch:001|batch:0050, train_loss:0.226671, train_ap:0.8698, train_acc:0.8867, train_auc:0.9508\n",
      "In epoch:001|batch:0100, train_loss:0.227607, train_ap:0.8968, train_acc:0.8965, train_auc:0.9611\n",
      "In epoch:001|batch:0150, train_loss:0.229003, train_ap:0.8878, train_acc:0.8926, train_auc:0.9551\n",
      "In epoch:001|batch:0200, train_loss:0.229899, train_ap:0.8655, train_acc:0.8828, train_auc:0.9546\n",
      "In epoch:001|batch:0250, train_loss:0.230443, train_ap:0.8948, train_acc:0.8828, train_auc:0.9593\n",
      "In epoch:001|batch:0300, train_loss:0.230032, train_ap:0.9146, train_acc:0.8965, train_auc:0.9570\n",
      "In epoch:001|batch:0350, train_loss:0.230460, train_ap:0.8679, train_acc:0.8633, train_auc:0.9459\n",
      "In epoch:001|batch:0400, train_loss:0.229626, train_ap:0.9111, train_acc:0.9160, train_auc:0.9709\n",
      "In epoch:001|batch:0450, train_loss:0.229057, train_ap:0.8993, train_acc:0.8965, train_auc:0.9622\n",
      "In epoch:001|batch:0500, train_loss:0.229221, train_ap:0.9091, train_acc:0.9082, train_auc:0.9638\n",
      "In epoch:001|batch:0550, train_loss:0.228839, train_ap:0.9232, train_acc:0.8965, train_auc:0.9688\n",
      "In epoch:001|batch:0600, train_loss:0.229195, train_ap:0.9139, train_acc:0.9043, train_auc:0.9589\n",
      "In epoch:001|batch:0650, train_loss:0.229070, train_ap:0.8715, train_acc:0.9004, train_auc:0.9431\n",
      "In epoch:001|batch:0700, train_loss:0.229071, train_ap:0.8826, train_acc:0.8867, train_auc:0.9531\n",
      "In epoch:001|batch:0000, val_loss:0.000503, val_ap:0.8725, val_acc:0.8750, val_auc:0.9466\n",
      "In epoch:001|batch:0050, val_loss:0.000446, val_ap:0.9068, val_acc:0.8945, val_auc:0.9677\n",
      "In epoch:001|batch:0100, val_loss:0.000441, val_ap:0.8730, val_acc:0.8906, val_auc:0.9509\n",
      "In epoch:001|batch:0150, val_loss:0.000441, val_ap:0.8485, val_acc:0.8633, val_auc:0.9382\n",
      "G: 0.917200, D: 0.643328, ALL: 0.780264\n",
      "In epoch:002|batch:0000, train_loss:0.217038, train_ap:0.9030, train_acc:0.9062, train_auc:0.9609\n",
      "In epoch:002|batch:0050, train_loss:0.230141, train_ap:0.8945, train_acc:0.8867, train_auc:0.9557\n",
      "In epoch:002|batch:0100, train_loss:0.228814, train_ap:0.9274, train_acc:0.9160, train_auc:0.9740\n",
      "In epoch:002|batch:0150, train_loss:0.228180, train_ap:0.8838, train_acc:0.8984, train_auc:0.9519\n",
      "In epoch:002|batch:0200, train_loss:0.228464, train_ap:0.9144, train_acc:0.9238, train_auc:0.9656\n",
      "In epoch:002|batch:0250, train_loss:0.227898, train_ap:0.8782, train_acc:0.9004, train_auc:0.9535\n",
      "In epoch:002|batch:0300, train_loss:0.229033, train_ap:0.8914, train_acc:0.9023, train_auc:0.9555\n",
      "In epoch:002|batch:0350, train_loss:0.228861, train_ap:0.9035, train_acc:0.9043, train_auc:0.9664\n",
      "In epoch:002|batch:0400, train_loss:0.228729, train_ap:0.8905, train_acc:0.8809, train_auc:0.9495\n",
      "In epoch:002|batch:0450, train_loss:0.227882, train_ap:0.8327, train_acc:0.8750, train_auc:0.9459\n",
      "In epoch:002|batch:0500, train_loss:0.227328, train_ap:0.8931, train_acc:0.9062, train_auc:0.9596\n",
      "In epoch:002|batch:0550, train_loss:0.227317, train_ap:0.9336, train_acc:0.9219, train_auc:0.9757\n",
      "In epoch:002|batch:0600, train_loss:0.227176, train_ap:0.8855, train_acc:0.8984, train_auc:0.9595\n",
      "In epoch:002|batch:0650, train_loss:0.227587, train_ap:0.8176, train_acc:0.8730, train_auc:0.9328\n",
      "In epoch:002|batch:0700, train_loss:0.227126, train_ap:0.9000, train_acc:0.8984, train_auc:0.9642\n",
      "In epoch:002|batch:0000, val_loss:0.000383, val_ap:0.9239, val_acc:0.9121, val_auc:0.9677\n",
      "In epoch:002|batch:0050, val_loss:0.000449, val_ap:0.8809, val_acc:0.8965, val_auc:0.9512\n",
      "In epoch:002|batch:0100, val_loss:0.000448, val_ap:0.8969, val_acc:0.8945, val_auc:0.9615\n",
      "In epoch:002|batch:0150, val_loss:0.000445, val_ap:0.8853, val_acc:0.9023, val_auc:0.9597\n",
      "G: 0.917358, D: 0.642869, ALL: 0.780114\n",
      "EarlyStoper count: 01\n",
      "In epoch:003|batch:0000, train_loss:0.237606, train_ap:0.8744, train_acc:0.9023, train_auc:0.9557\n",
      "In epoch:003|batch:0050, train_loss:0.231240, train_ap:0.9111, train_acc:0.9082, train_auc:0.9630\n",
      "In epoch:003|batch:0100, train_loss:0.229580, train_ap:0.8896, train_acc:0.8867, train_auc:0.9598\n",
      "In epoch:003|batch:0150, train_loss:0.228392, train_ap:0.8983, train_acc:0.8965, train_auc:0.9613\n",
      "In epoch:003|batch:0200, train_loss:0.226560, train_ap:0.8710, train_acc:0.8809, train_auc:0.9460\n",
      "In epoch:003|batch:0250, train_loss:0.225782, train_ap:0.8952, train_acc:0.9004, train_auc:0.9623\n",
      "In epoch:003|batch:0300, train_loss:0.225601, train_ap:0.8414, train_acc:0.8828, train_auc:0.9411\n",
      "In epoch:003|batch:0350, train_loss:0.225586, train_ap:0.8909, train_acc:0.8984, train_auc:0.9606\n",
      "In epoch:003|batch:0400, train_loss:0.225544, train_ap:0.9009, train_acc:0.8887, train_auc:0.9569\n",
      "In epoch:003|batch:0450, train_loss:0.224772, train_ap:0.8827, train_acc:0.9023, train_auc:0.9623\n",
      "In epoch:003|batch:0500, train_loss:0.224626, train_ap:0.8698, train_acc:0.8887, train_auc:0.9520\n",
      "In epoch:003|batch:0550, train_loss:0.224658, train_ap:0.9435, train_acc:0.8984, train_auc:0.9686\n",
      "In epoch:003|batch:0600, train_loss:0.224847, train_ap:0.8849, train_acc:0.8906, train_auc:0.9538\n",
      "In epoch:003|batch:0650, train_loss:0.224652, train_ap:0.8859, train_acc:0.9023, train_auc:0.9584\n",
      "In epoch:003|batch:0700, train_loss:0.224501, train_ap:0.8312, train_acc:0.8652, train_auc:0.9430\n",
      "In epoch:003|batch:0000, val_loss:0.000388, val_ap:0.9219, val_acc:0.9141, val_auc:0.9676\n",
      "In epoch:003|batch:0050, val_loss:0.000431, val_ap:0.8525, val_acc:0.8789, val_auc:0.9468\n",
      "In epoch:003|batch:0100, val_loss:0.000437, val_ap:0.8513, val_acc:0.8730, val_auc:0.9474\n",
      "In epoch:003|batch:0150, val_loss:0.000437, val_ap:0.8672, val_acc:0.8965, val_auc:0.9490\n",
      "G: 0.918873, D: 0.646791, ALL: 0.782832\n",
      "In epoch:004|batch:0000, train_loss:0.193869, train_ap:0.9155, train_acc:0.9297, train_auc:0.9708\n",
      "In epoch:004|batch:0050, train_loss:0.222897, train_ap:0.9073, train_acc:0.8887, train_auc:0.9614\n",
      "In epoch:004|batch:0100, train_loss:0.224944, train_ap:0.9049, train_acc:0.9121, train_auc:0.9634\n",
      "In epoch:004|batch:0150, train_loss:0.222199, train_ap:0.8739, train_acc:0.8965, train_auc:0.9598\n",
      "In epoch:004|batch:0200, train_loss:0.221628, train_ap:0.9054, train_acc:0.9160, train_auc:0.9665\n",
      "In epoch:004|batch:0250, train_loss:0.222080, train_ap:0.8883, train_acc:0.8965, train_auc:0.9568\n",
      "In epoch:004|batch:0300, train_loss:0.221838, train_ap:0.8751, train_acc:0.8867, train_auc:0.9555\n",
      "In epoch:004|batch:0350, train_loss:0.221800, train_ap:0.9096, train_acc:0.9180, train_auc:0.9630\n",
      "In epoch:004|batch:0400, train_loss:0.222113, train_ap:0.8590, train_acc:0.9023, train_auc:0.9504\n",
      "In epoch:004|batch:0450, train_loss:0.222423, train_ap:0.8983, train_acc:0.9121, train_auc:0.9646\n",
      "In epoch:004|batch:0500, train_loss:0.222388, train_ap:0.8886, train_acc:0.8984, train_auc:0.9532\n",
      "In epoch:004|batch:0550, train_loss:0.222252, train_ap:0.8863, train_acc:0.9023, train_auc:0.9576\n",
      "In epoch:004|batch:0600, train_loss:0.222137, train_ap:0.9058, train_acc:0.8789, train_auc:0.9580\n",
      "In epoch:004|batch:0650, train_loss:0.222612, train_ap:0.8845, train_acc:0.8984, train_auc:0.9561\n",
      "In epoch:004|batch:0700, train_loss:0.222554, train_ap:0.8671, train_acc:0.8926, train_auc:0.9469\n",
      "In epoch:004|batch:0000, val_loss:0.000508, val_ap:0.8760, val_acc:0.8926, val_auc:0.9472\n",
      "In epoch:004|batch:0050, val_loss:0.000452, val_ap:0.9169, val_acc:0.9238, val_auc:0.9700\n",
      "In epoch:004|batch:0100, val_loss:0.000449, val_ap:0.9006, val_acc:0.9004, val_auc:0.9645\n",
      "In epoch:004|batch:0150, val_loss:0.000446, val_ap:0.9142, val_acc:0.9082, val_auc:0.9693\n",
      "G: 0.919000, D: 0.647542, ALL: 0.783271\n",
      "In epoch:005|batch:0000, train_loss:0.216339, train_ap:0.8663, train_acc:0.9023, train_auc:0.9596\n",
      "In epoch:005|batch:0050, train_loss:0.223167, train_ap:0.8839, train_acc:0.9180, train_auc:0.9596\n",
      "In epoch:005|batch:0100, train_loss:0.218632, train_ap:0.9000, train_acc:0.9297, train_auc:0.9726\n",
      "In epoch:005|batch:0150, train_loss:0.218717, train_ap:0.8907, train_acc:0.9102, train_auc:0.9577\n",
      "In epoch:005|batch:0200, train_loss:0.218861, train_ap:0.8869, train_acc:0.8984, train_auc:0.9584\n",
      "In epoch:005|batch:0250, train_loss:0.218124, train_ap:0.8764, train_acc:0.8770, train_auc:0.9589\n",
      "In epoch:005|batch:0300, train_loss:0.217649, train_ap:0.9135, train_acc:0.9043, train_auc:0.9666\n",
      "In epoch:005|batch:0350, train_loss:0.217777, train_ap:0.8884, train_acc:0.8906, train_auc:0.9508\n",
      "In epoch:005|batch:0400, train_loss:0.218289, train_ap:0.8883, train_acc:0.8965, train_auc:0.9547\n",
      "In epoch:005|batch:0450, train_loss:0.218032, train_ap:0.9040, train_acc:0.9043, train_auc:0.9636\n",
      "In epoch:005|batch:0500, train_loss:0.218234, train_ap:0.8868, train_acc:0.9141, train_auc:0.9627\n",
      "In epoch:005|batch:0550, train_loss:0.217999, train_ap:0.9108, train_acc:0.9180, train_auc:0.9709\n",
      "In epoch:005|batch:0600, train_loss:0.217274, train_ap:0.9169, train_acc:0.9121, train_auc:0.9686\n",
      "In epoch:005|batch:0650, train_loss:0.217651, train_ap:0.8604, train_acc:0.8828, train_auc:0.9447\n",
      "In epoch:005|batch:0700, train_loss:0.217663, train_ap:0.9009, train_acc:0.9160, train_auc:0.9689\n",
      "In epoch:005|batch:0000, val_loss:0.000453, val_ap:0.8609, val_acc:0.9004, val_auc:0.9545\n",
      "In epoch:005|batch:0050, val_loss:0.000432, val_ap:0.8936, val_acc:0.8945, val_auc:0.9599\n",
      "In epoch:005|batch:0100, val_loss:0.000435, val_ap:0.8767, val_acc:0.8906, val_auc:0.9483\n",
      "In epoch:005|batch:0150, val_loss:0.000435, val_ap:0.8484, val_acc:0.8965, val_auc:0.9543\n",
      "G: 0.920268, D: 0.649670, ALL: 0.784969\n",
      "In epoch:006|batch:0000, train_loss:0.197231, train_ap:0.9109, train_acc:0.9121, train_auc:0.9664\n",
      "In epoch:006|batch:0050, train_loss:0.214165, train_ap:0.9173, train_acc:0.9023, train_auc:0.9652\n",
      "In epoch:006|batch:0100, train_loss:0.217180, train_ap:0.8469, train_acc:0.8730, train_auc:0.9410\n",
      "In epoch:006|batch:0150, train_loss:0.217996, train_ap:0.8996, train_acc:0.9023, train_auc:0.9506\n",
      "In epoch:006|batch:0200, train_loss:0.217367, train_ap:0.8963, train_acc:0.9023, train_auc:0.9587\n",
      "In epoch:006|batch:0250, train_loss:0.216687, train_ap:0.9297, train_acc:0.9023, train_auc:0.9668\n",
      "In epoch:006|batch:0300, train_loss:0.216704, train_ap:0.9135, train_acc:0.9102, train_auc:0.9684\n",
      "In epoch:006|batch:0350, train_loss:0.216576, train_ap:0.9221, train_acc:0.9199, train_auc:0.9685\n",
      "In epoch:006|batch:0400, train_loss:0.216196, train_ap:0.9042, train_acc:0.9121, train_auc:0.9682\n",
      "In epoch:006|batch:0450, train_loss:0.215803, train_ap:0.9281, train_acc:0.9082, train_auc:0.9725\n",
      "In epoch:006|batch:0500, train_loss:0.215683, train_ap:0.9178, train_acc:0.9082, train_auc:0.9682\n",
      "In epoch:006|batch:0550, train_loss:0.215656, train_ap:0.8920, train_acc:0.9062, train_auc:0.9585\n",
      "In epoch:006|batch:0600, train_loss:0.216064, train_ap:0.8991, train_acc:0.9102, train_auc:0.9624\n",
      "In epoch:006|batch:0650, train_loss:0.216058, train_ap:0.8935, train_acc:0.9082, train_auc:0.9676\n",
      "In epoch:006|batch:0700, train_loss:0.215903, train_ap:0.8672, train_acc:0.8906, train_auc:0.9511\n",
      "In epoch:006|batch:0000, val_loss:0.000476, val_ap:0.8703, val_acc:0.8867, val_auc:0.9515\n",
      "In epoch:006|batch:0050, val_loss:0.000440, val_ap:0.9103, val_acc:0.9238, val_auc:0.9678\n",
      "In epoch:006|batch:0100, val_loss:0.000435, val_ap:0.9217, val_acc:0.9102, val_auc:0.9673\n",
      "In epoch:006|batch:0150, val_loss:0.000435, val_ap:0.8292, val_acc:0.8984, val_auc:0.9522\n",
      "G: 0.920529, D: 0.649462, ALL: 0.784995\n",
      "In epoch:007|batch:0000, train_loss:0.170271, train_ap:0.9495, train_acc:0.9199, train_auc:0.9786\n",
      "In epoch:007|batch:0050, train_loss:0.216936, train_ap:0.8836, train_acc:0.8926, train_auc:0.9559\n",
      "In epoch:007|batch:0100, train_loss:0.213839, train_ap:0.9340, train_acc:0.9336, train_auc:0.9788\n",
      "In epoch:007|batch:0150, train_loss:0.214587, train_ap:0.8804, train_acc:0.9004, train_auc:0.9598\n",
      "In epoch:007|batch:0200, train_loss:0.213867, train_ap:0.9044, train_acc:0.9004, train_auc:0.9630\n",
      "In epoch:007|batch:0250, train_loss:0.214910, train_ap:0.9124, train_acc:0.9102, train_auc:0.9661\n",
      "In epoch:007|batch:0300, train_loss:0.215724, train_ap:0.9124, train_acc:0.9043, train_auc:0.9666\n",
      "In epoch:007|batch:0350, train_loss:0.215682, train_ap:0.8977, train_acc:0.9062, train_auc:0.9615\n",
      "In epoch:007|batch:0400, train_loss:0.215575, train_ap:0.9105, train_acc:0.9121, train_auc:0.9686\n",
      "In epoch:007|batch:0450, train_loss:0.215473, train_ap:0.8931, train_acc:0.9023, train_auc:0.9613\n",
      "In epoch:007|batch:0500, train_loss:0.215535, train_ap:0.9315, train_acc:0.9277, train_auc:0.9726\n",
      "In epoch:007|batch:0550, train_loss:0.215576, train_ap:0.9150, train_acc:0.9062, train_auc:0.9595\n",
      "In epoch:007|batch:0600, train_loss:0.215032, train_ap:0.8901, train_acc:0.9121, train_auc:0.9683\n",
      "In epoch:007|batch:0650, train_loss:0.214900, train_ap:0.9072, train_acc:0.8965, train_auc:0.9647\n",
      "In epoch:007|batch:0700, train_loss:0.214715, train_ap:0.9090, train_acc:0.9023, train_auc:0.9658\n",
      "In epoch:007|batch:0000, val_loss:0.000383, val_ap:0.9257, val_acc:0.9121, val_auc:0.9682\n",
      "In epoch:007|batch:0050, val_loss:0.000426, val_ap:0.9103, val_acc:0.9160, val_auc:0.9614\n",
      "In epoch:007|batch:0100, val_loss:0.000430, val_ap:0.8897, val_acc:0.9082, val_auc:0.9651\n",
      "In epoch:007|batch:0150, val_loss:0.000433, val_ap:0.9203, val_acc:0.9121, val_auc:0.9694\n",
      "G: 0.920574, D: 0.649712, ALL: 0.785143\n",
      "In epoch:008|batch:0000, train_loss:0.235246, train_ap:0.9114, train_acc:0.9043, train_auc:0.9586\n",
      "In epoch:008|batch:0050, train_loss:0.209781, train_ap:0.9383, train_acc:0.9316, train_auc:0.9697\n",
      "In epoch:008|batch:0100, train_loss:0.209806, train_ap:0.9274, train_acc:0.9297, train_auc:0.9687\n",
      "In epoch:008|batch:0150, train_loss:0.210742, train_ap:0.9256, train_acc:0.9238, train_auc:0.9707\n",
      "In epoch:008|batch:0200, train_loss:0.211320, train_ap:0.8999, train_acc:0.9297, train_auc:0.9595\n",
      "In epoch:008|batch:0250, train_loss:0.212137, train_ap:0.8768, train_acc:0.8828, train_auc:0.9552\n",
      "In epoch:008|batch:0300, train_loss:0.213034, train_ap:0.8847, train_acc:0.8965, train_auc:0.9555\n",
      "In epoch:008|batch:0350, train_loss:0.212716, train_ap:0.9093, train_acc:0.9199, train_auc:0.9645\n",
      "In epoch:008|batch:0400, train_loss:0.213274, train_ap:0.9168, train_acc:0.9160, train_auc:0.9688\n",
      "In epoch:008|batch:0450, train_loss:0.213192, train_ap:0.9036, train_acc:0.8945, train_auc:0.9605\n",
      "In epoch:008|batch:0500, train_loss:0.213438, train_ap:0.9113, train_acc:0.9004, train_auc:0.9653\n",
      "In epoch:008|batch:0550, train_loss:0.213517, train_ap:0.9040, train_acc:0.9043, train_auc:0.9637\n",
      "In epoch:008|batch:0600, train_loss:0.213916, train_ap:0.8676, train_acc:0.9102, train_auc:0.9548\n",
      "In epoch:008|batch:0650, train_loss:0.214310, train_ap:0.8809, train_acc:0.9160, train_auc:0.9525\n",
      "In epoch:008|batch:0700, train_loss:0.214377, train_ap:0.9568, train_acc:0.9277, train_auc:0.9797\n",
      "In epoch:008|batch:0000, val_loss:0.000347, val_ap:0.9256, val_acc:0.9160, val_auc:0.9742\n",
      "In epoch:008|batch:0050, val_loss:0.000430, val_ap:0.9165, val_acc:0.9160, val_auc:0.9672\n",
      "In epoch:008|batch:0100, val_loss:0.000434, val_ap:0.8738, val_acc:0.8828, val_auc:0.9491\n",
      "In epoch:008|batch:0150, val_loss:0.000435, val_ap:0.8956, val_acc:0.9043, val_auc:0.9610\n",
      "G: 0.920560, D: 0.650338, ALL: 0.785449\n",
      "In epoch:009|batch:0000, train_loss:0.184964, train_ap:0.9273, train_acc:0.9199, train_auc:0.9734\n",
      "In epoch:009|batch:0050, train_loss:0.214198, train_ap:0.8741, train_acc:0.9062, train_auc:0.9572\n",
      "In epoch:009|batch:0100, train_loss:0.211434, train_ap:0.8950, train_acc:0.8887, train_auc:0.9475\n",
      "In epoch:009|batch:0150, train_loss:0.212562, train_ap:0.8821, train_acc:0.8867, train_auc:0.9519\n",
      "In epoch:009|batch:0200, train_loss:0.213214, train_ap:0.8852, train_acc:0.8945, train_auc:0.9564\n",
      "In epoch:009|batch:0250, train_loss:0.213484, train_ap:0.9178, train_acc:0.9199, train_auc:0.9703\n",
      "In epoch:009|batch:0300, train_loss:0.214293, train_ap:0.8934, train_acc:0.8984, train_auc:0.9589\n",
      "In epoch:009|batch:0350, train_loss:0.214374, train_ap:0.9123, train_acc:0.9102, train_auc:0.9682\n",
      "In epoch:009|batch:0400, train_loss:0.214377, train_ap:0.8870, train_acc:0.9062, train_auc:0.9576\n",
      "In epoch:009|batch:0450, train_loss:0.214345, train_ap:0.8962, train_acc:0.9043, train_auc:0.9581\n",
      "In epoch:009|batch:0500, train_loss:0.214189, train_ap:0.8834, train_acc:0.9004, train_auc:0.9566\n",
      "In epoch:009|batch:0550, train_loss:0.214496, train_ap:0.9167, train_acc:0.9082, train_auc:0.9611\n",
      "In epoch:009|batch:0600, train_loss:0.214509, train_ap:0.8742, train_acc:0.8867, train_auc:0.9440\n",
      "In epoch:009|batch:0650, train_loss:0.214517, train_ap:0.8896, train_acc:0.9141, train_auc:0.9689\n",
      "In epoch:009|batch:0700, train_loss:0.214435, train_ap:0.8907, train_acc:0.8906, train_auc:0.9566\n",
      "In epoch:009|batch:0000, val_loss:0.000494, val_ap:0.8797, val_acc:0.8965, val_auc:0.9499\n",
      "In epoch:009|batch:0050, val_loss:0.000431, val_ap:0.8374, val_acc:0.8926, val_auc:0.9523\n",
      "In epoch:009|batch:0100, val_loss:0.000433, val_ap:0.8786, val_acc:0.8867, val_auc:0.9534\n",
      "In epoch:009|batch:0150, val_loss:0.000432, val_ap:0.8824, val_acc:0.8965, val_auc:0.9580\n",
      "G: 0.920561, D: 0.650255, ALL: 0.785408\n",
      "EarlyStoper count: 01\n",
      "In epoch:010|batch:0000, train_loss:0.182416, train_ap:0.9471, train_acc:0.9238, train_auc:0.9770\n",
      "In epoch:010|batch:0050, train_loss:0.214927, train_ap:0.9167, train_acc:0.9102, train_auc:0.9700\n",
      "In epoch:010|batch:0100, train_loss:0.212993, train_ap:0.8822, train_acc:0.8984, train_auc:0.9558\n",
      "In epoch:010|batch:0150, train_loss:0.214329, train_ap:0.9174, train_acc:0.8965, train_auc:0.9638\n",
      "In epoch:010|batch:0200, train_loss:0.214890, train_ap:0.9045, train_acc:0.9062, train_auc:0.9611\n",
      "In epoch:010|batch:0250, train_loss:0.215345, train_ap:0.8854, train_acc:0.8926, train_auc:0.9549\n",
      "In epoch:010|batch:0300, train_loss:0.215207, train_ap:0.9264, train_acc:0.9180, train_auc:0.9727\n",
      "In epoch:010|batch:0350, train_loss:0.215444, train_ap:0.8671, train_acc:0.8828, train_auc:0.9529\n",
      "In epoch:010|batch:0400, train_loss:0.215395, train_ap:0.9163, train_acc:0.9160, train_auc:0.9749\n",
      "In epoch:010|batch:0450, train_loss:0.215410, train_ap:0.8940, train_acc:0.8848, train_auc:0.9549\n",
      "In epoch:010|batch:0500, train_loss:0.215196, train_ap:0.9060, train_acc:0.9238, train_auc:0.9630\n",
      "In epoch:010|batch:0550, train_loss:0.214739, train_ap:0.9043, train_acc:0.9082, train_auc:0.9618\n",
      "In epoch:010|batch:0600, train_loss:0.214007, train_ap:0.9226, train_acc:0.9141, train_auc:0.9748\n",
      "In epoch:010|batch:0650, train_loss:0.214499, train_ap:0.9114, train_acc:0.8906, train_auc:0.9650\n",
      "In epoch:010|batch:0700, train_loss:0.214569, train_ap:0.9163, train_acc:0.9062, train_auc:0.9623\n",
      "In epoch:010|batch:0000, val_loss:0.000452, val_ap:0.8804, val_acc:0.9102, val_auc:0.9575\n",
      "In epoch:010|batch:0050, val_loss:0.000439, val_ap:0.8990, val_acc:0.9121, val_auc:0.9613\n",
      "In epoch:010|batch:0100, val_loss:0.000430, val_ap:0.8911, val_acc:0.8750, val_auc:0.9520\n",
      "In epoch:010|batch:0150, val_loss:0.000430, val_ap:0.8898, val_acc:0.9043, val_auc:0.9640\n",
      "G: 0.920565, D: 0.650380, ALL: 0.785472\n",
      "In epoch:011|batch:0000, train_loss:0.202167, train_ap:0.9295, train_acc:0.9180, train_auc:0.9693\n",
      "In epoch:011|batch:0050, train_loss:0.220272, train_ap:0.9382, train_acc:0.9238, train_auc:0.9739\n",
      "In epoch:011|batch:0100, train_loss:0.220833, train_ap:0.9054, train_acc:0.9102, train_auc:0.9689\n",
      "In epoch:011|batch:0150, train_loss:0.216968, train_ap:0.8530, train_acc:0.8867, train_auc:0.9489\n",
      "In epoch:011|batch:0200, train_loss:0.216410, train_ap:0.8847, train_acc:0.8965, train_auc:0.9606\n",
      "In epoch:011|batch:0250, train_loss:0.215780, train_ap:0.8957, train_acc:0.8867, train_auc:0.9594\n",
      "In epoch:011|batch:0300, train_loss:0.215114, train_ap:0.8992, train_acc:0.8965, train_auc:0.9591\n",
      "In epoch:011|batch:0350, train_loss:0.214808, train_ap:0.9004, train_acc:0.9043, train_auc:0.9604\n",
      "In epoch:011|batch:0400, train_loss:0.214226, train_ap:0.8934, train_acc:0.9023, train_auc:0.9655\n",
      "In epoch:011|batch:0450, train_loss:0.213999, train_ap:0.8811, train_acc:0.9062, train_auc:0.9612\n",
      "In epoch:011|batch:0500, train_loss:0.213975, train_ap:0.9094, train_acc:0.9043, train_auc:0.9638\n",
      "In epoch:011|batch:0550, train_loss:0.214189, train_ap:0.9195, train_acc:0.9258, train_auc:0.9735\n",
      "In epoch:011|batch:0600, train_loss:0.214252, train_ap:0.8773, train_acc:0.9180, train_auc:0.9575\n",
      "In epoch:011|batch:0650, train_loss:0.214042, train_ap:0.9194, train_acc:0.9277, train_auc:0.9733\n",
      "In epoch:011|batch:0700, train_loss:0.214315, train_ap:0.9069, train_acc:0.9062, train_auc:0.9652\n",
      "In epoch:011|batch:0000, val_loss:0.000405, val_ap:0.9019, val_acc:0.9141, val_auc:0.9645\n",
      "In epoch:011|batch:0050, val_loss:0.000437, val_ap:0.9127, val_acc:0.9238, val_auc:0.9608\n",
      "In epoch:011|batch:0100, val_loss:0.000431, val_ap:0.9083, val_acc:0.9121, val_auc:0.9636\n",
      "In epoch:011|batch:0150, val_loss:0.000434, val_ap:0.8774, val_acc:0.8770, val_auc:0.9518\n",
      "G: 0.920566, D: 0.650714, ALL: 0.785640\n",
      "In epoch:012|batch:0000, train_loss:0.241466, train_ap:0.8911, train_acc:0.8828, train_auc:0.9517\n",
      "In epoch:012|batch:0050, train_loss:0.212009, train_ap:0.9123, train_acc:0.8965, train_auc:0.9629\n",
      "In epoch:012|batch:0100, train_loss:0.214596, train_ap:0.8898, train_acc:0.9082, train_auc:0.9620\n",
      "In epoch:012|batch:0150, train_loss:0.215967, train_ap:0.9450, train_acc:0.9492, train_auc:0.9818\n",
      "In epoch:012|batch:0200, train_loss:0.215020, train_ap:0.9151, train_acc:0.9102, train_auc:0.9725\n",
      "In epoch:012|batch:0250, train_loss:0.215556, train_ap:0.9273, train_acc:0.9121, train_auc:0.9744\n",
      "In epoch:012|batch:0300, train_loss:0.214974, train_ap:0.9093, train_acc:0.8984, train_auc:0.9629\n",
      "In epoch:012|batch:0350, train_loss:0.214502, train_ap:0.9284, train_acc:0.9141, train_auc:0.9724\n",
      "In epoch:012|batch:0400, train_loss:0.213703, train_ap:0.9134, train_acc:0.9180, train_auc:0.9672\n",
      "In epoch:012|batch:0450, train_loss:0.214205, train_ap:0.9140, train_acc:0.8965, train_auc:0.9626\n",
      "In epoch:012|batch:0500, train_loss:0.214343, train_ap:0.8953, train_acc:0.8887, train_auc:0.9571\n",
      "In epoch:012|batch:0550, train_loss:0.214090, train_ap:0.9206, train_acc:0.9043, train_auc:0.9679\n",
      "In epoch:012|batch:0600, train_loss:0.214337, train_ap:0.8663, train_acc:0.8926, train_auc:0.9488\n",
      "In epoch:012|batch:0650, train_loss:0.214122, train_ap:0.9030, train_acc:0.9160, train_auc:0.9654\n",
      "In epoch:012|batch:0700, train_loss:0.214219, train_ap:0.8815, train_acc:0.8926, train_auc:0.9566\n",
      "In epoch:012|batch:0000, val_loss:0.000467, val_ap:0.8891, val_acc:0.8867, val_auc:0.9553\n",
      "In epoch:012|batch:0050, val_loss:0.000430, val_ap:0.8957, val_acc:0.8965, val_auc:0.9560\n",
      "In epoch:012|batch:0100, val_loss:0.000428, val_ap:0.8868, val_acc:0.8867, val_auc:0.9545\n",
      "In epoch:012|batch:0150, val_loss:0.000432, val_ap:0.9006, val_acc:0.8867, val_auc:0.9565\n",
      "G: 0.920567, D: 0.650797, ALL: 0.785682\n",
      "In epoch:013|batch:0000, train_loss:0.242321, train_ap:0.8782, train_acc:0.8906, train_auc:0.9520\n",
      "In epoch:013|batch:0050, train_loss:0.215998, train_ap:0.8748, train_acc:0.9023, train_auc:0.9617\n",
      "In epoch:013|batch:0100, train_loss:0.212716, train_ap:0.8925, train_acc:0.8809, train_auc:0.9611\n",
      "In epoch:013|batch:0150, train_loss:0.212935, train_ap:0.9180, train_acc:0.9121, train_auc:0.9698\n",
      "In epoch:013|batch:0200, train_loss:0.213014, train_ap:0.8898, train_acc:0.9023, train_auc:0.9617\n",
      "In epoch:013|batch:0250, train_loss:0.214295, train_ap:0.9093, train_acc:0.9004, train_auc:0.9618\n",
      "In epoch:013|batch:0300, train_loss:0.213608, train_ap:0.9137, train_acc:0.9043, train_auc:0.9667\n",
      "In epoch:013|batch:0350, train_loss:0.213218, train_ap:0.8664, train_acc:0.8789, train_auc:0.9515\n",
      "In epoch:013|batch:0400, train_loss:0.213461, train_ap:0.9106, train_acc:0.9062, train_auc:0.9615\n",
      "In epoch:013|batch:0450, train_loss:0.213961, train_ap:0.9373, train_acc:0.9082, train_auc:0.9695\n",
      "In epoch:013|batch:0500, train_loss:0.214135, train_ap:0.9192, train_acc:0.9141, train_auc:0.9716\n",
      "In epoch:013|batch:0550, train_loss:0.214325, train_ap:0.9322, train_acc:0.9160, train_auc:0.9701\n",
      "In epoch:013|batch:0600, train_loss:0.214387, train_ap:0.9248, train_acc:0.9082, train_auc:0.9735\n",
      "In epoch:013|batch:0650, train_loss:0.214284, train_ap:0.9126, train_acc:0.8926, train_auc:0.9671\n",
      "In epoch:013|batch:0700, train_loss:0.214373, train_ap:0.9168, train_acc:0.9297, train_auc:0.9702\n",
      "In epoch:013|batch:0000, val_loss:0.000486, val_ap:0.8890, val_acc:0.8770, val_auc:0.9529\n",
      "In epoch:013|batch:0050, val_loss:0.000429, val_ap:0.9061, val_acc:0.9043, val_auc:0.9633\n",
      "In epoch:013|batch:0100, val_loss:0.000428, val_ap:0.9078, val_acc:0.9043, val_auc:0.9680\n",
      "In epoch:013|batch:0150, val_loss:0.000432, val_ap:0.8801, val_acc:0.8965, val_auc:0.9509\n",
      "G: 0.920565, D: 0.650255, ALL: 0.785410\n",
      "EarlyStoper count: 01\n",
      "In epoch:014|batch:0000, train_loss:0.208030, train_ap:0.9083, train_acc:0.9121, train_auc:0.9654\n",
      "In epoch:014|batch:0050, train_loss:0.214072, train_ap:0.9065, train_acc:0.9102, train_auc:0.9665\n",
      "In epoch:014|batch:0100, train_loss:0.213672, train_ap:0.9118, train_acc:0.9062, train_auc:0.9645\n",
      "In epoch:014|batch:0150, train_loss:0.213116, train_ap:0.8834, train_acc:0.9023, train_auc:0.9555\n",
      "In epoch:014|batch:0200, train_loss:0.213511, train_ap:0.9281, train_acc:0.9004, train_auc:0.9685\n",
      "In epoch:014|batch:0250, train_loss:0.213893, train_ap:0.8931, train_acc:0.8965, train_auc:0.9567\n",
      "In epoch:014|batch:0300, train_loss:0.214303, train_ap:0.9168, train_acc:0.9121, train_auc:0.9704\n",
      "In epoch:014|batch:0350, train_loss:0.214293, train_ap:0.8949, train_acc:0.9121, train_auc:0.9585\n",
      "In epoch:014|batch:0400, train_loss:0.214816, train_ap:0.9175, train_acc:0.9160, train_auc:0.9706\n",
      "In epoch:014|batch:0450, train_loss:0.214278, train_ap:0.9142, train_acc:0.9141, train_auc:0.9690\n",
      "In epoch:014|batch:0500, train_loss:0.214222, train_ap:0.9029, train_acc:0.9121, train_auc:0.9622\n",
      "In epoch:014|batch:0550, train_loss:0.214135, train_ap:0.9028, train_acc:0.9102, train_auc:0.9695\n",
      "In epoch:014|batch:0600, train_loss:0.214342, train_ap:0.9111, train_acc:0.9219, train_auc:0.9720\n",
      "In epoch:014|batch:0650, train_loss:0.214394, train_ap:0.8813, train_acc:0.8945, train_auc:0.9550\n",
      "In epoch:014|batch:0700, train_loss:0.214437, train_ap:0.8961, train_acc:0.8984, train_auc:0.9579\n",
      "In epoch:014|batch:0000, val_loss:0.000458, val_ap:0.8717, val_acc:0.8945, val_auc:0.9549\n",
      "In epoch:014|batch:0050, val_loss:0.000437, val_ap:0.8482, val_acc:0.8711, val_auc:0.9453\n",
      "In epoch:014|batch:0100, val_loss:0.000432, val_ap:0.9106, val_acc:0.9121, val_auc:0.9664\n",
      "In epoch:014|batch:0150, val_loss:0.000433, val_ap:0.9016, val_acc:0.8965, val_auc:0.9615\n",
      "G: 0.920566, D: 0.650547, ALL: 0.785556\n",
      "EarlyStoper count: 02\n",
      "In epoch:015|batch:0000, train_loss:0.196184, train_ap:0.8906, train_acc:0.9062, train_auc:0.9657\n",
      "In epoch:015|batch:0050, train_loss:0.214800, train_ap:0.9095, train_acc:0.9219, train_auc:0.9654\n",
      "In epoch:015|batch:0100, train_loss:0.213035, train_ap:0.8940, train_acc:0.9082, train_auc:0.9630\n",
      "In epoch:015|batch:0150, train_loss:0.212873, train_ap:0.9277, train_acc:0.9062, train_auc:0.9644\n",
      "In epoch:015|batch:0200, train_loss:0.211796, train_ap:0.9065, train_acc:0.8848, train_auc:0.9608\n",
      "In epoch:015|batch:0250, train_loss:0.212857, train_ap:0.9235, train_acc:0.9121, train_auc:0.9671\n",
      "In epoch:015|batch:0300, train_loss:0.213146, train_ap:0.9489, train_acc:0.9238, train_auc:0.9749\n",
      "In epoch:015|batch:0350, train_loss:0.213477, train_ap:0.9182, train_acc:0.9199, train_auc:0.9699\n",
      "In epoch:015|batch:0400, train_loss:0.213752, train_ap:0.8837, train_acc:0.8867, train_auc:0.9527\n",
      "In epoch:015|batch:0450, train_loss:0.214039, train_ap:0.8327, train_acc:0.8750, train_auc:0.9370\n",
      "In epoch:015|batch:0500, train_loss:0.214000, train_ap:0.9118, train_acc:0.9082, train_auc:0.9668\n",
      "In epoch:015|batch:0550, train_loss:0.213650, train_ap:0.9110, train_acc:0.9062, train_auc:0.9658\n",
      "In epoch:015|batch:0600, train_loss:0.213756, train_ap:0.8782, train_acc:0.8945, train_auc:0.9558\n",
      "In epoch:015|batch:0650, train_loss:0.214102, train_ap:0.8921, train_acc:0.9121, train_auc:0.9673\n",
      "In epoch:015|batch:0700, train_loss:0.214412, train_ap:0.9051, train_acc:0.9062, train_auc:0.9655\n",
      "In epoch:015|batch:0000, val_loss:0.000416, val_ap:0.8820, val_acc:0.9141, val_auc:0.9619\n",
      "In epoch:015|batch:0050, val_loss:0.000428, val_ap:0.9074, val_acc:0.9121, val_auc:0.9679\n",
      "In epoch:015|batch:0100, val_loss:0.000432, val_ap:0.9019, val_acc:0.9316, val_auc:0.9654\n",
      "In epoch:015|batch:0150, val_loss:0.000433, val_ap:0.9102, val_acc:0.9082, val_auc:0.9643\n",
      "G: 0.920565, D: 0.650255, ALL: 0.785410\n",
      "EarlyStoper count: 03\n",
      "In epoch:016|batch:0000, train_loss:0.226034, train_ap:0.8756, train_acc:0.9023, train_auc:0.9550\n",
      "In epoch:016|batch:0050, train_loss:0.207435, train_ap:0.9019, train_acc:0.8984, train_auc:0.9617\n",
      "In epoch:016|batch:0100, train_loss:0.211329, train_ap:0.8360, train_acc:0.8887, train_auc:0.9446\n",
      "In epoch:016|batch:0150, train_loss:0.212458, train_ap:0.8676, train_acc:0.8730, train_auc:0.9468\n",
      "In epoch:016|batch:0200, train_loss:0.212125, train_ap:0.8907, train_acc:0.9082, train_auc:0.9580\n",
      "In epoch:016|batch:0250, train_loss:0.212427, train_ap:0.9200, train_acc:0.9141, train_auc:0.9742\n",
      "In epoch:016|batch:0300, train_loss:0.213476, train_ap:0.9127, train_acc:0.9141, train_auc:0.9649\n",
      "In epoch:016|batch:0350, train_loss:0.213783, train_ap:0.9163, train_acc:0.9023, train_auc:0.9590\n",
      "In epoch:016|batch:0400, train_loss:0.214217, train_ap:0.8950, train_acc:0.9102, train_auc:0.9642\n",
      "In epoch:016|batch:0450, train_loss:0.213953, train_ap:0.9089, train_acc:0.9062, train_auc:0.9654\n",
      "In epoch:016|batch:0500, train_loss:0.214076, train_ap:0.8881, train_acc:0.9004, train_auc:0.9604\n",
      "In epoch:016|batch:0550, train_loss:0.214103, train_ap:0.9250, train_acc:0.9238, train_auc:0.9678\n",
      "In epoch:016|batch:0600, train_loss:0.214046, train_ap:0.8590, train_acc:0.8789, train_auc:0.9451\n",
      "In epoch:016|batch:0650, train_loss:0.214566, train_ap:0.8925, train_acc:0.9043, train_auc:0.9566\n",
      "In epoch:016|batch:0700, train_loss:0.214425, train_ap:0.8395, train_acc:0.8867, train_auc:0.9482\n",
      "In epoch:016|batch:0000, val_loss:0.000496, val_ap:0.8950, val_acc:0.9004, val_auc:0.9499\n",
      "In epoch:016|batch:0050, val_loss:0.000437, val_ap:0.9395, val_acc:0.9199, val_auc:0.9751\n",
      "In epoch:016|batch:0100, val_loss:0.000430, val_ap:0.8905, val_acc:0.9023, val_auc:0.9559\n",
      "In epoch:016|batch:0150, val_loss:0.000434, val_ap:0.8763, val_acc:0.9102, val_auc:0.9497\n",
      "G: 0.920565, D: 0.650547, ALL: 0.785556\n",
      "EarlyStoper count: 04\n",
      "Early Stopping!\n",
      "Best val_metric is: 0.7856819\n",
      "Inferring validation data...\n",
      "In epoch:016|batch:0000, val_loss:0.000436, val_ap:0.9251, val_acc:0.8906, val_auc:0.9657\n",
      "In epoch:016|batch:0050, val_loss:0.000436, val_ap:0.8764, val_acc:0.8906, val_auc:0.9522\n",
      "In epoch:016|batch:0100, val_loss:0.000431, val_ap:0.8660, val_acc:0.8867, val_auc:0.9479\n",
      "In epoch:016|batch:0150, val_loss:0.000434, val_ap:0.8898, val_acc:0.9062, val_auc:0.9599\n",
      "\n",
      "G: 0.920567, D: 0.650797, ALL: 0.785682\n",
      "Fold 3 CV= 0.7856818815935978\n",
      "\n",
      "#########################\n",
      "### Fold 4 with valid files [7, 8]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "In epoch:000|batch:0000, train_loss:0.638713, train_ap:0.4383, train_acc:0.7363, train_auc:0.6349\n",
      "In epoch:000|batch:0050, train_loss:0.338211, train_ap:0.8914, train_acc:0.8926, train_auc:0.9583\n",
      "In epoch:000|batch:0100, train_loss:0.293714, train_ap:0.8614, train_acc:0.8809, train_auc:0.9447\n",
      "In epoch:000|batch:0150, train_loss:0.275706, train_ap:0.8720, train_acc:0.8770, train_auc:0.9494\n",
      "In epoch:000|batch:0200, train_loss:0.268569, train_ap:0.8731, train_acc:0.8984, train_auc:0.9534\n",
      "In epoch:000|batch:0250, train_loss:0.262796, train_ap:0.8469, train_acc:0.8711, train_auc:0.9350\n",
      "In epoch:000|batch:0300, train_loss:0.258462, train_ap:0.8946, train_acc:0.9121, train_auc:0.9618\n",
      "In epoch:000|batch:0350, train_loss:0.255328, train_ap:0.8994, train_acc:0.9121, train_auc:0.9643\n",
      "In epoch:000|batch:0400, train_loss:0.252443, train_ap:0.9066, train_acc:0.8848, train_auc:0.9527\n",
      "In epoch:000|batch:0450, train_loss:0.250287, train_ap:0.8713, train_acc:0.8730, train_auc:0.9444\n",
      "In epoch:000|batch:0500, train_loss:0.248576, train_ap:0.8742, train_acc:0.8828, train_auc:0.9480\n",
      "In epoch:000|batch:0550, train_loss:0.247073, train_ap:0.8727, train_acc:0.8770, train_auc:0.9437\n",
      "In epoch:000|batch:0600, train_loss:0.246111, train_ap:0.9023, train_acc:0.8945, train_auc:0.9655\n",
      "In epoch:000|batch:0650, train_loss:0.245344, train_ap:0.8648, train_acc:0.9004, train_auc:0.9453\n",
      "In epoch:000|batch:0700, train_loss:0.244277, train_ap:0.8789, train_acc:0.8867, train_auc:0.9521\n",
      "In epoch:000|batch:0000, val_loss:0.000452, val_ap:0.8987, val_acc:0.9043, val_auc:0.9603\n",
      "In epoch:000|batch:0050, val_loss:0.000447, val_ap:0.8561, val_acc:0.8809, val_auc:0.9477\n",
      "In epoch:000|batch:0100, val_loss:0.000448, val_ap:0.8643, val_acc:0.9062, val_auc:0.9572\n",
      "In epoch:000|batch:0150, val_loss:0.000447, val_ap:0.9017, val_acc:0.8984, val_auc:0.9660\n",
      "G: 0.914517, D: 0.634032, ALL: 0.774274\n",
      "In epoch:001|batch:0000, train_loss:0.226744, train_ap:0.8930, train_acc:0.8887, train_auc:0.9602\n",
      "In epoch:001|batch:0050, train_loss:0.228492, train_ap:0.9095, train_acc:0.9199, train_auc:0.9711\n",
      "In epoch:001|batch:0100, train_loss:0.229380, train_ap:0.8618, train_acc:0.8867, train_auc:0.9479\n",
      "In epoch:001|batch:0150, train_loss:0.229623, train_ap:0.8453, train_acc:0.8906, train_auc:0.9500\n",
      "In epoch:001|batch:0200, train_loss:0.229952, train_ap:0.8631, train_acc:0.8828, train_auc:0.9341\n",
      "In epoch:001|batch:0250, train_loss:0.230553, train_ap:0.9091, train_acc:0.9043, train_auc:0.9632\n",
      "In epoch:001|batch:0300, train_loss:0.229714, train_ap:0.8963, train_acc:0.9141, train_auc:0.9604\n",
      "In epoch:001|batch:0350, train_loss:0.229155, train_ap:0.8878, train_acc:0.9062, train_auc:0.9593\n",
      "In epoch:001|batch:0400, train_loss:0.229285, train_ap:0.8994, train_acc:0.8945, train_auc:0.9578\n",
      "In epoch:001|batch:0450, train_loss:0.229348, train_ap:0.8757, train_acc:0.8848, train_auc:0.9528\n",
      "In epoch:001|batch:0500, train_loss:0.229755, train_ap:0.9056, train_acc:0.8965, train_auc:0.9591\n",
      "In epoch:001|batch:0550, train_loss:0.229604, train_ap:0.9127, train_acc:0.9180, train_auc:0.9694\n",
      "In epoch:001|batch:0600, train_loss:0.229882, train_ap:0.9075, train_acc:0.8984, train_auc:0.9534\n",
      "In epoch:001|batch:0650, train_loss:0.229767, train_ap:0.8927, train_acc:0.8906, train_auc:0.9550\n",
      "In epoch:001|batch:0700, train_loss:0.229791, train_ap:0.8710, train_acc:0.8691, train_auc:0.9454\n",
      "In epoch:001|batch:0000, val_loss:0.000416, val_ap:0.9265, val_acc:0.8984, val_auc:0.9654\n",
      "In epoch:001|batch:0050, val_loss:0.000449, val_ap:0.8566, val_acc:0.8887, val_auc:0.9588\n",
      "In epoch:001|batch:0100, val_loss:0.000444, val_ap:0.8719, val_acc:0.8887, val_auc:0.9570\n",
      "In epoch:001|batch:0150, val_loss:0.000444, val_ap:0.8988, val_acc:0.9082, val_auc:0.9625\n",
      "G: 0.916121, D: 0.637212, ALL: 0.776666\n",
      "In epoch:002|batch:0000, train_loss:0.220593, train_ap:0.9244, train_acc:0.9102, train_auc:0.9665\n",
      "In epoch:002|batch:0050, train_loss:0.230214, train_ap:0.8703, train_acc:0.8906, train_auc:0.9496\n",
      "In epoch:002|batch:0100, train_loss:0.228290, train_ap:0.8717, train_acc:0.8691, train_auc:0.9479\n",
      "In epoch:002|batch:0150, train_loss:0.226135, train_ap:0.9374, train_acc:0.9336, train_auc:0.9769\n",
      "In epoch:002|batch:0200, train_loss:0.226291, train_ap:0.8976, train_acc:0.8945, train_auc:0.9611\n",
      "In epoch:002|batch:0250, train_loss:0.227285, train_ap:0.8862, train_acc:0.9043, train_auc:0.9563\n",
      "In epoch:002|batch:0300, train_loss:0.226847, train_ap:0.9252, train_acc:0.9102, train_auc:0.9694\n",
      "In epoch:002|batch:0350, train_loss:0.227299, train_ap:0.8645, train_acc:0.8848, train_auc:0.9494\n",
      "In epoch:002|batch:0400, train_loss:0.226986, train_ap:0.8976, train_acc:0.8965, train_auc:0.9612\n",
      "In epoch:002|batch:0450, train_loss:0.227696, train_ap:0.8660, train_acc:0.8887, train_auc:0.9466\n",
      "In epoch:002|batch:0500, train_loss:0.227542, train_ap:0.8916, train_acc:0.9082, train_auc:0.9597\n",
      "In epoch:002|batch:0550, train_loss:0.227010, train_ap:0.9032, train_acc:0.8867, train_auc:0.9600\n",
      "In epoch:002|batch:0600, train_loss:0.227197, train_ap:0.9064, train_acc:0.9121, train_auc:0.9634\n",
      "In epoch:002|batch:0650, train_loss:0.226936, train_ap:0.8812, train_acc:0.8789, train_auc:0.9512\n",
      "In epoch:002|batch:0700, train_loss:0.226923, train_ap:0.8974, train_acc:0.8906, train_auc:0.9551\n",
      "In epoch:002|batch:0000, val_loss:0.000358, val_ap:0.9002, val_acc:0.9043, val_auc:0.9687\n",
      "In epoch:002|batch:0050, val_loss:0.000442, val_ap:0.8691, val_acc:0.8867, val_auc:0.9515\n",
      "In epoch:002|batch:0100, val_loss:0.000447, val_ap:0.8768, val_acc:0.8809, val_auc:0.9530\n",
      "In epoch:002|batch:0150, val_loss:0.000444, val_ap:0.8436, val_acc:0.8730, val_auc:0.9409\n",
      "G: 0.917401, D: 0.643148, ALL: 0.780274\n",
      "In epoch:003|batch:0000, train_loss:0.253122, train_ap:0.8876, train_acc:0.8926, train_auc:0.9515\n",
      "In epoch:003|batch:0050, train_loss:0.228188, train_ap:0.8809, train_acc:0.8770, train_auc:0.9509\n",
      "In epoch:003|batch:0100, train_loss:0.227258, train_ap:0.9376, train_acc:0.9160, train_auc:0.9683\n",
      "In epoch:003|batch:0150, train_loss:0.226631, train_ap:0.9174, train_acc:0.9004, train_auc:0.9629\n",
      "In epoch:003|batch:0200, train_loss:0.227032, train_ap:0.9074, train_acc:0.9004, train_auc:0.9649\n",
      "In epoch:003|batch:0250, train_loss:0.226878, train_ap:0.8874, train_acc:0.8809, train_auc:0.9519\n",
      "In epoch:003|batch:0300, train_loss:0.227042, train_ap:0.9253, train_acc:0.9121, train_auc:0.9670\n",
      "In epoch:003|batch:0350, train_loss:0.226930, train_ap:0.8928, train_acc:0.9023, train_auc:0.9659\n",
      "In epoch:003|batch:0400, train_loss:0.226841, train_ap:0.9238, train_acc:0.9102, train_auc:0.9668\n",
      "In epoch:003|batch:0450, train_loss:0.226295, train_ap:0.8791, train_acc:0.8984, train_auc:0.9547\n",
      "In epoch:003|batch:0500, train_loss:0.226622, train_ap:0.8977, train_acc:0.9043, train_auc:0.9671\n",
      "In epoch:003|batch:0550, train_loss:0.226497, train_ap:0.9223, train_acc:0.9082, train_auc:0.9646\n",
      "In epoch:003|batch:0600, train_loss:0.226012, train_ap:0.9210, train_acc:0.9062, train_auc:0.9700\n",
      "In epoch:003|batch:0650, train_loss:0.225660, train_ap:0.8896, train_acc:0.8984, train_auc:0.9622\n",
      "In epoch:003|batch:0700, train_loss:0.225099, train_ap:0.9040, train_acc:0.9023, train_auc:0.9607\n",
      "In epoch:003|batch:0000, val_loss:0.000525, val_ap:0.8828, val_acc:0.8730, val_auc:0.9436\n",
      "In epoch:003|batch:0050, val_loss:0.000442, val_ap:0.9246, val_acc:0.9043, val_auc:0.9678\n",
      "In epoch:003|batch:0100, val_loss:0.000440, val_ap:0.9160, val_acc:0.9082, val_auc:0.9644\n",
      "In epoch:003|batch:0150, val_loss:0.000439, val_ap:0.9098, val_acc:0.9141, val_auc:0.9649\n",
      "G: 0.918621, D: 0.645692, ALL: 0.782157\n",
      "In epoch:004|batch:0000, train_loss:0.164904, train_ap:0.9421, train_acc:0.9473, train_auc:0.9784\n",
      "In epoch:004|batch:0050, train_loss:0.222528, train_ap:0.9002, train_acc:0.9023, train_auc:0.9587\n",
      "In epoch:004|batch:0100, train_loss:0.221795, train_ap:0.8896, train_acc:0.9199, train_auc:0.9575\n",
      "In epoch:004|batch:0150, train_loss:0.222747, train_ap:0.8982, train_acc:0.9023, train_auc:0.9578\n",
      "In epoch:004|batch:0200, train_loss:0.223453, train_ap:0.8838, train_acc:0.8828, train_auc:0.9554\n",
      "In epoch:004|batch:0250, train_loss:0.223944, train_ap:0.9197, train_acc:0.9219, train_auc:0.9740\n",
      "In epoch:004|batch:0300, train_loss:0.224173, train_ap:0.8889, train_acc:0.9082, train_auc:0.9599\n",
      "In epoch:004|batch:0350, train_loss:0.223976, train_ap:0.8991, train_acc:0.8730, train_auc:0.9490\n",
      "In epoch:004|batch:0400, train_loss:0.224256, train_ap:0.9254, train_acc:0.9062, train_auc:0.9653\n",
      "In epoch:004|batch:0450, train_loss:0.223955, train_ap:0.8844, train_acc:0.8867, train_auc:0.9578\n",
      "In epoch:004|batch:0500, train_loss:0.223477, train_ap:0.8877, train_acc:0.8848, train_auc:0.9544\n",
      "In epoch:004|batch:0550, train_loss:0.223156, train_ap:0.8983, train_acc:0.9023, train_auc:0.9595\n",
      "In epoch:004|batch:0600, train_loss:0.223613, train_ap:0.9037, train_acc:0.8906, train_auc:0.9528\n",
      "In epoch:004|batch:0650, train_loss:0.223356, train_ap:0.8904, train_acc:0.8906, train_auc:0.9615\n",
      "In epoch:004|batch:0700, train_loss:0.223674, train_ap:0.8904, train_acc:0.8848, train_auc:0.9523\n",
      "In epoch:004|batch:0000, val_loss:0.000479, val_ap:0.8774, val_acc:0.8906, val_auc:0.9483\n",
      "In epoch:004|batch:0050, val_loss:0.000436, val_ap:0.8794, val_acc:0.9004, val_auc:0.9562\n",
      "In epoch:004|batch:0100, val_loss:0.000432, val_ap:0.9097, val_acc:0.9023, val_auc:0.9631\n",
      "In epoch:004|batch:0150, val_loss:0.000434, val_ap:0.8495, val_acc:0.9043, val_auc:0.9497\n",
      "G: 0.919514, D: 0.646243, ALL: 0.782878\n",
      "In epoch:005|batch:0000, train_loss:0.205616, train_ap:0.9042, train_acc:0.9082, train_auc:0.9662\n",
      "In epoch:005|batch:0050, train_loss:0.220151, train_ap:0.8771, train_acc:0.8770, train_auc:0.9501\n",
      "In epoch:005|batch:0100, train_loss:0.219612, train_ap:0.9104, train_acc:0.9082, train_auc:0.9650\n",
      "In epoch:005|batch:0150, train_loss:0.219804, train_ap:0.8750, train_acc:0.8984, train_auc:0.9557\n",
      "In epoch:005|batch:0200, train_loss:0.217878, train_ap:0.8811, train_acc:0.8965, train_auc:0.9557\n",
      "In epoch:005|batch:0250, train_loss:0.218425, train_ap:0.9019, train_acc:0.9062, train_auc:0.9626\n",
      "In epoch:005|batch:0300, train_loss:0.217810, train_ap:0.9063, train_acc:0.9121, train_auc:0.9623\n",
      "In epoch:005|batch:0350, train_loss:0.217531, train_ap:0.8929, train_acc:0.8926, train_auc:0.9592\n",
      "In epoch:005|batch:0400, train_loss:0.217153, train_ap:0.9199, train_acc:0.9316, train_auc:0.9736\n",
      "In epoch:005|batch:0450, train_loss:0.217823, train_ap:0.8822, train_acc:0.8848, train_auc:0.9509\n",
      "In epoch:005|batch:0500, train_loss:0.217623, train_ap:0.8995, train_acc:0.9141, train_auc:0.9636\n",
      "In epoch:005|batch:0550, train_loss:0.217198, train_ap:0.9051, train_acc:0.9082, train_auc:0.9718\n",
      "In epoch:005|batch:0600, train_loss:0.217355, train_ap:0.9064, train_acc:0.8906, train_auc:0.9613\n",
      "In epoch:005|batch:0650, train_loss:0.217718, train_ap:0.8650, train_acc:0.8711, train_auc:0.9493\n",
      "In epoch:005|batch:0700, train_loss:0.218252, train_ap:0.8763, train_acc:0.8672, train_auc:0.9410\n",
      "In epoch:005|batch:0000, val_loss:0.000459, val_ap:0.8964, val_acc:0.9004, val_auc:0.9581\n",
      "In epoch:005|batch:0050, val_loss:0.000427, val_ap:0.8989, val_acc:0.9102, val_auc:0.9572\n",
      "In epoch:005|batch:0100, val_loss:0.000431, val_ap:0.9230, val_acc:0.9121, val_auc:0.9673\n",
      "In epoch:005|batch:0150, val_loss:0.000430, val_ap:0.9363, val_acc:0.9121, val_auc:0.9743\n",
      "G: 0.920597, D: 0.650187, ALL: 0.785392\n",
      "In epoch:006|batch:0000, train_loss:0.195219, train_ap:0.8981, train_acc:0.9160, train_auc:0.9675\n",
      "In epoch:006|batch:0050, train_loss:0.217299, train_ap:0.8851, train_acc:0.8867, train_auc:0.9554\n",
      "In epoch:006|batch:0100, train_loss:0.218712, train_ap:0.8971, train_acc:0.8867, train_auc:0.9562\n",
      "In epoch:006|batch:0150, train_loss:0.216694, train_ap:0.9091, train_acc:0.8945, train_auc:0.9678\n",
      "In epoch:006|batch:0200, train_loss:0.214944, train_ap:0.9013, train_acc:0.9004, train_auc:0.9612\n",
      "In epoch:006|batch:0250, train_loss:0.216126, train_ap:0.8670, train_acc:0.9023, train_auc:0.9533\n",
      "In epoch:006|batch:0300, train_loss:0.216405, train_ap:0.8676, train_acc:0.8926, train_auc:0.9505\n",
      "In epoch:006|batch:0350, train_loss:0.216350, train_ap:0.8994, train_acc:0.9062, train_auc:0.9633\n",
      "In epoch:006|batch:0400, train_loss:0.216533, train_ap:0.9127, train_acc:0.8906, train_auc:0.9659\n",
      "In epoch:006|batch:0450, train_loss:0.217234, train_ap:0.8818, train_acc:0.8809, train_auc:0.9571\n",
      "In epoch:006|batch:0500, train_loss:0.216547, train_ap:0.8998, train_acc:0.9238, train_auc:0.9609\n",
      "In epoch:006|batch:0550, train_loss:0.216728, train_ap:0.9131, train_acc:0.9023, train_auc:0.9613\n",
      "In epoch:006|batch:0600, train_loss:0.216538, train_ap:0.8947, train_acc:0.8965, train_auc:0.9607\n",
      "In epoch:006|batch:0650, train_loss:0.216964, train_ap:0.8807, train_acc:0.8965, train_auc:0.9523\n",
      "In epoch:006|batch:0700, train_loss:0.216852, train_ap:0.8755, train_acc:0.9004, train_auc:0.9553\n",
      "In epoch:006|batch:0000, val_loss:0.000396, val_ap:0.9135, val_acc:0.9199, val_auc:0.9670\n",
      "In epoch:006|batch:0050, val_loss:0.000431, val_ap:0.8792, val_acc:0.8809, val_auc:0.9542\n",
      "In epoch:006|batch:0100, val_loss:0.000430, val_ap:0.8828, val_acc:0.8965, val_auc:0.9573\n",
      "In epoch:006|batch:0150, val_loss:0.000431, val_ap:0.9077, val_acc:0.9141, val_auc:0.9646\n",
      "G: 0.920863, D: 0.651331, ALL: 0.786097\n",
      "In epoch:007|batch:0000, train_loss:0.213029, train_ap:0.9203, train_acc:0.9180, train_auc:0.9647\n",
      "In epoch:007|batch:0050, train_loss:0.218378, train_ap:0.8760, train_acc:0.8887, train_auc:0.9580\n",
      "In epoch:007|batch:0100, train_loss:0.217323, train_ap:0.9223, train_acc:0.9062, train_auc:0.9678\n",
      "In epoch:007|batch:0150, train_loss:0.215729, train_ap:0.9296, train_acc:0.9277, train_auc:0.9716\n",
      "In epoch:007|batch:0200, train_loss:0.216165, train_ap:0.9073, train_acc:0.9062, train_auc:0.9651\n",
      "In epoch:007|batch:0250, train_loss:0.215086, train_ap:0.8911, train_acc:0.8867, train_auc:0.9600\n",
      "In epoch:007|batch:0300, train_loss:0.215440, train_ap:0.8953, train_acc:0.9082, train_auc:0.9639\n",
      "In epoch:007|batch:0350, train_loss:0.216048, train_ap:0.8896, train_acc:0.8965, train_auc:0.9540\n",
      "In epoch:007|batch:0400, train_loss:0.215980, train_ap:0.8951, train_acc:0.8730, train_auc:0.9565\n",
      "In epoch:007|batch:0450, train_loss:0.215765, train_ap:0.9109, train_acc:0.9121, train_auc:0.9673\n",
      "In epoch:007|batch:0500, train_loss:0.215750, train_ap:0.8599, train_acc:0.9023, train_auc:0.9539\n",
      "In epoch:007|batch:0550, train_loss:0.215999, train_ap:0.9052, train_acc:0.8984, train_auc:0.9562\n",
      "In epoch:007|batch:0600, train_loss:0.216280, train_ap:0.8747, train_acc:0.8711, train_auc:0.9434\n",
      "In epoch:007|batch:0650, train_loss:0.215893, train_ap:0.9125, train_acc:0.8945, train_auc:0.9639\n",
      "In epoch:007|batch:0700, train_loss:0.215827, train_ap:0.8987, train_acc:0.9082, train_auc:0.9617\n",
      "In epoch:007|batch:0000, val_loss:0.000489, val_ap:0.8915, val_acc:0.8867, val_auc:0.9520\n",
      "In epoch:007|batch:0050, val_loss:0.000427, val_ap:0.9220, val_acc:0.9219, val_auc:0.9671\n",
      "In epoch:007|batch:0100, val_loss:0.000429, val_ap:0.9017, val_acc:0.9023, val_auc:0.9611\n",
      "In epoch:007|batch:0150, val_loss:0.000430, val_ap:0.9183, val_acc:0.9277, val_auc:0.9709\n",
      "G: 0.920869, D: 0.650483, ALL: 0.785676\n",
      "EarlyStoper count: 01\n",
      "In epoch:008|batch:0000, train_loss:0.211412, train_ap:0.9074, train_acc:0.9160, train_auc:0.9656\n",
      "In epoch:008|batch:0050, train_loss:0.216663, train_ap:0.8890, train_acc:0.8984, train_auc:0.9602\n",
      "In epoch:008|batch:0100, train_loss:0.218741, train_ap:0.9196, train_acc:0.9023, train_auc:0.9679\n",
      "In epoch:008|batch:0150, train_loss:0.217378, train_ap:0.9095, train_acc:0.9062, train_auc:0.9662\n",
      "In epoch:008|batch:0200, train_loss:0.216194, train_ap:0.9121, train_acc:0.9160, train_auc:0.9709\n",
      "In epoch:008|batch:0250, train_loss:0.216189, train_ap:0.9224, train_acc:0.9004, train_auc:0.9698\n",
      "In epoch:008|batch:0300, train_loss:0.215971, train_ap:0.8725, train_acc:0.8945, train_auc:0.9468\n",
      "In epoch:008|batch:0350, train_loss:0.215821, train_ap:0.8962, train_acc:0.9082, train_auc:0.9616\n",
      "In epoch:008|batch:0400, train_loss:0.215447, train_ap:0.8911, train_acc:0.9082, train_auc:0.9547\n",
      "In epoch:008|batch:0450, train_loss:0.214985, train_ap:0.9251, train_acc:0.9141, train_auc:0.9726\n",
      "In epoch:008|batch:0500, train_loss:0.214496, train_ap:0.9080, train_acc:0.9082, train_auc:0.9645\n",
      "In epoch:008|batch:0550, train_loss:0.214737, train_ap:0.9142, train_acc:0.9004, train_auc:0.9655\n",
      "In epoch:008|batch:0600, train_loss:0.214902, train_ap:0.8981, train_acc:0.8906, train_auc:0.9572\n",
      "In epoch:008|batch:0650, train_loss:0.215623, train_ap:0.9120, train_acc:0.8945, train_auc:0.9631\n",
      "In epoch:008|batch:0700, train_loss:0.215535, train_ap:0.9351, train_acc:0.9141, train_auc:0.9736\n",
      "In epoch:008|batch:0000, val_loss:0.000414, val_ap:0.9019, val_acc:0.9043, val_auc:0.9633\n",
      "In epoch:008|batch:0050, val_loss:0.000426, val_ap:0.9135, val_acc:0.9023, val_auc:0.9650\n",
      "In epoch:008|batch:0100, val_loss:0.000429, val_ap:0.8768, val_acc:0.8965, val_auc:0.9585\n",
      "In epoch:008|batch:0150, val_loss:0.000431, val_ap:0.9232, val_acc:0.9043, val_auc:0.9681\n",
      "G: 0.920855, D: 0.650865, ALL: 0.785860\n",
      "EarlyStoper count: 02\n",
      "In epoch:009|batch:0000, train_loss:0.226224, train_ap:0.8921, train_acc:0.9141, train_auc:0.9568\n",
      "In epoch:009|batch:0050, train_loss:0.215909, train_ap:0.9294, train_acc:0.9102, train_auc:0.9720\n",
      "In epoch:009|batch:0100, train_loss:0.216641, train_ap:0.9249, train_acc:0.8945, train_auc:0.9647\n",
      "In epoch:009|batch:0150, train_loss:0.217947, train_ap:0.9135, train_acc:0.9141, train_auc:0.9684\n",
      "In epoch:009|batch:0200, train_loss:0.216975, train_ap:0.9050, train_acc:0.9121, train_auc:0.9702\n",
      "In epoch:009|batch:0250, train_loss:0.217424, train_ap:0.9306, train_acc:0.9121, train_auc:0.9694\n",
      "In epoch:009|batch:0300, train_loss:0.216844, train_ap:0.9353, train_acc:0.9082, train_auc:0.9688\n",
      "In epoch:009|batch:0350, train_loss:0.216005, train_ap:0.9210, train_acc:0.9043, train_auc:0.9608\n",
      "In epoch:009|batch:0400, train_loss:0.214903, train_ap:0.9053, train_acc:0.8965, train_auc:0.9620\n",
      "In epoch:009|batch:0450, train_loss:0.214689, train_ap:0.8845, train_acc:0.8887, train_auc:0.9523\n",
      "In epoch:009|batch:0500, train_loss:0.214944, train_ap:0.8393, train_acc:0.8672, train_auc:0.9389\n",
      "In epoch:009|batch:0550, train_loss:0.215264, train_ap:0.8827, train_acc:0.9043, train_auc:0.9577\n",
      "In epoch:009|batch:0600, train_loss:0.215230, train_ap:0.9120, train_acc:0.9180, train_auc:0.9706\n",
      "In epoch:009|batch:0650, train_loss:0.215008, train_ap:0.8686, train_acc:0.8848, train_auc:0.9516\n",
      "In epoch:009|batch:0700, train_loss:0.215351, train_ap:0.8978, train_acc:0.8828, train_auc:0.9540\n",
      "In epoch:009|batch:0000, val_loss:0.000404, val_ap:0.9139, val_acc:0.9082, val_auc:0.9669\n",
      "In epoch:009|batch:0050, val_loss:0.000429, val_ap:0.9021, val_acc:0.8867, val_auc:0.9548\n",
      "In epoch:009|batch:0100, val_loss:0.000431, val_ap:0.9197, val_acc:0.9023, val_auc:0.9724\n",
      "In epoch:009|batch:0150, val_loss:0.000431, val_ap:0.8755, val_acc:0.8887, val_auc:0.9540\n",
      "G: 0.920855, D: 0.650780, ALL: 0.785818\n",
      "EarlyStoper count: 03\n",
      "In epoch:010|batch:0000, train_loss:0.209436, train_ap:0.8981, train_acc:0.9102, train_auc:0.9621\n",
      "In epoch:010|batch:0050, train_loss:0.220485, train_ap:0.9084, train_acc:0.8945, train_auc:0.9586\n",
      "In epoch:010|batch:0100, train_loss:0.217695, train_ap:0.9209, train_acc:0.9199, train_auc:0.9721\n",
      "In epoch:010|batch:0150, train_loss:0.217289, train_ap:0.9059, train_acc:0.9102, train_auc:0.9629\n",
      "In epoch:010|batch:0200, train_loss:0.216559, train_ap:0.8706, train_acc:0.9023, train_auc:0.9604\n",
      "In epoch:010|batch:0250, train_loss:0.217016, train_ap:0.9157, train_acc:0.9043, train_auc:0.9609\n",
      "In epoch:010|batch:0300, train_loss:0.216695, train_ap:0.8963, train_acc:0.8984, train_auc:0.9588\n",
      "In epoch:010|batch:0350, train_loss:0.216185, train_ap:0.8918, train_acc:0.9062, train_auc:0.9626\n",
      "In epoch:010|batch:0400, train_loss:0.216006, train_ap:0.8777, train_acc:0.8926, train_auc:0.9533\n",
      "In epoch:010|batch:0450, train_loss:0.215839, train_ap:0.9104, train_acc:0.9238, train_auc:0.9674\n",
      "In epoch:010|batch:0500, train_loss:0.215620, train_ap:0.9250, train_acc:0.9043, train_auc:0.9683\n",
      "In epoch:010|batch:0550, train_loss:0.215905, train_ap:0.8772, train_acc:0.9102, train_auc:0.9503\n",
      "In epoch:010|batch:0600, train_loss:0.216107, train_ap:0.9083, train_acc:0.9023, train_auc:0.9659\n",
      "In epoch:010|batch:0650, train_loss:0.215888, train_ap:0.9283, train_acc:0.9316, train_auc:0.9737\n",
      "In epoch:010|batch:0700, train_loss:0.215498, train_ap:0.9058, train_acc:0.9121, train_auc:0.9644\n",
      "In epoch:010|batch:0000, val_loss:0.000422, val_ap:0.9326, val_acc:0.9043, val_auc:0.9665\n",
      "In epoch:010|batch:0050, val_loss:0.000426, val_ap:0.9092, val_acc:0.8945, val_auc:0.9603\n",
      "In epoch:010|batch:0100, val_loss:0.000430, val_ap:0.9025, val_acc:0.8984, val_auc:0.9597\n",
      "In epoch:010|batch:0150, val_loss:0.000429, val_ap:0.8655, val_acc:0.8926, val_auc:0.9447\n",
      "G: 0.920854, D: 0.650738, ALL: 0.785796\n",
      "EarlyStoper count: 04\n",
      "Early Stopping!\n",
      "Best val_metric is: 0.7860971\n",
      "Inferring validation data...\n",
      "In epoch:010|batch:0000, val_loss:0.000486, val_ap:0.8853, val_acc:0.8984, val_auc:0.9520\n",
      "In epoch:010|batch:0050, val_loss:0.000420, val_ap:0.8924, val_acc:0.9023, val_auc:0.9587\n",
      "In epoch:010|batch:0100, val_loss:0.000428, val_ap:0.9153, val_acc:0.9180, val_auc:0.9690\n",
      "In epoch:010|batch:0150, val_loss:0.000428, val_ap:0.8917, val_acc:0.8906, val_auc:0.9620\n",
      "\n",
      "G: 0.920863, D: 0.651331, ALL: 0.786097\n",
      "Fold 4 CV= 0.7860971471656946\n",
      "\n",
      "#########################\n",
      "### Fold 5 with valid files [9, 10]\n",
      "### Training data shapes (367128, 13, 188) (367128,)\n",
      "### Validation data shapes (91785, 13, 188) (91785,)\n",
      "#########################\n",
      "In epoch:000|batch:0000, train_loss:0.663641, train_ap:0.4046, train_acc:0.7598, train_auc:0.6554\n",
      "In epoch:000|batch:0050, train_loss:0.352480, train_ap:0.8652, train_acc:0.8770, train_auc:0.9407\n",
      "In epoch:000|batch:0100, train_loss:0.299578, train_ap:0.9139, train_acc:0.9043, train_auc:0.9669\n",
      "In epoch:000|batch:0150, train_loss:0.281342, train_ap:0.8611, train_acc:0.8711, train_auc:0.9466\n",
      "In epoch:000|batch:0200, train_loss:0.269214, train_ap:0.8869, train_acc:0.9160, train_auc:0.9643\n",
      "In epoch:000|batch:0250, train_loss:0.262896, train_ap:0.9015, train_acc:0.9121, train_auc:0.9646\n",
      "In epoch:000|batch:0300, train_loss:0.259861, train_ap:0.8930, train_acc:0.8945, train_auc:0.9591\n",
      "In epoch:000|batch:0350, train_loss:0.256550, train_ap:0.8851, train_acc:0.8965, train_auc:0.9582\n",
      "In epoch:000|batch:0400, train_loss:0.253851, train_ap:0.8953, train_acc:0.9004, train_auc:0.9585\n",
      "In epoch:000|batch:0450, train_loss:0.251547, train_ap:0.8916, train_acc:0.9160, train_auc:0.9668\n",
      "In epoch:000|batch:0500, train_loss:0.249542, train_ap:0.9154, train_acc:0.9160, train_auc:0.9663\n",
      "In epoch:000|batch:0550, train_loss:0.248131, train_ap:0.8848, train_acc:0.8770, train_auc:0.9524\n",
      "In epoch:000|batch:0600, train_loss:0.246806, train_ap:0.9045, train_acc:0.9004, train_auc:0.9597\n",
      "In epoch:000|batch:0650, train_loss:0.245843, train_ap:0.8789, train_acc:0.9160, train_auc:0.9612\n",
      "In epoch:000|batch:0700, train_loss:0.245009, train_ap:0.8718, train_acc:0.8945, train_auc:0.9539\n",
      "In epoch:000|batch:0000, val_loss:0.000434, val_ap:0.8995, val_acc:0.9062, val_auc:0.9590\n",
      "In epoch:000|batch:0050, val_loss:0.000451, val_ap:0.8354, val_acc:0.8750, val_auc:0.9353\n",
      "In epoch:000|batch:0100, val_loss:0.000450, val_ap:0.8770, val_acc:0.9004, val_auc:0.9569\n",
      "In epoch:000|batch:0150, val_loss:0.000450, val_ap:0.8849, val_acc:0.8906, val_auc:0.9551\n",
      "G: 0.915575, D: 0.637495, ALL: 0.776535\n",
      "In epoch:001|batch:0000, train_loss:0.230491, train_ap:0.9012, train_acc:0.8965, train_auc:0.9598\n",
      "In epoch:001|batch:0050, train_loss:0.233729, train_ap:0.8870, train_acc:0.8965, train_auc:0.9494\n",
      "In epoch:001|batch:0100, train_loss:0.229740, train_ap:0.9223, train_acc:0.9082, train_auc:0.9677\n",
      "In epoch:001|batch:0150, train_loss:0.230345, train_ap:0.8939, train_acc:0.8965, train_auc:0.9564\n",
      "In epoch:001|batch:0200, train_loss:0.231011, train_ap:0.8904, train_acc:0.8984, train_auc:0.9586\n",
      "In epoch:001|batch:0250, train_loss:0.231294, train_ap:0.8965, train_acc:0.8789, train_auc:0.9587\n",
      "In epoch:001|batch:0300, train_loss:0.231024, train_ap:0.8854, train_acc:0.8945, train_auc:0.9613\n",
      "In epoch:001|batch:0350, train_loss:0.230013, train_ap:0.9182, train_acc:0.9102, train_auc:0.9681\n",
      "In epoch:001|batch:0400, train_loss:0.229907, train_ap:0.9007, train_acc:0.9180, train_auc:0.9644\n",
      "In epoch:001|batch:0450, train_loss:0.229855, train_ap:0.8747, train_acc:0.9023, train_auc:0.9507\n",
      "In epoch:001|batch:0500, train_loss:0.229667, train_ap:0.9172, train_acc:0.9160, train_auc:0.9691\n",
      "In epoch:001|batch:0550, train_loss:0.229784, train_ap:0.8929, train_acc:0.8848, train_auc:0.9541\n",
      "In epoch:001|batch:0600, train_loss:0.229477, train_ap:0.9217, train_acc:0.9023, train_auc:0.9670\n",
      "In epoch:001|batch:0650, train_loss:0.229738, train_ap:0.8714, train_acc:0.8789, train_auc:0.9498\n",
      "In epoch:001|batch:0700, train_loss:0.229875, train_ap:0.9074, train_acc:0.8926, train_auc:0.9579\n",
      "In epoch:001|batch:0000, val_loss:0.000580, val_ap:0.8541, val_acc:0.8672, val_auc:0.9337\n",
      "In epoch:001|batch:0050, val_loss:0.000456, val_ap:0.8954, val_acc:0.9004, val_auc:0.9496\n",
      "In epoch:001|batch:0100, val_loss:0.000454, val_ap:0.9065, val_acc:0.9043, val_auc:0.9584\n",
      "In epoch:001|batch:0150, val_loss:0.000452, val_ap:0.8814, val_acc:0.8926, val_auc:0.9551\n",
      "G: 0.916662, D: 0.643253, ALL: 0.779958\n",
      "In epoch:002|batch:0000, train_loss:0.223104, train_ap:0.8866, train_acc:0.9023, train_auc:0.9609\n",
      "In epoch:002|batch:0050, train_loss:0.226714, train_ap:0.8259, train_acc:0.8711, train_auc:0.9421\n",
      "In epoch:002|batch:0100, train_loss:0.226505, train_ap:0.8798, train_acc:0.9121, train_auc:0.9601\n",
      "In epoch:002|batch:0150, train_loss:0.226754, train_ap:0.8912, train_acc:0.9043, train_auc:0.9577\n",
      "In epoch:002|batch:0200, train_loss:0.225973, train_ap:0.8727, train_acc:0.9043, train_auc:0.9588\n",
      "In epoch:002|batch:0250, train_loss:0.226271, train_ap:0.8766, train_acc:0.8867, train_auc:0.9494\n",
      "In epoch:002|batch:0300, train_loss:0.227412, train_ap:0.9342, train_acc:0.9082, train_auc:0.9695\n",
      "In epoch:002|batch:0350, train_loss:0.228360, train_ap:0.9013, train_acc:0.8828, train_auc:0.9588\n",
      "In epoch:002|batch:0400, train_loss:0.228155, train_ap:0.8515, train_acc:0.8789, train_auc:0.9329\n",
      "In epoch:002|batch:0450, train_loss:0.227761, train_ap:0.8957, train_acc:0.8984, train_auc:0.9606\n",
      "In epoch:002|batch:0500, train_loss:0.228287, train_ap:0.8583, train_acc:0.8828, train_auc:0.9424\n",
      "In epoch:002|batch:0550, train_loss:0.228775, train_ap:0.8993, train_acc:0.9199, train_auc:0.9576\n",
      "In epoch:002|batch:0600, train_loss:0.228643, train_ap:0.9176, train_acc:0.9160, train_auc:0.9727\n",
      "In epoch:002|batch:0650, train_loss:0.228229, train_ap:0.8818, train_acc:0.9043, train_auc:0.9572\n",
      "In epoch:002|batch:0700, train_loss:0.227600, train_ap:0.9090, train_acc:0.9160, train_auc:0.9637\n",
      "In epoch:002|batch:0000, val_loss:0.000417, val_ap:0.8759, val_acc:0.8965, val_auc:0.9599\n",
      "In epoch:002|batch:0050, val_loss:0.000444, val_ap:0.9195, val_acc:0.9160, val_auc:0.9639\n",
      "In epoch:002|batch:0100, val_loss:0.000442, val_ap:0.8987, val_acc:0.8965, val_auc:0.9594\n",
      "In epoch:002|batch:0150, val_loss:0.000436, val_ap:0.8898, val_acc:0.9160, val_auc:0.9673\n",
      "G: 0.918642, D: 0.649985, ALL: 0.784313\n",
      "In epoch:003|batch:0000, train_loss:0.214286, train_ap:0.9098, train_acc:0.9062, train_auc:0.9630\n",
      "In epoch:003|batch:0050, train_loss:0.226464, train_ap:0.9311, train_acc:0.9199, train_auc:0.9684\n",
      "In epoch:003|batch:0100, train_loss:0.226566, train_ap:0.8611, train_acc:0.9004, train_auc:0.9583\n",
      "In epoch:003|batch:0150, train_loss:0.225999, train_ap:0.8770, train_acc:0.8848, train_auc:0.9442\n",
      "In epoch:003|batch:0200, train_loss:0.226358, train_ap:0.8868, train_acc:0.9023, train_auc:0.9526\n",
      "In epoch:003|batch:0250, train_loss:0.226328, train_ap:0.8122, train_acc:0.8789, train_auc:0.9461\n",
      "In epoch:003|batch:0300, train_loss:0.226139, train_ap:0.8838, train_acc:0.8848, train_auc:0.9532\n",
      "In epoch:003|batch:0350, train_loss:0.226013, train_ap:0.9285, train_acc:0.9062, train_auc:0.9708\n",
      "In epoch:003|batch:0400, train_loss:0.225876, train_ap:0.8904, train_acc:0.9219, train_auc:0.9645\n",
      "In epoch:003|batch:0450, train_loss:0.225356, train_ap:0.8944, train_acc:0.9082, train_auc:0.9589\n",
      "In epoch:003|batch:0500, train_loss:0.225523, train_ap:0.9049, train_acc:0.9043, train_auc:0.9653\n",
      "In epoch:003|batch:0550, train_loss:0.225426, train_ap:0.9278, train_acc:0.9102, train_auc:0.9686\n",
      "In epoch:003|batch:0600, train_loss:0.225598, train_ap:0.8706, train_acc:0.8770, train_auc:0.9518\n",
      "In epoch:003|batch:0650, train_loss:0.225557, train_ap:0.9116, train_acc:0.8945, train_auc:0.9669\n",
      "In epoch:003|batch:0700, train_loss:0.225342, train_ap:0.9069, train_acc:0.8926, train_auc:0.9624\n",
      "In epoch:003|batch:0000, val_loss:0.000457, val_ap:0.8815, val_acc:0.9004, val_auc:0.9584\n",
      "In epoch:003|batch:0050, val_loss:0.000432, val_ap:0.9188, val_acc:0.9160, val_auc:0.9687\n",
      "In epoch:003|batch:0100, val_loss:0.000436, val_ap:0.8739, val_acc:0.8926, val_auc:0.9543\n",
      "In epoch:003|batch:0150, val_loss:0.000435, val_ap:0.9248, val_acc:0.9043, val_auc:0.9642\n",
      "G: 0.919869, D: 0.654431, ALL: 0.787150\n",
      "In epoch:004|batch:0000, train_loss:0.231439, train_ap:0.8850, train_acc:0.8984, train_auc:0.9571\n",
      "In epoch:004|batch:0050, train_loss:0.223819, train_ap:0.8895, train_acc:0.8926, train_auc:0.9579\n",
      "In epoch:004|batch:0100, train_loss:0.223430, train_ap:0.8880, train_acc:0.8965, train_auc:0.9566\n",
      "In epoch:004|batch:0150, train_loss:0.222407, train_ap:0.8803, train_acc:0.8984, train_auc:0.9530\n",
      "In epoch:004|batch:0200, train_loss:0.222729, train_ap:0.9124, train_acc:0.8984, train_auc:0.9622\n",
      "In epoch:004|batch:0250, train_loss:0.221751, train_ap:0.9031, train_acc:0.8926, train_auc:0.9599\n",
      "In epoch:004|batch:0300, train_loss:0.221891, train_ap:0.8905, train_acc:0.9238, train_auc:0.9687\n",
      "In epoch:004|batch:0350, train_loss:0.222585, train_ap:0.8710, train_acc:0.8887, train_auc:0.9538\n",
      "In epoch:004|batch:0400, train_loss:0.222902, train_ap:0.8530, train_acc:0.9004, train_auc:0.9485\n",
      "In epoch:004|batch:0450, train_loss:0.222961, train_ap:0.9191, train_acc:0.9023, train_auc:0.9708\n",
      "In epoch:004|batch:0500, train_loss:0.223053, train_ap:0.9062, train_acc:0.9062, train_auc:0.9630\n",
      "In epoch:004|batch:0550, train_loss:0.223216, train_ap:0.8535, train_acc:0.8672, train_auc:0.9347\n",
      "In epoch:004|batch:0600, train_loss:0.222858, train_ap:0.9190, train_acc:0.9258, train_auc:0.9740\n",
      "In epoch:004|batch:0650, train_loss:0.223166, train_ap:0.8953, train_acc:0.8887, train_auc:0.9595\n",
      "In epoch:004|batch:0700, train_loss:0.223355, train_ap:0.9063, train_acc:0.9062, train_auc:0.9569\n",
      "In epoch:004|batch:0000, val_loss:0.000501, val_ap:0.8720, val_acc:0.8926, val_auc:0.9475\n",
      "In epoch:004|batch:0050, val_loss:0.000433, val_ap:0.8636, val_acc:0.8848, val_auc:0.9463\n",
      "In epoch:004|batch:0100, val_loss:0.000428, val_ap:0.8983, val_acc:0.9082, val_auc:0.9700\n",
      "In epoch:004|batch:0150, val_loss:0.000432, val_ap:0.9069, val_acc:0.9062, val_auc:0.9619\n",
      "G: 0.920268, D: 0.654727, ALL: 0.787498\n",
      "In epoch:005|batch:0000, train_loss:0.180136, train_ap:0.9284, train_acc:0.9336, train_auc:0.9754\n",
      "In epoch:005|batch:0050, train_loss:0.224553, train_ap:0.9021, train_acc:0.8770, train_auc:0.9555\n",
      "In epoch:005|batch:0100, train_loss:0.223460, train_ap:0.8837, train_acc:0.9180, train_auc:0.9588\n",
      "In epoch:005|batch:0150, train_loss:0.221181, train_ap:0.8886, train_acc:0.9004, train_auc:0.9581\n",
      "In epoch:005|batch:0200, train_loss:0.220875, train_ap:0.9043, train_acc:0.9102, train_auc:0.9640\n",
      "In epoch:005|batch:0250, train_loss:0.219737, train_ap:0.9088, train_acc:0.9082, train_auc:0.9651\n",
      "In epoch:005|batch:0300, train_loss:0.219482, train_ap:0.8609, train_acc:0.8926, train_auc:0.9465\n",
      "In epoch:005|batch:0350, train_loss:0.219855, train_ap:0.9142, train_acc:0.9121, train_auc:0.9663\n",
      "In epoch:005|batch:0400, train_loss:0.219507, train_ap:0.8953, train_acc:0.9141, train_auc:0.9591\n",
      "In epoch:005|batch:0450, train_loss:0.219340, train_ap:0.8973, train_acc:0.9297, train_auc:0.9662\n",
      "In epoch:005|batch:0500, train_loss:0.219445, train_ap:0.8922, train_acc:0.9004, train_auc:0.9624\n",
      "In epoch:005|batch:0550, train_loss:0.218978, train_ap:0.8987, train_acc:0.8965, train_auc:0.9624\n",
      "In epoch:005|batch:0600, train_loss:0.218624, train_ap:0.9024, train_acc:0.9062, train_auc:0.9635\n",
      "In epoch:005|batch:0650, train_loss:0.218328, train_ap:0.9096, train_acc:0.9121, train_auc:0.9618\n",
      "In epoch:005|batch:0700, train_loss:0.218281, train_ap:0.8840, train_acc:0.9102, train_auc:0.9590\n",
      "In epoch:005|batch:0000, val_loss:0.000422, val_ap:0.9090, val_acc:0.9141, val_auc:0.9606\n",
      "In epoch:005|batch:0050, val_loss:0.000421, val_ap:0.8721, val_acc:0.8926, val_auc:0.9510\n",
      "In epoch:005|batch:0100, val_loss:0.000429, val_ap:0.8340, val_acc:0.8672, val_auc:0.9347\n",
      "In epoch:005|batch:0150, val_loss:0.000428, val_ap:0.8985, val_acc:0.9277, val_auc:0.9655\n",
      "G: 0.921480, D: 0.659003, ALL: 0.790242\n",
      "In epoch:006|batch:0000, train_loss:0.200633, train_ap:0.9222, train_acc:0.9141, train_auc:0.9694\n",
      "In epoch:006|batch:0050, train_loss:0.216614, train_ap:0.9330, train_acc:0.9102, train_auc:0.9672\n",
      "In epoch:006|batch:0100, train_loss:0.216123, train_ap:0.9003, train_acc:0.8926, train_auc:0.9638\n",
      "In epoch:006|batch:0150, train_loss:0.214221, train_ap:0.9129, train_acc:0.9004, train_auc:0.9660\n",
      "In epoch:006|batch:0200, train_loss:0.215262, train_ap:0.9250, train_acc:0.9199, train_auc:0.9746\n",
      "In epoch:006|batch:0250, train_loss:0.216590, train_ap:0.9031, train_acc:0.9102, train_auc:0.9654\n",
      "In epoch:006|batch:0300, train_loss:0.216007, train_ap:0.8999, train_acc:0.8945, train_auc:0.9604\n",
      "In epoch:006|batch:0350, train_loss:0.216272, train_ap:0.8849, train_acc:0.9043, train_auc:0.9551\n",
      "In epoch:006|batch:0400, train_loss:0.215780, train_ap:0.9193, train_acc:0.9141, train_auc:0.9709\n",
      "In epoch:006|batch:0450, train_loss:0.216217, train_ap:0.8974, train_acc:0.9082, train_auc:0.9631\n",
      "In epoch:006|batch:0500, train_loss:0.215925, train_ap:0.8936, train_acc:0.8965, train_auc:0.9558\n",
      "In epoch:006|batch:0550, train_loss:0.216144, train_ap:0.9141, train_acc:0.9199, train_auc:0.9631\n",
      "In epoch:006|batch:0600, train_loss:0.216416, train_ap:0.9133, train_acc:0.9004, train_auc:0.9604\n",
      "In epoch:006|batch:0650, train_loss:0.216766, train_ap:0.8906, train_acc:0.9121, train_auc:0.9635\n",
      "In epoch:006|batch:0700, train_loss:0.216832, train_ap:0.9306, train_acc:0.9121, train_auc:0.9714\n",
      "In epoch:006|batch:0000, val_loss:0.000393, val_ap:0.8955, val_acc:0.9023, val_auc:0.9652\n",
      "In epoch:006|batch:0050, val_loss:0.000433, val_ap:0.8797, val_acc:0.9102, val_auc:0.9596\n",
      "In epoch:006|batch:0100, val_loss:0.000431, val_ap:0.8991, val_acc:0.8926, val_auc:0.9551\n",
      "In epoch:006|batch:0150, val_loss:0.000431, val_ap:0.8929, val_acc:0.9062, val_auc:0.9605\n",
      "G: 0.921430, D: 0.658326, ALL: 0.789878\n",
      "EarlyStoper count: 01\n",
      "In epoch:007|batch:0000, train_loss:0.224204, train_ap:0.8963, train_acc:0.8965, train_auc:0.9599\n",
      "In epoch:007|batch:0050, train_loss:0.217213, train_ap:0.8986, train_acc:0.9180, train_auc:0.9672\n",
      "In epoch:007|batch:0100, train_loss:0.218336, train_ap:0.9065, train_acc:0.9258, train_auc:0.9656\n",
      "In epoch:007|batch:0150, train_loss:0.217307, train_ap:0.9167, train_acc:0.9160, train_auc:0.9648\n",
      "In epoch:007|batch:0200, train_loss:0.218530, train_ap:0.9045, train_acc:0.8848, train_auc:0.9588\n",
      "In epoch:007|batch:0250, train_loss:0.217400, train_ap:0.9272, train_acc:0.9121, train_auc:0.9691\n",
      "In epoch:007|batch:0300, train_loss:0.216763, train_ap:0.8616, train_acc:0.9004, train_auc:0.9529\n",
      "In epoch:007|batch:0350, train_loss:0.216407, train_ap:0.9371, train_acc:0.9199, train_auc:0.9708\n",
      "In epoch:007|batch:0400, train_loss:0.216449, train_ap:0.9048, train_acc:0.9160, train_auc:0.9645\n",
      "In epoch:007|batch:0450, train_loss:0.216369, train_ap:0.8887, train_acc:0.8945, train_auc:0.9556\n",
      "In epoch:007|batch:0500, train_loss:0.216538, train_ap:0.9125, train_acc:0.9258, train_auc:0.9711\n",
      "In epoch:007|batch:0550, train_loss:0.216474, train_ap:0.9249, train_acc:0.9141, train_auc:0.9675\n",
      "In epoch:007|batch:0600, train_loss:0.215873, train_ap:0.8744, train_acc:0.9062, train_auc:0.9607\n",
      "In epoch:007|batch:0650, train_loss:0.215751, train_ap:0.9161, train_acc:0.9121, train_auc:0.9650\n",
      "In epoch:007|batch:0700, train_loss:0.215547, train_ap:0.9480, train_acc:0.9180, train_auc:0.9758\n",
      "In epoch:007|batch:0000, val_loss:0.000497, val_ap:0.8765, val_acc:0.8848, val_auc:0.9483\n",
      "In epoch:007|batch:0050, val_loss:0.000435, val_ap:0.9119, val_acc:0.9062, val_auc:0.9662\n",
      "In epoch:007|batch:0100, val_loss:0.000430, val_ap:0.8808, val_acc:0.8984, val_auc:0.9630\n",
      "In epoch:007|batch:0150, val_loss:0.000429, val_ap:0.8796, val_acc:0.8887, val_auc:0.9566\n",
      "G: 0.921573, D: 0.658665, ALL: 0.790119\n",
      "EarlyStoper count: 02\n",
      "In epoch:008|batch:0000, train_loss:0.217570, train_ap:0.9040, train_acc:0.9082, train_auc:0.9653\n",
      "In epoch:008|batch:0050, train_loss:0.216284, train_ap:0.9275, train_acc:0.8965, train_auc:0.9665\n",
      "In epoch:008|batch:0100, train_loss:0.214147, train_ap:0.8853, train_acc:0.8926, train_auc:0.9567\n",
      "In epoch:008|batch:0150, train_loss:0.212967, train_ap:0.9210, train_acc:0.9141, train_auc:0.9727\n",
      "In epoch:008|batch:0200, train_loss:0.213762, train_ap:0.8786, train_acc:0.9082, train_auc:0.9587\n",
      "In epoch:008|batch:0250, train_loss:0.214312, train_ap:0.8925, train_acc:0.9277, train_auc:0.9683\n",
      "In epoch:008|batch:0300, train_loss:0.213741, train_ap:0.9253, train_acc:0.9160, train_auc:0.9704\n",
      "In epoch:008|batch:0350, train_loss:0.214069, train_ap:0.9202, train_acc:0.9043, train_auc:0.9691\n",
      "In epoch:008|batch:0400, train_loss:0.214141, train_ap:0.8777, train_acc:0.9238, train_auc:0.9528\n",
      "In epoch:008|batch:0450, train_loss:0.214721, train_ap:0.8876, train_acc:0.8965, train_auc:0.9575\n",
      "In epoch:008|batch:0500, train_loss:0.214590, train_ap:0.9128, train_acc:0.9043, train_auc:0.9644\n",
      "In epoch:008|batch:0550, train_loss:0.214871, train_ap:0.8885, train_acc:0.9004, train_auc:0.9626\n",
      "In epoch:008|batch:0600, train_loss:0.214707, train_ap:0.9172, train_acc:0.9023, train_auc:0.9653\n",
      "In epoch:008|batch:0650, train_loss:0.214910, train_ap:0.8485, train_acc:0.8789, train_auc:0.9508\n",
      "In epoch:008|batch:0700, train_loss:0.215208, train_ap:0.9387, train_acc:0.9219, train_auc:0.9714\n",
      "In epoch:008|batch:0000, val_loss:0.000402, val_ap:0.9224, val_acc:0.9199, val_auc:0.9678\n",
      "In epoch:008|batch:0050, val_loss:0.000423, val_ap:0.8980, val_acc:0.9043, val_auc:0.9680\n",
      "In epoch:008|batch:0100, val_loss:0.000428, val_ap:0.9192, val_acc:0.9062, val_auc:0.9690\n",
      "In epoch:008|batch:0150, val_loss:0.000428, val_ap:0.9270, val_acc:0.9141, val_auc:0.9668\n",
      "G: 0.921580, D: 0.658495, ALL: 0.790038\n",
      "EarlyStoper count: 03\n",
      "In epoch:009|batch:0000, train_loss:0.225635, train_ap:0.8901, train_acc:0.8906, train_auc:0.9596\n",
      "In epoch:009|batch:0050, train_loss:0.217643, train_ap:0.8700, train_acc:0.8809, train_auc:0.9512\n",
      "In epoch:009|batch:0100, train_loss:0.215461, train_ap:0.9054, train_acc:0.9004, train_auc:0.9647\n",
      "In epoch:009|batch:0150, train_loss:0.215474, train_ap:0.9000, train_acc:0.9199, train_auc:0.9667\n",
      "In epoch:009|batch:0200, train_loss:0.215635, train_ap:0.8926, train_acc:0.9238, train_auc:0.9658\n",
      "In epoch:009|batch:0250, train_loss:0.216074, train_ap:0.8955, train_acc:0.8984, train_auc:0.9654\n",
      "In epoch:009|batch:0300, train_loss:0.215072, train_ap:0.9233, train_acc:0.9199, train_auc:0.9691\n",
      "In epoch:009|batch:0350, train_loss:0.214670, train_ap:0.9065, train_acc:0.9062, train_auc:0.9676\n",
      "In epoch:009|batch:0400, train_loss:0.214544, train_ap:0.8686, train_acc:0.8750, train_auc:0.9499\n",
      "In epoch:009|batch:0450, train_loss:0.214742, train_ap:0.8987, train_acc:0.9141, train_auc:0.9637\n",
      "In epoch:009|batch:0500, train_loss:0.214395, train_ap:0.8740, train_acc:0.9062, train_auc:0.9520\n",
      "In epoch:009|batch:0550, train_loss:0.215005, train_ap:0.9081, train_acc:0.8945, train_auc:0.9577\n",
      "In epoch:009|batch:0600, train_loss:0.215051, train_ap:0.9275, train_acc:0.9004, train_auc:0.9707\n",
      "In epoch:009|batch:0650, train_loss:0.215317, train_ap:0.9002, train_acc:0.8809, train_auc:0.9595\n",
      "In epoch:009|batch:0700, train_loss:0.215463, train_ap:0.8843, train_acc:0.8984, train_auc:0.9563\n",
      "In epoch:009|batch:0000, val_loss:0.000393, val_ap:0.9321, val_acc:0.9121, val_auc:0.9687\n",
      "In epoch:009|batch:0050, val_loss:0.000425, val_ap:0.9092, val_acc:0.8945, val_auc:0.9606\n",
      "In epoch:009|batch:0100, val_loss:0.000430, val_ap:0.9157, val_acc:0.9160, val_auc:0.9618\n",
      "In epoch:009|batch:0150, val_loss:0.000430, val_ap:0.8747, val_acc:0.8867, val_auc:0.9492\n",
      "G: 0.921584, D: 0.658792, ALL: 0.790188\n",
      "EarlyStoper count: 04\n",
      "Early Stopping!\n",
      "Best val_metric is: 0.7902415\n",
      "Inferring validation data...\n",
      "In epoch:009|batch:0000, val_loss:0.000424, val_ap:0.8997, val_acc:0.9141, val_auc:0.9613\n",
      "In epoch:009|batch:0050, val_loss:0.000430, val_ap:0.8673, val_acc:0.8848, val_auc:0.9507\n",
      "In epoch:009|batch:0100, val_loss:0.000426, val_ap:0.8952, val_acc:0.9062, val_auc:0.9603\n",
      "In epoch:009|batch:0150, val_loss:0.000428, val_ap:0.8956, val_acc:0.9023, val_auc:0.9612\n",
      "\n",
      "G: 0.921480, D: 0.659003, ALL: 0.790242\n",
      "Fold 5 CV= 0.7902415417865801\n",
      "\n",
      "#########################\n",
      "G: 0.919940, D: 0.651017, ALL: 0.785478\n",
      "Overall CV = 0.785478282057473\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'model': 'gru_model',\n",
    "    'batch_size': 512,\n",
    "    'lr': 0.002,\n",
    "    'wd': 1e-5,\n",
    "    #'device': 'cpu',\n",
    "    'device': 'cuda:0',\n",
    "    'early_stopping': 4,\n",
    "    'n_fold': 5,\n",
    "    'seed': 2021,\n",
    "    'max_epochs': 20,\n",
    "}\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    # SAVE TRUE AND OOF\n",
    "    device = params['device']\n",
    "    true = np.array([])\n",
    "    oof = np.array([])\n",
    "\n",
    "    for fold in range(5):\n",
    "\n",
    "        # INDICES OF TRAIN AND VALID FOLDS\n",
    "        valid_idx = [2*fold+1, 2*fold+2]\n",
    "        train_idx = [x for x in [1,2,3,4,5,6,7,8,9,10] if x not in valid_idx]\n",
    "\n",
    "        print('#'*25)\n",
    "        print(f'### Fold {fold+1} with valid files', valid_idx)\n",
    "\n",
    "        # READ TRAIN DATA FROM DISK\n",
    "        X_train = []; y_train = []\n",
    "        for k in train_idx:\n",
    "            X_train.append( np.load(f'{PATH_TO_DATA}data_{k}.npy'))\n",
    "            y_train.append( pd.read_parquet(f'{PATH_TO_DATA}targets_{k}.pqt') )\n",
    "        X_train = np.concatenate(X_train,axis=0)\n",
    "        y_train = pd.concat(y_train).target.values\n",
    "        print('### Training data shapes', X_train.shape, y_train.shape)\n",
    "\n",
    "        # READ VALID DATA FROM DISK\n",
    "        X_valid = []; y_valid = []\n",
    "        for k in valid_idx:\n",
    "            X_valid.append( np.load(f'{PATH_TO_DATA}data_{k}.npy'))\n",
    "            y_valid.append( pd.read_parquet(f'{PATH_TO_DATA}targets_{k}.pqt') )\n",
    "        X_valid = np.concatenate(X_valid,axis=0)\n",
    "        y_valid = pd.concat(y_valid).target.values\n",
    "        print('### Validation data shapes', X_valid.shape, y_valid.shape)\n",
    "        print('#'*25)\n",
    "\n",
    "        # TRAIN MODEL\n",
    "        # loss_fn = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([118828, 340085])).float()).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "        train_sample_strategy = torch.utils.data.sampler.WeightedRandomSampler(np.ones(X_train.shape[0]),\n",
    "                                                                               num_samples=X_train.shape[0], replacement=False)\n",
    "        train_dataloader = torch.utils.data.DataLoader(np.array(range(X_train.shape[0])), batch_size=params['batch_size'], num_workers=0,\n",
    "                                                       sampler=train_sample_strategy, drop_last=False)\n",
    "        val_sample_strategy = torch.utils.data.sampler.WeightedRandomSampler(np.ones(X_valid.shape[0]),\n",
    "                                                                             num_samples=X_valid.shape[0], replacement=False)\n",
    "        val_dataloader = torch.utils.data.DataLoader(np.array(range(X_valid.shape[0])), batch_size=params['batch_size'], num_workers=0,\n",
    "                                                     sampler=val_sample_strategy, drop_last=False)\n",
    "        oof_predictions = torch.zeros(X_valid.shape[0], 2).float().to(device)\n",
    "        model = eval(params['model'])(X_train.shape[-1]).to(device)\n",
    "        lr = params['lr'] * np.sqrt(params['batch_size']/2048)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=params['wd'])\n",
    "        lr_scheduler = MultiStepLR(optimizer=optimizer, milestones=[3600, 5000, 6000], gamma=0.1)\n",
    "        earlystoper = early_stopper(patience=params['early_stopping'], verbose=True)\n",
    "        start_epoch = 0\n",
    "        for epoch in range(start_epoch, params['max_epochs']):\n",
    "            train_loss_list = []\n",
    "            # train_acc_list = []\n",
    "            model.train()\n",
    "            for step, input_seeds in enumerate(train_dataloader):\n",
    "                batch_inputs = torch.from_numpy(X_train[input_seeds]).to(device)\n",
    "                batch_labels = torch.from_numpy(y_train[input_seeds]).to(device).long()\n",
    "                model.hidden_state = model.init_hidden(len(input_seeds), device)\n",
    "                train_batch_logits = model(batch_inputs)\n",
    "                train_loss = loss_fn(train_batch_logits, batch_labels)\n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                train_loss_list.append(train_loss.cpu().detach().numpy())\n",
    "                \n",
    "                #tr_batch_pred = None\n",
    "\n",
    "                if step % 50 == 0:\n",
    "                    tr_batch_pred = torch.sum(torch.argmax(train_batch_logits.clone().detach(), dim=1) == batch_labels) / batch_labels.shape[0]\n",
    "                    score = torch.softmax(train_batch_logits.clone().detach(), dim=1)[:, 1].cpu().numpy()\n",
    "                    print('In epoch:{:03d}|batch:{:04d}, train_loss:{:4f}, '\n",
    "                          'train_ap:{:.4f}, train_acc:{:.4f}, train_auc:{:.4f}'.format(epoch,step,\n",
    "                                                                                       np.mean(train_loss_list),\n",
    "                                                                                       average_precision_score(batch_labels.cpu().numpy(), score), \n",
    "                                                                                       tr_batch_pred.detach(),\n",
    "                                                                                       roc_auc_score(batch_labels.cpu().numpy(), score)))\n",
    "        \n",
    "            # mini-batch for validation\n",
    "            val_loss_list = 0\n",
    "            val_acc_list = 0\n",
    "            #val_correct_list = 0\n",
    "            val_all_list = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for step, input_seeds in enumerate(val_dataloader):\n",
    "                    batch_inputs = torch.from_numpy(X_valid[input_seeds]).to(device)\n",
    "                    batch_labels = torch.from_numpy(y_valid[input_seeds]).to(device).long()\n",
    "                    model.hidden_state = model.init_hidden(len(input_seeds), device)\n",
    "                    val_batch_logits = model(batch_inputs)\n",
    "                    oof_predictions[input_seeds] = val_batch_logits\n",
    "                    val_loss_list = val_loss_list + loss_fn(val_batch_logits, batch_labels)\n",
    "                    val_batch_pred = torch.sum(torch.argmax(val_batch_logits, dim=1) == batch_labels) / torch.tensor(batch_labels.shape[0])\n",
    "                    val_acc_list = val_acc_list + val_batch_pred * torch.tensor(batch_labels.shape[0])\n",
    "                    val_all_list = val_all_list + batch_labels.shape[0]\n",
    "                    if step % 50 == 0:\n",
    "                        score = torch.softmax(val_batch_logits.clone().detach(), dim=1)[:, 1].cpu().numpy()\n",
    "                        print('In epoch:{:03d}|batch:{:04d}, val_loss:{:4f}, val_ap:{:.4f}, '\n",
    "                              'val_acc:{:.4f}, val_auc:{:.4f}'.format(epoch,\n",
    "                                                                      step,\n",
    "                                                                      val_loss_list/val_all_list,\n",
    "                                                                      average_precision_score(batch_labels.cpu().numpy(), score), \n",
    "                                                                      val_batch_pred.detach(),\n",
    "                                                                      roc_auc_score(batch_labels.cpu().numpy(), score)))\n",
    "                #tmp_predictions = model(test_feature).cpu().numpy()\n",
    "            #infold_preds[fold] = tmp_predictions\n",
    "            #test_predictions += tmp_predictions / params['n_fold']\n",
    "            val_predictions = torch.softmax(oof_predictions.detach(), dim=-1)[:, 1].cpu().numpy()\n",
    "            earlystoper.earlystop(val_loss_list, amex_metric(y_valid, val_predictions), model)\n",
    "            if earlystoper.is_earlystop:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "        print(\"Best val_metric is: {:.7f}\".format(earlystoper.best_cv))\n",
    "        if not os.path.exists(PATH_TO_MODEL): os.makedirs(PATH_TO_MODEL)\n",
    "        torch.save(earlystoper.best_model.to('cpu').state_dict(), f'{PATH_TO_MODEL}gru_fold_{fold+1}.h5')\n",
    "\n",
    "        # INFER VALID DATA\n",
    "        print('Inferring validation data...')\n",
    "        # mini-batch for validation\n",
    "        val_loss_list = 0\n",
    "        val_acc_list = 0\n",
    "        #val_correct_list = 0\n",
    "        val_all_list = 0\n",
    "        model.load_state_dict(torch.load(f'{PATH_TO_MODEL}gru_fold_{fold+1}.h5'))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, input_seeds in enumerate(val_dataloader):\n",
    "                batch_inputs = torch.from_numpy(X_valid[input_seeds]).to(device)\n",
    "                batch_labels = torch.from_numpy(y_valid[input_seeds]).to(device).long()\n",
    "                model.hidden_state = model.init_hidden(len(input_seeds), device)\n",
    "                val_batch_logits = model(batch_inputs)\n",
    "                oof_predictions[input_seeds] = val_batch_logits\n",
    "                val_loss_list = val_loss_list + loss_fn(val_batch_logits, batch_labels)\n",
    "                val_batch_pred = torch.sum(torch.argmax(val_batch_logits, dim=1) == batch_labels) / torch.tensor(batch_labels.shape[0])\n",
    "                val_acc_list = val_acc_list + val_batch_pred * torch.tensor(batch_labels.shape[0])\n",
    "                val_all_list = val_all_list + batch_labels.shape[0]\n",
    "                if step % 50 == 0:\n",
    "                    score = torch.softmax(val_batch_logits.clone().detach(), dim=1)[:, 1].cpu().numpy()\n",
    "                    print('In epoch:{:03d}|batch:{:04d}, val_loss:{:4f}, val_ap:{:.4f}, '\n",
    "                          'val_acc:{:.4f}, val_auc:{:.4f}'.format(epoch,\n",
    "                                                                  step,\n",
    "                                                                  val_loss_list/val_all_list,\n",
    "                                                                  average_precision_score(batch_labels.cpu().numpy(), score), \n",
    "                                                                  val_batch_pred.detach(),\n",
    "                                                                  roc_auc_score(batch_labels.cpu().numpy(), score)))\n",
    "        val_predictions = torch.softmax(oof_predictions.detach(), dim=-1)[:, 1].cpu().numpy()\n",
    "        print()\n",
    "        print(f'Fold {fold+1} CV=', amex_metric(y_valid, val_predictions) )\n",
    "        print()\n",
    "        true = np.concatenate([true, y_valid])\n",
    "        oof = np.concatenate([oof, val_predictions])\n",
    "        \n",
    "        # CLEAN MEMORY\n",
    "        del model, X_train, y_train, X_valid, y_valid\n",
    "        gc.collect()\n",
    "\n",
    "    # PRINT OVERALL RESULTS\n",
    "    print('#'*25)\n",
    "    print(f'Overall CV =', amex_metric(true, oof) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9f695",
   "metadata": {
    "papermill": {
     "duration": 0.065298,
     "end_time": "2022-08-10T11:08:53.748522",
     "exception": false,
     "start_time": "2022-08-10T11:08:53.683224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Process Test Data\n",
    "We process the test data in the same way as train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "335ca915",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2022-08-10T11:08:53.878146Z",
     "iopub.status.busy": "2022-08-10T11:08:53.877185Z",
     "iopub.status.idle": "2022-08-10T11:08:53.884807Z",
     "shell.execute_reply": "2022-08-10T11:08:53.883613Z"
    },
    "papermill": {
     "duration": 0.074443,
     "end_time": "2022-08-10T11:08:53.887200",
     "exception": false,
     "start_time": "2022-08-10T11:08:53.812757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if PROCESS_DATA:\n",
    "    # GET TEST COLUMN NAMES\n",
    "    test = cudf.read_csv('../input/amex-default-prediction/test_data.csv', nrows=1)\n",
    "    T_COLS = test.columns\n",
    "    print(f'There are {len(T_COLS)} test dataframe columns')\n",
    "    \n",
    "    # GET TEST CUSTOMER NAMES (use pandas to avoid memory error)\n",
    "    if PATH_TO_CUSTOMER_HASHES:\n",
    "        test = cudf.read_parquet(f'{PATH_TO_CUSTOMER_HASHES}test_customer_hashes.pqt')\n",
    "    else:\n",
    "        test = pd.read_csv('/raid/Kaggle/amex/test_data.csv', usecols=['customer_ID'])\n",
    "        test['customer_ID'] = test['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\n",
    "    customers = test.drop_duplicates().sort_index().values.flatten()\n",
    "    print(f'There are {len(customers)} unique customers in test.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f58316e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T11:08:54.015520Z",
     "iopub.status.busy": "2022-08-10T11:08:54.015197Z",
     "iopub.status.idle": "2022-08-10T11:08:54.020041Z",
     "shell.execute_reply": "2022-08-10T11:08:54.018965Z"
    },
    "papermill": {
     "duration": 0.071053,
     "end_time": "2022-08-10T11:08:54.022280",
     "exception": false,
     "start_time": "2022-08-10T11:08:53.951227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_FILES = 20\n",
    "if PROCESS_DATA:\n",
    "    # CALCULATE SIZE OF EACH SEPARATE FILE\n",
    "    rows = get_rows(customers, test, NUM_FILES = NUM_FILES, verbose = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d39d7b36",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-10T11:08:54.147757Z",
     "iopub.status.busy": "2022-08-10T11:08:54.147397Z",
     "iopub.status.idle": "2022-08-10T11:08:54.156429Z",
     "shell.execute_reply": "2022-08-10T11:08:54.155287Z"
    },
    "papermill": {
     "duration": 0.075063,
     "end_time": "2022-08-10T11:08:54.158931",
     "exception": false,
     "start_time": "2022-08-10T11:08:54.083868",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if PROCESS_DATA:\n",
    "    # SAVE TEST CUSTOMERS INDEX\n",
    "    test_customer_hashes = cupy.array([],dtype='int64')\n",
    "    \n",
    "    # CREATE PROCESSED TEST FILES AND SAVE TO DISK\n",
    "    for k in range(NUM_FILES):\n",
    "\n",
    "        # READ CHUNK OF TEST CSV FILE\n",
    "        skip = int(np.sum( rows[:k] ) + 1) #the plus one is for skipping header\n",
    "        test = cudf.read_csv('../input/amex-default-prediction/test_data.csv', nrows=rows[k], \n",
    "                              skiprows=skip, header=None, names=T_COLS)\n",
    "\n",
    "        # FEATURE ENGINEER DATAFRAME\n",
    "        test = feature_engineer(test, targets = None)\n",
    "        \n",
    "        # SAVE TEST CUSTOMERS INDEX\n",
    "        cust = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n",
    "        test_customer_hashes = cupy.concatenate([test_customer_hashes,cust])\n",
    "\n",
    "        # SAVE FILES\n",
    "        print(f'Test_File_{k+1} has {test.customer_ID.nunique()} customers and shape',test.shape)\n",
    "        data = test.iloc[:,1:].values.reshape((-1,13,188))\n",
    "        cupy.save(f'{PATH_TO_DATA}test_data_{k+1}',data.astype('float32'))\n",
    "        \n",
    "    # SAVE CUSTOMER INDEX OF ALL TEST FILES\n",
    "    cupy.save(f'{PATH_TO_DATA}test_hashes_data', test_customer_hashes)\n",
    "\n",
    "    # CLEAN MEMORY\n",
    "    del test, data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457e1af",
   "metadata": {
    "papermill": {
     "duration": 0.212614,
     "end_time": "2022-08-10T11:08:54.441493",
     "exception": false,
     "start_time": "2022-08-10T11:08:54.228879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Infer Test Data\n",
    "We infer the test data from our saved fold models. If you don't wish to infer test but you only want your notebook to compute a validation score to evaluate model changes, then set variable `INFER_TEST = False` in the beginning of this notebook. Also if you wish to infer from previously trained models, then add the path to the Kaggle dataset in the variable `PATH_TO_MODEL` in the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "686869fa",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-10T11:08:54.666218Z",
     "iopub.status.busy": "2022-08-10T11:08:54.665694Z",
     "iopub.status.idle": "2022-08-10T11:10:45.178967Z",
     "shell.execute_reply": "2022-08-10T11:10:45.177888Z"
    },
    "papermill": {
     "duration": 110.626747,
     "end_time": "2022-08-10T11:10:45.181445",
     "exception": false,
     "start_time": "2022-08-10T11:08:54.554698",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring Test_File_1\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_2\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_3\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_4\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_5\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_6\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_7\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_8\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_9\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_10\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_11\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_12\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_13\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_14\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_15\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_16\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_17\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_18\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_19\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n",
      "Inferring Test_File_20\n",
      "In fold 2 test batch:0000\n",
      "In fold 2 test batch:0050\n",
      "In fold 3 test batch:0000\n",
      "In fold 3 test batch:0050\n",
      "In fold 4 test batch:0000\n",
      "In fold 4 test batch:0050\n",
      "In fold 5 test batch:0000\n",
      "In fold 5 test batch:0050\n"
     ]
    }
   ],
   "source": [
    "if INFER_TEST:\n",
    "    # INFER TEST DATA\n",
    "    start = 0; end = 0\n",
    "    sub = pd.read_csv('../input/amex-default-prediction/sample_submission.csv')\n",
    "    NUM_FILES = 20\n",
    "    # REARANGE SUB ROWS TO MATCH PROCESSED TEST FILES\n",
    "    sub['hash'] = sub['customer_ID'].str[-16:].apply(lambda x: int(x, 16)).astype('int64')\n",
    "    test_hash_index = np.load(f'{PATH_TO_DATA}test_hashes_data.npy')\n",
    "    sub = sub.set_index('hash').loc[test_hash_index].reset_index(drop=True)\n",
    "    \n",
    "    for k in range(NUM_FILES):\n",
    "        # LOAD TEST DATA\n",
    "        print(f'Inferring Test_File_{k+1}')\n",
    "        X_test = np.load(f'{PATH_TO_DATA}test_data_{k+1}.npy')\n",
    "        end = start + X_test.shape[0]\n",
    "\n",
    "        # BUILD MODEL\n",
    "        model = eval(params['model'])(X_test.shape[-1]).to(device)\n",
    "        \n",
    "        # INFER 5 FOLD MODELS\n",
    "        model.load_state_dict(torch.load(f'{PATH_TO_MODEL}gru_fold_1.h5'))\n",
    "        test_predictions = torch.zeros(X_test.shape[0], 2).to(device).float()\n",
    "        test_sample_strategy = torch.utils.data.sampler.WeightedRandomSampler(np.ones(X_test.shape[0]),\n",
    "                                                                              num_samples=X_test.shape[0], replacement=False)\n",
    "        test_dataloader = torch.utils.data.DataLoader(np.array(range(X_test.shape[0])), batch_size=params['batch_size'], num_workers=0,\n",
    "                                                      sampler=test_sample_strategy, drop_last=False)\n",
    "        \n",
    "        for j in range(1,5):\n",
    "            model.load_state_dict(torch.load(f'{PATH_TO_MODEL}gru_fold_{j+1}.h5'))\n",
    "            with torch.no_grad():\n",
    "                for step, input_seeds in enumerate(test_dataloader):\n",
    "                    batch_inputs = torch.from_numpy(X_test[input_seeds]).to(device).float()\n",
    "                    model.hidden_state = model.init_hidden(len(input_seeds), device)\n",
    "                    test_batch_logits = model(batch_inputs)\n",
    "                    test_predictions[input_seeds] = test_predictions[input_seeds] + torch.softmax(test_batch_logits, dim=-1)\n",
    "                    #test_batch_pred = torch.sum(torch.argmax(test_batch_logits, dim=1) == batch_labels) / torch.tensor(batch_labels.shape[0])\n",
    "                    if step % 50 == 0:\n",
    "                        print('In fold {} test batch:{:04d}'.format(j+1, step))\n",
    "        test_predictions /= 5.0\n",
    "\n",
    "        # SAVE TEST PREDICTIONS\n",
    "        sub.loc[start:end-1,'prediction'] = test_predictions[:, 1].cpu().numpy()\n",
    "        start = end\n",
    "        \n",
    "        # CLEAN MEMORY\n",
    "        del model, X_test\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8437fb7",
   "metadata": {
    "papermill": {
     "duration": 0.071191,
     "end_time": "2022-08-10T11:10:45.327777",
     "exception": false,
     "start_time": "2022-08-10T11:10:45.256586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45fcb226",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T11:10:45.475964Z",
     "iopub.status.busy": "2022-08-10T11:10:45.475543Z",
     "iopub.status.idle": "2022-08-10T11:10:48.644459Z",
     "shell.execute_reply": "2022-08-10T11:10:48.643051Z"
    },
    "papermill": {
     "duration": 3.24659,
     "end_time": "2022-08-10T11:10:48.647239",
     "exception": false,
     "start_time": "2022-08-10T11:10:45.400649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file shape is (924621, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>038be0571bd6b3776cb8512731968f4de302c811030124...</td>\n",
       "      <td>0.002693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0074a0233ef766b52884608cc8cf9098f59d885b5d59fc...</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>060b8b7f30f795a0e93995d45b29461ffa6ece0eeb5c3d...</td>\n",
       "      <td>0.116437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03a1d125bdd776000bf0b28238d0bea240ad581d332e70...</td>\n",
       "      <td>0.101910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0290f245dd35ba899af52316ccc62b2627e7ae18cd76a2...</td>\n",
       "      <td>0.263011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID  prediction\n",
       "0  038be0571bd6b3776cb8512731968f4de302c811030124...    0.002693\n",
       "1  0074a0233ef766b52884608cc8cf9098f59d885b5d59fc...    0.000180\n",
       "2  060b8b7f30f795a0e93995d45b29461ffa6ece0eeb5c3d...    0.116437\n",
       "3  03a1d125bdd776000bf0b28238d0bea240ad581d332e70...    0.101910\n",
       "4  0290f245dd35ba899af52316ccc62b2627e7ae18cd76a2...    0.263011"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if INFER_TEST:\n",
    "    sub.to_csv('submission.csv',index=False)\n",
    "    print('Submission file shape is', sub.shape )\n",
    "    display( sub.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24981e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T11:10:48.790618Z",
     "iopub.status.busy": "2022-08-10T11:10:48.789720Z",
     "iopub.status.idle": "2022-08-10T11:10:49.275535Z",
     "shell.execute_reply": "2022-08-10T11:10:49.274437Z"
    },
    "papermill": {
     "duration": 0.558552,
     "end_time": "2022-08-10T11:10:49.277899",
     "exception": false,
     "start_time": "2022-08-10T11:10:48.719347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if INFER_TEST:\n",
    "    # DISPLAY SUBMISSION PREDICTIONS\n",
    "    plt.hist(sub.prediction, bins=100)\n",
    "    plt.title('Test Predictions')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 579.035936,
   "end_time": "2022-08-10T11:10:52.776132",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-10T11:01:13.740196",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
